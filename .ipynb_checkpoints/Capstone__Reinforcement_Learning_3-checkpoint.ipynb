{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs\n",
    "\n",
    "- put on git\n",
    "- append jsons\n",
    "- append code with full model -> \"File, Download as Python\"\n",
    "- !!! if done, r = 0 !!!!!, anstatt r_t? als parameter flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Implementing and Comparing three Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Reinforcement Learning As My Capstone?\n",
    "\n",
    "### Motivation\n",
    "\n",
    "I have a background in supervised learning mostly, a bit also unsupervised, and I've been applying these techniques to hands-on business use cases for 10 years now.\n",
    "\n",
    "When I applied for the machine learning engineer nanodegree, I wanted to deepen my knowledge in areas known to me, and also discover new areas.\n",
    "\n",
    "Both objectives were met during the course. The latter came into play with the Smartcab task, and I was struck. This was new. It combined two areas where I see a steep learning curve for me as well as a lot of interesting applications in business life:\n",
    "\n",
    "- First, Artifical Neural Nets as the main computing block of learning. During my studies, networks were not as much on the screen as today. During this project, I will thus need to dive into this topic and gain insights which will drive my interest and career onwards. I see networks as a generic computing block, helpful in many areas of prediction.\n",
    "\n",
    "- Second, Reinforcement Learning. Markov Decision Processes are just a lot of fun and a real challenge for a greenhorn brain. Still, there are components of supervised learning and statistics in general, which I can apply hereby. Lastly, I am interested in robotics and learning in general, i.e. getting (sensory) data, transform it, predict, and see how the reward or punishment gets in. Human learning is fascinating, as is machine learning.\n",
    "\n",
    "- Third, Cloud Computing. During the computation of quantile estimates for my master thesis in 2009, I remember my Macbook running during days and getting so hot that I had to put Ice underneath it. Here, I want to get aquainted with the resources available on the cloud to do heavy computing. I will hook onto Amazon Web Services EC2 Instances with GPUs in order to compute the Keras/TensorFlow parts of my Capstone. \n",
    "\n",
    "### Outline\n",
    "\n",
    "- I will try to train an toy, Atari-based AI agent on 1D input data. \n",
    "\n",
    "- I will not touch 2D visual inputs, since I found that convolutions are a separate topic block whick easily can be implemented as an extension to the presented building blocks. Besides that, one could also convert 2D to input 1D beforehand and proceed with the presented blocks.\n",
    "\n",
    "- The models will consist of fully-connected layers, activation functions and output layers in the appropriate form.\n",
    "\n",
    "- In order to train the model, I will benchmark three reinforcement learning algorithms:\n",
    "    - Deep Q Learning\n",
    "    - Advantage Actor-Critic Policy Gradients\n",
    "    - Q Actor-Critic Policy Gradients\n",
    "\n",
    "Deep Q Learning is the natural entry point, since I can use my so-far knowledge of standard Q Learning and go one step further with it. Besides that, it only uses one model, compared with Policy Gradients, where two models come into play. I will develop the algorithm step-by-step, since it was the process I had to go through anyway to understand the logic. \n",
    "\n",
    "Policy Gradients will not be developed step-by-step, but recycling many building blocks of Deep Q Learning as well as the methodology.\n",
    "\n",
    "Benchmarking will be starting with brute-force parametrization combinatorics in order to hopefully find working models.\n",
    "\n",
    "Since this is expected to be computationally intensive, I will shift computation to AWS EC2 cloud.\n",
    "\n",
    "### Credits and Thanks\n",
    "\n",
    "- Neural Networks\n",
    "    - Andrew Trask https://iamtrask.github.io\n",
    "    - Sebastian Raschka http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html\n",
    "- Deep Q Learning\n",
    "    - Deep Mind: Playing Atari with Deep Reinforcement Learning https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "    - Tambet Matiisen https://www.nervanasys.com/demystifying-deep-reinforcement-learning/ https://github.com/tambetm/simple_dqn/blob/master/src/replay_memory.py\n",
    "    - Eder Santana http://edersantana.github.io/articles/keras_rl/\n",
    "    - Ben Lau https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html\n",
    "- Policy Gradients\n",
    "    - Andrej Karpathy http://karpathy.github.io/2016/05/31/rl/\n",
    "    - Denny Britz https://github.com/dennybritz/reinforcement-learning/tree/master/PolicyGradient/\n",
    "    - Open AI Gym https://gym.openai.com/docs/rl#policy-gradients\n",
    "    - David Silver https://www.youtube.com/watch?v=KHZVXao4qXs\n",
    "    - Non-standard gradient optimization: https://github.com/fchollet/keras/issues/3062\n",
    "- Keras\n",
    "    - Francois Chollet https://keras.io https://github.com/fchollet/keras/tree/master/examples \n",
    "- Cloud Computing on Amazon Web Services EC2\n",
    "    - The Amazon Tutorials https://aws.amazon.com/de/ec2/\n",
    "    - Jie Yang http://yangjie.me/2015/08/26/Run-Jupyter-Notebook-Server-on-AWS-EC2/\n",
    "    \n",
    "### Main Dependencies\n",
    "\n",
    "We are working with Elon Musk's Open AI Gym (https://gym.openai.com/) as a training environment for our AI agent.\n",
    "\n",
    "Lunar Lander environment (https://gym.openai.com/envs/LunarLander-v2) is particularily appealing to me. It is based on box2d, which simulates real life physics. Charming! And, with its 1D input state vector, it is a first step for creating and tuning an AI agent, before proceeding with convolution preprocessing of 2D inputs.\n",
    "\n",
    "The environment home page says the following:\n",
    "\n",
    ">\"Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each step. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine.\"\n",
    "\n",
    "Model building and training is done with Keras (https://keras.io). This modular, minimalist library makes ANN life as easy as it can get, and in plus runs on both Theano and Tensorflow backends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm 1: Deep Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Starting Point: Evaluating Bellman Equations From Data (Q-Learning)\n",
    "\n",
    "Q learning is about revisiting states. We are in a specific state s at time t, and because the state space is sufficiently small, we might discover that the agent has already been in s before. It therefore has made an experience for s by taking an action, and collecting a reward or punishment. All this is stored in the agent's \"memory\" (possible a dictionary of dictionaries, main keys being the states, values being the actions / keys and their values). We want the agent to take advantage of this \"memory\": We look up the expected lifetime rewards per each possible action in s (a.k.a. action-value function, q values), select the maximum ```q``` value, and execute the chosen action. \n",
    "\n",
    "We now have fresh evidence about the consequence of a specific action in a specific state: We know the initial state ```s_t```, we know the selected action ```a_t```, we know the reward ```r_t```, and we know the new state this all lead to, ```s_t1```. This knowledge we now use to update the agent's memory: We calculate a new ```q``` value for ```s_t``` by taking the observed reward ```r_t```, and adding to it the discounted maximum ```q``` value for ```s_t1```. The difference to the old ```q``` value is the new ```q``` value for action ```a_t``` in state ```s_t```.\n",
    "\n",
    "### Why Q-Learning Often Does Not Work: Exploding State Spaces\n",
    "\n",
    "Revisiting states is often not possible, even in the long run, because there are simply too many combinations of relevant inputs which constitute a state. Just think a small number of inputs, each input being a floating number with 4 digits. Even this small setting is creating a large amount of combinations: the state space explodes. Revisiting states is very unlikely, we will need a huge number of trials to generate memory updates. As a consequence, learning is slow, or even not happening.\n",
    "\n",
    "In order to illustrate the point, we are going to set up a basic q-learning algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from itertools import count\n",
    "from sys import stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Global Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Hyperparameters\n",
    "\n",
    "- ```NUM_EPISODE``` denotes the maximum number of episodes an epoch will embrace\n",
    "- ```GAMMA``` is the factor by which future expected rewards are discounted\n",
    "- ```ALPHA``` is the learning rate TODO\n",
    "- ```Q_TABLE``` is the agent's memory. The states are the keys, and dictionaries of actions and their respective values are the values\n",
    "- ```VALUE_INIT``` is the initial value for the actions of a state, once the state is visited the first time. It is set to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_EPISODE = 100\n",
    "GAMMA = 0.99\n",
    "ALPHA = 0.1\n",
    "Q_TABLE = {}\n",
    "VALUE_INIT = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Environment\n",
    "\n",
    "We create an instance (```ENV```) of the Lunar lander environment. Its input dimensions ```NUM_INPUT``` are obtained by resetting the environment, and the actions by the environment method ```action_space```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-11-16 19:19:47,623] Making new env: LunarLander-v2\n"
     ]
    }
   ],
   "source": [
    "ENV = gym.make(\"LunarLander-v2\")\n",
    "NUM_INPUT = ENV.reset().shape[0]\n",
    "NUM_ACTION = ENV.action_space.n\n",
    "ACTIONS = np.arange(0, NUM_ACTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Training Epoch\n",
    "\n",
    "We are going to create the main building block of this exercise: The training block for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_ql(render=False):\n",
    "    # Init epoch stats\n",
    "    stats = {}\n",
    "    \n",
    "    # Init counter for how many times we revisited states\n",
    "    revisiting_states = 0 \n",
    "\n",
    "    # Play through episodes\n",
    "    for episode in range(NUM_EPISODE):\n",
    "        # Init episode stats\n",
    "        stats[episode] = [0]*3\n",
    "        \n",
    "        # Init observations\n",
    "        x_t = ENV.reset() \n",
    "        # Take only the current observation as state, in order to keep the state space as small as possible\n",
    "        s_t = tuple(x_t) \n",
    "        done = False\n",
    "\n",
    "        # Start an episode\n",
    "        for step in count():\n",
    "            \n",
    "            # Reset the environment for a new gaming step\n",
    "            if render: ENV.render()\n",
    "            \n",
    "            # Look up the action with the highest value at s_t, as well as its value\n",
    "            a_t, q, r_s  = best_action(s_t)\n",
    "\n",
    "            # Observe after action a_t\n",
    "            x_t1, r_t, done, info = ENV.step(a_t) \n",
    "            # Again, take only the current observation\n",
    "            s_t1 = tuple(x_t1) \n",
    "            \n",
    "            # Look up the action with the highest value at s_t1, as well as its value\n",
    "            a_t1, Q_sa, _ = best_action(s_t1) \n",
    "            \n",
    "            # Look up the action with the highest value at s_t1, as well as its value\n",
    "            Q_TABLE[s_t][a_t] = q + ALPHA * (r_t + GAMMA * Q_sa - q) \n",
    "            \n",
    "            # Update statistics: Sum up rewards, count steps, successes, solved\n",
    "            stats[episode][0] += r_t\n",
    "            if r_t >= 100: stats[episode][1] += 1\n",
    "            if r_t >= 200: stats[episode][2] += 1\n",
    "            \n",
    "            # Visualize\n",
    "            stdout.write(\"\\r{} | Step {} @ Episode {}/{}\".format(\\\n",
    "                EPOCH, step, episode, NUM_EPISODE))\n",
    "            \n",
    "            # Exit episode after crash or deadlock\n",
    "            if done or step > 10000: break\n",
    "            \n",
    "            # Update state\n",
    "            s_t = s_t1\n",
    "            revisiting_states += r_s\n",
    "            \n",
    "    # Visualize revisiting states issue\n",
    "    print(\"\\rQ Table size {}, # Revisited States {}\".format(len(Q_TABLE), revisiting_states))\n",
    "    \n",
    "    return stats\n",
    "    \n",
    "    \n",
    "# Create helper function to initialize and query Q-table\n",
    "def best_action(state):\n",
    "    # Create init scenario for table queries at t and t1\n",
    "    if state not in Q_TABLE or sum(Q_TABLE[state].values()) == 0: \n",
    "        \n",
    "        # Bookkeeping: Init revisiting states counter\n",
    "        revisit_state = 0 \n",
    "        \n",
    "        # Init q function\n",
    "        q_function = {} \n",
    "        for A in ACTIONS: q_function[A] = VALUE_INIT\n",
    "        Q_TABLE[state] = q_function \n",
    "        \n",
    "        # Do random action\n",
    "        action = np.random.choice(ACTIONS, 1)[0] \n",
    "    else: \n",
    "        revisit_state = 1\n",
    "        \n",
    "        # Select action according to max q\n",
    "        action = max(Q_TABLE[state], key=Q_TABLE[state].get) \n",
    "    \n",
    "    # Get q value for action selected\n",
    "    q = Q_TABLE[state][action] \n",
    "    \n",
    "    return action, q, revisit_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we recorded statistics (*stats*) along the way:\n",
    "\n",
    "- The reward sum per episode\n",
    "- A counter on how many times the lander was successful during the episode (with a reward greater or equal 100)\n",
    "- A counter on how many times the game was solved (with a reward greater or equal 200)\n",
    "\n",
    "As a special statistic over the epoch, we record the number of revisited states, and compare it to the length of the Q table.\n",
    "\n",
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Table size 9212, # Revisited States 0\n",
      "Epoch Q-Learning, Maximum Reward -79.3559299262, Successful Episodes 0, Solved Episodes 0"
     ]
    }
   ],
   "source": [
    "# Create model id\n",
    "EPOCH = \"Q-Learning\"\n",
    "                                \n",
    "# Train model / Play epoch\n",
    "stats = train_ql(render=True) \n",
    "\n",
    "# Calculate cumulative epoch statistics\n",
    "highest_reward = max([ v[0] for v in stats.values() ])\n",
    "success_episodes = sum([ v[1] for v in stats.values() ])\n",
    "solved_episodes = sum([ v[2] for v in stats.values() ])\n",
    "                                \n",
    "# Visualize\n",
    "stdout.write(\"\\rEpoch {}, Maximum Reward {}, Successful Episodes {}, Solved Episodes {}\".format(\\\n",
    "    EPOCH, highest_reward, success_episodes, solved_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What to Do? Do Not Update the Q Function, But the Q  Function Estimator: Deep Q Learning\n",
    "\n",
    "In this situation, we replace the \"revisiting states\" by a function approximator: \n",
    "\n",
    "We let a Artifical Neural Net (ANN) estimate the ```q``` function for the state ```s``` the agent is visiting at step ```t```. \n",
    "\n",
    "Once we performed the action based on the maximum of the ```q``` function (just the action with the highest expected lifetime reward at ```t```), we know the reward, and the subsequent state. \n",
    "\n",
    "Based on this, we are able to update the agent's memory. But this time, we do not update the ```q``` function directly. Instead, we are updating the ANN, which means that we are updating the weights used in the ANN. And that is how exactly:\n",
    "\n",
    "- At time t, we already know the estimation of the ```q``` function for state ```s_t```: We used it to pick an action ```a_t``` accordingly. Read again: this is the **estimation** of the ```q``` function.\n",
    "\n",
    "- After action ```a_t```, we know ```s_t```, ```a_t```, ```r_t``` and ```s_t1```. This allows us to update the ```q``` function, **but only for the action taken**. We take ```r_t```, and add to it the discounted expected lifetime reward, in other words we let the ANN estimate the ```q``` function for state ```s_t1```. \n",
    "\n",
    "- For the action taken, we can update the ```q``` value now. All the other actions are not performed, we do not know about the reward, or a subsequent state ```s_t1```. So, we cannot learn for those. This updated ```q``` function is the **target**.\n",
    "\n",
    "- We feed the error, which is the difference between the estimation and the target.\n",
    "\n",
    "- We backpropagate the error through the network, such that the weights are updated.\n",
    "\n",
    "Next time we estimate the q function for another state ```s```, we have updated weights\n",
    "\n",
    "### 0) Understand the Gist of ANNs\n",
    "\n",
    "Firstly, I wanted to understand how a neural network works. Andrew Trask's excellent toy examples helped me understand it. I create a network with one hidden layer, and the output layer, both sigmoid activated, which returns probablities for each of the outputs.\n",
    "\n",
    "I will use a sigmoid activation function due to the easy calculation of the derivative and the widespread application in neural nets.\n",
    "\n",
    "Except for numpy, there are no other dependencies.\n",
    "\n",
    "In the procedure, the magic happens with ```layer_2_weighted_loss```: Each output loss is multiplied by the slope / gradient of the predicted value on the sigmoid curve.\n",
    "\n",
    "In order to get a feeling for changes invoked by changing parametrization, I will alter the learning rate ```ALPHA``` as well as the number of neurons in the hidden layer ```NUM_HIDDEN_NEURON``` within an arbitrary range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ALPHA_RANGE = [10**-4, 10**-2, 1, 10**2, 10**4] # Constant over one epoch\n",
    "NUM_HIDDEN_NEURON_RANGE = [8, 16, 32, 64, 128, 256, 512] # Constant over one epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create A Basic ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Size    8, alpha  0.0001: Avg loss 0.495749838549\n",
      "Hidden Size    8, alpha    0.01: Avg loss 0.493850319492\n",
      "Hidden Size    8, alpha       1: Avg loss 0.0776593803342\n",
      "Hidden Size    8, alpha     100: Avg loss 0.00388596579178\n",
      "Hidden Size    8, alpha   10000: Avg loss 0.00033695731853\n",
      "Hidden Size   16, alpha  0.0001: Avg loss 0.499660442888\n",
      "Hidden Size   16, alpha    0.01: Avg loss 0.499654295517\n",
      "Hidden Size   16, alpha       1: Avg loss 0.146303014885\n",
      "Hidden Size   16, alpha     100: Avg loss 0.00280178435272\n",
      "Hidden Size   16, alpha   10000: Avg loss 0.000276309251117\n",
      "Hidden Size   32, alpha  0.0001: Avg loss 0.499984086847\n",
      "Hidden Size   32, alpha    0.01: Avg loss  0.49998406965\n",
      "Hidden Size   32, alpha       1: Avg loss  0.49998213631\n",
      "Hidden Size   32, alpha     100: Avg loss            0.5\n",
      "Hidden Size   32, alpha   10000: Avg loss            0.5\n",
      "Hidden Size   64, alpha  0.0001: Avg loss 0.499999998759\n",
      "Hidden Size   64, alpha    0.01: Avg loss 0.499999998759\n",
      "Hidden Size   64, alpha       1: Avg loss 0.499999998759\n",
      "Hidden Size   64, alpha     100: Avg loss 0.499999998758\n",
      "Hidden Size   64, alpha   10000: Avg loss 0.499999998554\n",
      "Hidden Size  128, alpha  0.0001: Avg loss            0.5\n",
      "Hidden Size  128, alpha    0.01: Avg loss            0.5\n",
      "Hidden Size  128, alpha       1: Avg loss            0.5\n",
      "Hidden Size  128, alpha     100: Avg loss            0.5\n",
      "Hidden Size  128, alpha   10000: Avg loss            0.5\n",
      "Hidden Size  256, alpha  0.0001: Avg loss            0.5\n",
      "Hidden Size  256, alpha    0.01: Avg loss            0.5\n",
      "Hidden Size  256, alpha       1: Avg loss            0.5\n",
      "Hidden Size  256, alpha     100: Avg loss            0.5\n",
      "Hidden Size  256, alpha   10000: Avg loss            0.5\n",
      "Hidden Size  512, alpha  0.0001: Avg loss            0.5\n",
      "Hidden Size  512, alpha    0.01: Avg loss            0.5\n",
      "Hidden Size  512, alpha       1: Avg loss            0.5\n",
      "Hidden Size  512, alpha     100: Avg loss            0.5\n",
      "Hidden Size  512, alpha   10000: Avg loss            0.5\n"
     ]
    }
   ],
   "source": [
    "# Create the helper functions \n",
    "\n",
    "# Create the sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Create the sigmoid function's derivative\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "# Initialize weights with the correct dimensionality to fit in the input layer\n",
    "X = np.random.randint(2, size=(4, 3))\n",
    "y = np.random.randint(2, size=(4, 1))\n",
    "\n",
    "# Test the hidden dimensions\n",
    "for NUM_HIDDEN_NEURON in NUM_HIDDEN_NEURON_RANGE: \n",
    "    \n",
    "    # Initialize 1st set of weights\n",
    "    W1 = np.random.rand(X.shape[1], NUM_HIDDEN_NEURON) \n",
    "\n",
    "    # Initialize 2nd set of weights\n",
    "    W2 = np.random.rand(NUM_HIDDEN_NEURON, y.shape[1]) \n",
    "    \n",
    "    # Test the alphas\n",
    "    for ALPHA in ALPHA_RANGE: \n",
    "        \n",
    "        for episode in range(NUM_EPISODE):\n",
    "            # Forward propagate\n",
    "            \n",
    "            # Initialize hidden layer (fully connected)\n",
    "            layer_1 = np.dot(X, W1)\n",
    "            \n",
    "            # Apply sigmoid activation\n",
    "            layer_1 = sigmoid(layer_1) \n",
    "            \n",
    "            # Initialize output layer(fully connected)\n",
    "            layer_2 = np.dot(layer_1, W2) \n",
    "            \n",
    "            # Apply sigmoid activation \n",
    "            layer_2 = sigmoid(layer_2) \n",
    "\n",
    "            # Calculate loss\n",
    "            layer_2_loss = y - layer_2 \n",
    "\n",
    "            ''' Apply SGD to the loss: the more certain the estimate, the less weighted it will get: \n",
    "                The gradient at the extremes is smaller than in the middle\n",
    "            '''\n",
    "            layer_2_weighted_loss = layer_2_loss * sigmoid_derivative(layer_2) # element-wise multiplication!\n",
    "            \n",
    "            # Backpropagate\n",
    "            \n",
    "            # Compute the effect of the hidden layer to the weighted loss\n",
    "            layer_1_loss = np.dot(layer_2_weighted_loss, W2.T) \n",
    "\n",
    "            # Apply SGD\n",
    "            layer_1_weighted_loss = layer_1_loss * sigmoid_derivative(layer_1) \n",
    "                \n",
    "            # Update weights\n",
    "            W2 += ALPHA * np.dot(layer_1.T, layer_2_weighted_loss)\n",
    "            W1 += ALPHA * np.dot(X.T, layer_1_weighted_loss)\n",
    "            \n",
    "            # Visualize\n",
    "            if episode == NUM_EPISODE - 1: print(\"Hidden Size {}, alpha {}: Avg loss {}\".format(\n",
    "                                                '%4s' % NUM_HIDDEN_NEURON, \\\n",
    "                                                '%7s' % ALPHA, \\\n",
    "                                                '%14s' % np.mean(np.abs(layer_2_loss))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "Clearly visible, the parametrization of the network has an impaction upon the network performance. Some configurations work well, like e.g. a hidden size of 8 with an ```ALPHA``` of 100. Others look fishy with an average loss close or equal to 0.5.\n",
    "\n",
    "### 1) Deep Q Learning from Single Current Observations\n",
    "\n",
    "In order to implement the estimator, we now change the training epoch function. We first get rid of the lookup table ```Q_TABLE``` and the table's ```q``` value initialization ```Q_INIT```. In plus, we do not need the learning rate ```ALPHA``` anymore. In stead, we implement the above-standing routine 1) to 5). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend Hyperparameters\n",
    "\n",
    "We start with a first set of fixed hyperparameters. They will undergo changes along the way:\n",
    "\n",
    "- ```STEP_MEM``` is the number of steps the agent should take into account as the current state it is in: This is the \"operational\" memory of the agent. I will refer to it as step memory.\n",
    "- ```NUM_HIDDEN_NEURON``` is the number of neurons in the hidden layer, see below\n",
    "- ```INITIALIZATION``` is one of the available weight initializations in Keras, see below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ALPHA = 10\n",
    "STEP_MEM = 1\n",
    "NUM_HIDDEN_NEURON = 200\n",
    "INITIALIZATION = 'glorot_uniform'\n",
    "ACTIVATION = 'relu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Keras Model\n",
    "\n",
    "When I started with building models, I naturally had to get aquainted with the Keras API semantics first. As indicated above, I want to start out with a shallow model version to understand fully the model mechanics, and leave it to a later stage to deepen the model.\n",
    "\n",
    "At this point, I play around with the parametrization quite a bit, and go forward with a nicely working model: \n",
    "\n",
    "I chose Keras' ```Sequential``` model architecture, creating a model consisting of \n",
    "\n",
    "- one fully-connected hidden layer (Keras terminology is ```Dense```) plus an non-linear ```relu``` Activation (as seen with Ben Lau too).\n",
    "\n",
    "- the output layer with softmax activation, which produces probabilities for each of the possible actions given a certain state input. TODO Rechtfertigung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _create_network():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(NUM_HIDDEN_NEURON, init=INITIALIZATION, input_shape=(STEP_MEM*NUM_INPUT,))) \n",
    "    model.add(Activation(ACTIVATION))\n",
    "    model.add(Dense(NUM_ACTION, init=INITIALIZATION))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Build Training Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dqn_1(model, render=False):\n",
    "    # Init epoch stats\n",
    "    stats = {}\n",
    "\n",
    "    # Play through episodes\n",
    "    for episode in range(NUM_EPISODE): \n",
    "        # Init episode stats\n",
    "        stats[episode] = [0]*3\n",
    "        \n",
    "        # Init observations\n",
    "        x_t = ENV.reset()\n",
    "        # Pile up FRAME_MEM times the same init observation, in order to be consistent with the model input\n",
    "        s_t = np.tile(x_t, STEP_MEM)\n",
    "        done = False\n",
    "\n",
    "        # Start an episode\n",
    "        for step in count():\n",
    "            \n",
    "            # Reset the environment for a new gaming step\n",
    "            if render: ENV.render()\n",
    "    \n",
    "            # Estimate q for each action at s_t\n",
    "            q = model.predict(s_t[np.newaxis])[0] \n",
    "\n",
    "            # Take action with highest estimated reward (argmax returns index)\n",
    "            a_t = np.argmax(q) \n",
    "            \n",
    "            # Observe after action a_t\n",
    "            x_t, r_t, done, info = ENV.step(a_t) \n",
    "            \n",
    "            # Create state at t1: Append x observations, throw away the earliest\n",
    "            s_t1 = np.concatenate((x_t, s_t[:(STEP_MEM-1) * NUM_INPUT,]), axis=0) \n",
    "\n",
    "            # Estimate q for each action at s_t1 (again a forward pass)\n",
    "            Q_sa = model.predict(s_t1[np.newaxis])[0] \n",
    "\n",
    "            ''' Create reference/targets by updating estimated reward for chosen action:\n",
    "                For action taken, replace estimated reward by remaining cumulative lifetime reward\n",
    "            ''' \n",
    "            targets = q\n",
    "            targets[a_t] = r_t + GAMMA * np.max(Q_sa) if not done else r_t\n",
    "\n",
    "            ''' Learn!\n",
    "                - Again, predict q values for state s_t\n",
    "                - Calculate loss by comparing predictions to targets: they will differ only for the action taken\n",
    "                - backpropagate error for action taken, update weights\n",
    "            ''' \n",
    "            model.fit(s_t[np.newaxis], targets[np.newaxis], verbose=0)\n",
    "            \n",
    "            # Update statistics: Sum up rewards, count steps, successes, solved\n",
    "            stats[episode][0] += r_t\n",
    "            if r_t >= 100: stats[episode][1] += 1\n",
    "            if r_t >= 200: stats[episode][2] += 1\n",
    "            \n",
    "            # Visualize\n",
    "            stdout.write(\"\\r{} | Step {} @ Episode {}/{}\".format(\\\n",
    "                EPOCH, step, episode, NUM_EPISODE))\n",
    "   \n",
    "            if done or step > 10000: break\n",
    "\n",
    "            # Update state\n",
    "            s_t = s_t1\n",
    "            \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model\n",
    "\n",
    "Let's try! I now play around with some parametrizations and see what works, then I go back up and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10_1_200_glorot_uniform_relu_0.99, Maximum Reward 190.773348262, Successful Episodes 6, Solved Episodes 0"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = _create_network() \n",
    "\n",
    "# Create model id\n",
    "EPOCH = '_'.join([repr(ALPHA), repr(STEP_MEM), str(NUM_HIDDEN_NEURON), INITIALIZATION, ACTIVATION, str(round(GAMMA, 2))])\n",
    "                                \n",
    "# Train model / Play epoch\n",
    "stats = dqn_1(model, render=True) \n",
    "\n",
    "# Calculate cumulative epoch statistics\n",
    "highest_reward = max([ v[0] for v in stats.values() ])\n",
    "success_episodes = sum([ v[1] for v in stats.values() ])\n",
    "solved_episodes = sum([ v[2] for v in stats.values() ])\n",
    "                                \n",
    "# Visualize\n",
    "stdout.write(\"\\rEpoch {}, Maximum Reward {}, Successful Episodes {}, Solved Episodes {}\".format(\\\n",
    "    EPOCH, highest_reward, success_episodes, solved_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is quite working nicely already! The frantic, purpose-less behaviour is gone most of the time, or is vanishing quickly within a few episodes only.\n",
    "\n",
    "What can learn from this basic implementation?\n",
    "\n",
    "- We could consider the model architecture:\n",
    "\n",
    "    - Does the weight initializations make sense? What are the alternatives?\n",
    "    - Does the nonlinear activation function for the hidden layer make sense? What are the alternatives? \n",
    "    - What about the number of neurons in the hidden layer?\n",
    "    \n",
    "We already step into the vast domain of parameter tuning for ANNs. \n",
    "\n",
    "We also should think of tuning the enviroment / model input. step memory ```STEP_MEM```: Does it make sense to let the agent know only its current state, or shall we allow him to take into consideration also some of the states before? If yes, how far back should it remember? \n",
    "\n",
    "We will explore parameter tuning in depth at the end of this notebook.\n",
    "\n",
    "#### Observations\n",
    "\n",
    "- Swinging movement: Most of the time, we obseve an extensive swinging movement. The agent tries to counter-act skewed positions by engaging the lateral engines, and overdoes it. Then, it corrects again, and again, and from all these corrections forgets to fire against the moon's gravity, and the agent is crashing into the surface.\n",
    "- Local minima. There are plenty of times one can see the agent trapped into a locally optimal policy. For example, it stays on the ground, engaging left and right engine forever, perfectly stable, but not reaching the ultimate goal. Or a setting where left and right engines are engaged, but the lower engine does not fire at all, over long episodes.\n",
    "\n",
    "### 2) *Epsilon Greedy* Deep Q Learning from Single Observations\n",
    "\n",
    "At this point, let us tackle the issue of getting stuck in local minima. As a remedy, Deep Q Learning makes use the so called *epsilon greedy* action selection policy. It allows for a random move with probability ```epsilon```, and by that introduces the notion of exploration (random moves) vs. exploitation (act on estimation of the q function). \n",
    "\n",
    "*Exploration* will reduce the probability of getting stuck in local minima, which are *not* reflecting the best action given a certain experience level of the agen. It's just like fresh air for the AI brain, introducing random ideas from outside. \n",
    "\n",
    "On the other hand, the agent needs to train and get experience with his selected moves. It needs evidence that one decision was (not) the right one, and to update its decicion finding process (the weights of the ANN. It only gets it by acting according to its own decisions undisturbed by random inputs. This is where *exploitation* comes in.\n",
    "\n",
    "In RL, usually ```epsilon``` decreases over a certain exploration period. This reflects the idea that the agent will start with many random moves to fathom the environment by just observing. With time and growing experience, it will decrease the share of random moves, since it feels more confident in its own decisions. \n",
    "\n",
    "I will follow th custom of allowing random moves at a linearly decreasing exploration rate during the exploration period. Following Deep Mind: Playing Atari with Deep Reinforcement Learning (p.6), we define a interval between the maximum and minimum ```epsilon``` allowed (```EPSILON_RANGE```). The exploration period (```NUM_EXPLORATION_STEP```) is set to roughly 1/10 of the total number of steps, which we only can estimate. Let's leap ahead one step: Assuming an average 100 steps per episode lets ```epsilon``` decrease until Episode 6. If we increase the average to 150, we end up with the desired 10 episodes until minimum ```epsilon```. \n",
    "\n",
    "During training, the agent will run on the minimum ```epsilon``` constantly.\n",
    "\n",
    "#### Extend Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPSILON_RANGE = [0.1, 0.0001]\n",
    "NUM_EXPLORATION_STEP = NUM_EPISODE * 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Training Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dqn_2(model, render=False): \n",
    "    ##NEW Initialize epsilon at its maximum value\n",
    "    epsilon = EPSILON_RANGE[0] \n",
    "    \n",
    "    # Init epoch stats\n",
    "    stats = {}\n",
    "\n",
    "    # Play through episodes\n",
    "    for episode in range(NUM_EPISODE): \n",
    "        # Init episode stats\n",
    "        stats[episode] = [0]*3\n",
    "        \n",
    "        # Init observations        \n",
    "        x_t = ENV.reset()\n",
    "        s_t = np.tile(x_t, STEP_MEM)\n",
    "        done = False\n",
    "\n",
    "        # Start an episode\n",
    "        for step in count():\n",
    "            \n",
    "            # Reset the environment for a new gaming step\n",
    "            if render: ENV.render()\n",
    "            \n",
    "            ###NEW Linarily anneal random exploration rate epsilon over exploration period            \n",
    "            epsilon = max(epsilon - (EPSILON_RANGE[0] - EPSILON_RANGE[1]) / NUM_EXPLORATION_STEP, EPSILON_RANGE[1])\n",
    "    \n",
    "            # Estimate q for each action at s_t\n",
    "            q = model.predict(s_t[np.newaxis])[0]\n",
    "\n",
    "            # Take action with highest estimated reward \n",
    "            ###NEW Do this with probability 1-epsilon (\"epsilon greedy\" policy)\n",
    "            a_t = np.argmax(q) if np.random.random() > epsilon else np.random.choice(ACTIONS, 1)[0]\n",
    "\n",
    "            # Observe after action a_t\n",
    "            x_t, r_t, done, info = ENV.step(a_t)\n",
    "        \n",
    "            # Create state at t1: Append x observations, throw away the earliest\n",
    "            s_t1 = np.concatenate((x_t, s_t[:(STEP_MEM-1) * NUM_INPUT,]), axis=0)\n",
    "\n",
    "            # Estimate q for each action at s_t1 (again a forward pass)\n",
    "            Q_sa = model.predict(s_t1[np.newaxis])[0]\n",
    "\n",
    "            ''' Create reference/targets by updating estimated reward for chosen action\n",
    "                For action taken, replace estimated reward by remaining cumulative lifetime reward\n",
    "            ''' \n",
    "            targets = q\n",
    "            targets[a_t] = r_t + GAMMA * np.max(Q_sa) if not done else r_t\n",
    "\n",
    "            ''' Learn!\n",
    "                - Again, predict q values for state s_t\n",
    "                - Calculate loss by comparing predictions to targets: they will differ only for the action taken\n",
    "                - backpropagate error for action taken, update weights\n",
    "            ''' \n",
    "            model.fit(s_t[np.newaxis], targets[np.newaxis], verbose=0)\n",
    "            \n",
    "            # Update statistics: Sum up rewards, count steps, successes, solved\n",
    "            stats[episode][0] += r_t\n",
    "            if r_t >= 100: stats[episode][1] += 1\n",
    "            if r_t >= 200: stats[episode][2] += 1\n",
    "            \n",
    "            # Visualize\n",
    "            stdout.write(\"\\r{} | Step {} @ Episode {}/{} | Epsilon {}\".format(\\\n",
    "                EPOCH, step, episode, NUM_EPISODE, epsilon))\n",
    "   \n",
    "            # Exit episode after crash or deadlock\n",
    "            if done or step > 10000: break            \n",
    "\n",
    "            # Update state\n",
    "            s_t = s_t1\n",
    "            \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10_1_200_glorot_uniform_relu_0.99, Maximum Reward 209.020101356, Successful Episodes 9, Solved Episodes 0"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = _create_network() \n",
    "\n",
    "# Create model id\n",
    "EPOCH = '_'.join([repr(ALPHA), repr(STEP_MEM), str(NUM_HIDDEN_NEURON), INITIALIZATION, ACTIVATION, str(round(GAMMA, 2))])\n",
    "                                \n",
    "# Train model / Play epoch\n",
    "stats = dqn_2(model, render=True) \n",
    "\n",
    "# Calculate cumulative epoch statistics\n",
    "highest_reward = max([ v[0] for v in stats.values() ])\n",
    "success_episodes = sum([ v[1] for v in stats.values() ])\n",
    "solved_episodes = sum([ v[2] for v in stats.values() ])\n",
    "                                \n",
    "# Visualize\n",
    "stdout.write(\"\\rEpoch {}, Maximum Reward {}, Successful Episodes {}, Solved Episodes {}\".format(\\\n",
    "    EPOCH, highest_reward, success_episodes, solved_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Deep Q Learning *from Stored Experiences* \n",
    "\n",
    "Deep Mind: Playing Atari with Deep Reinforcement Learning claims that \n",
    "\n",
    ">learning directly from consecutive samples is inefficient, due to the strong correlations between the samples\n",
    "\n",
    "(p.5). Instead, the trick is to learn from a memory storage, which I will call Experience Replay Memory (```ERM```), in batches. \n",
    "\n",
    "We are thus going to create the main database of the agent: It is the place where it \n",
    "- stores states and its experiences with the states (transitions ```s_t```, ```a_t```, ```r_t```, and ```s_t1```)\n",
    "- recalls on the memory, collects a memory sample, trains on the sample, and updates the ```q``` function estimator.\n",
    "\n",
    "The ```ERM``` is set up once per epoch and is fed at each step with fresh transition evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend Hyperparameters\n",
    "\n",
    "*ERM_SIZE* is setting the size of the experience replay memory. Following the recommendation of Deep Mind: Playing Atari with Deep Reinforcement Learning (p.6), we set it equal to the number of exploration steps. *BATCH_SIZE* denotes the size of the transition sample which is drawn uniformly without replacement from the *ERM* at each step. Again, its size is following the recommendations of Deep Mind: Playing Atari with Deep Reinforcement Learning (p.6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ERM_SIZE = NUM_EXPLORATION_STEP\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Training Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dqn_3(model, render=False):\n",
    "    # Init epsilon and ERM\n",
    "    epsilon = EPSILON_RANGE[0]\n",
    "    ERM = deque(maxlen=ERM_SIZE)\n",
    "    \n",
    "    # Init epoch stats\n",
    "    stats = {}\n",
    "    \n",
    "    # Play through episodes\n",
    "    for episode in range(NUM_EPISODE): \n",
    "        # Init episode stats\n",
    "        stats[episode] = [0]*3\n",
    "        \n",
    "        # Init observations        \n",
    "        x_t = ENV.reset()\n",
    "        s_t = np.tile(x_t, STEP_MEM)\n",
    "        done = False\n",
    "\n",
    "        # Start an episode\n",
    "        for step in count():\n",
    "            \n",
    "            # Reset the environment for a new gaming step\n",
    "            if render: ENV.render()\n",
    "            \n",
    "            # Linarily anneal random exploration rate epsilon over exploration period    \n",
    "            ###NEW Exploration starts only when ERM is complete\n",
    "            if len(ERM) < ERM_SIZE: epsilon = EPSILON_RANGE[0]\n",
    "            else: epsilon = max(epsilon - (EPSILON_RANGE[0] - EPSILON_RANGE[1]) / NUM_EXPLORATION_STEP, EPSILON_RANGE[1])\n",
    "    \n",
    "            # Estimate q for each action at s_t\n",
    "            q = model.predict(s_t[np.newaxis])[0]\n",
    "\n",
    "            # Take action with highest estimated reward with probability 1-epsilon (\"epsilon greedy\" policy)\n",
    "            a_t = np.argmax(q) if np.random.random() > epsilon else np.random.choice(ACTIONS, 1)[0]\n",
    "\n",
    "            # Observe after action a_t\n",
    "            x_t, r_t, done, info = ENV.step(a_t)\n",
    "        \n",
    "            # Create state at t1: Append x observations, throw away the earliest\n",
    "            s_t1 = np.concatenate((x_t, s_t[:(STEP_MEM-1) * NUM_INPUT,]), axis=0)\n",
    "            \n",
    "            ###NEW Store transition in experience replay memory\n",
    "            ERM.append((s_t, a_t, r_t, s_t1))\n",
    "\n",
    "            ###NEW Choose a batch of maximum length BATCH_SIZE\n",
    "            minibatch = np.array([ ERM[i] for i in np.random.choice(np.arange(0, len(ERM)), min(len(ERM), BATCH_SIZE)) ])\n",
    "            \n",
    "            ###NEW Compute targets/reference for each transition in minibatch\n",
    "            inputs = deque(); targets = deque()\n",
    "            for m in minibatch:\n",
    "                inputs.append(m[0]) # Append s_t of batch transition m to inputs\n",
    "                m_q = model.predict(m[0][np.newaxis])[0] # Estimate rewards for each action (targets), at s_t\n",
    "                m_Q_sa = model.predict(m[3][np.newaxis])[0] # Estimate rewards for each action (targets), at s_t1\n",
    "                m_targets = m_q\n",
    "                m_targets[m[1]] = m[2] + GAMMA * np.max(m_Q_sa)\n",
    "                targets.append(m_targets) # Append target of batch transition m to targets\n",
    "                \n",
    "            ###NEW Train the model by backpropagating the errors and update weights\n",
    "            model.train_on_batch(np.array(inputs), np.array(targets))\n",
    "            \n",
    "            # Update statistics: Sum up rewards, count steps, successes, solved\n",
    "            stats[episode][0] += r_t\n",
    "            if r_t >= 100: stats[episode][1] += 1\n",
    "            if r_t >= 200: stats[episode][2] += 1\n",
    "            \n",
    "            # Visualize\n",
    "            stdout.write(\"\\r{} | Step {} @ Episode {}/{} | Epsilon {}\".format(\\\n",
    "                EPOCH, step, episode, NUM_EPISODE, epsilon))\n",
    "            \n",
    "            # Exit episode after crash or deadlock\n",
    "            if done or step > 10000: break           \n",
    "            \n",
    "            # Update state\n",
    "            s_t = s_t1  \n",
    "            \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10_1_200_glorot_uniform_relu_0.99, Maximum Reward -30.5726153344, Successful Episodes 4, Solved Episodes 0"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = _create_network() \n",
    "\n",
    "# Create model id\n",
    "EPOCH = '_'.join([repr(ALPHA), repr(STEP_MEM), str(NUM_HIDDEN_NEURON), INITIALIZATION, ACTIVATION, str(round(GAMMA, 2))])\n",
    "                                \n",
    "# Train model / Play epoch\n",
    "stats = dqn_3(model, render=True) \n",
    "\n",
    "# Calculate cumulative epoch statistics\n",
    "highest_reward = max([ v[0] for v in stats.values() ])\n",
    "success_episodes = sum([ v[1] for v in stats.values() ])\n",
    "solved_episodes = sum([ v[2] for v in stats.values() ])\n",
    "                                \n",
    "# Visualize\n",
    "stdout.write(\"\\rEpoch {}, Maximum Reward {}, Successful Episodes {}, Solved Episodes {}\".format(\\\n",
    "    EPOCH, highest_reward, success_episodes, solved_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "Learning from stored experiences clearly needs more time!\n",
    "\n",
    "The question crossed my mind: Why don't we predict beforehand, on the fly, at every step? Would that not be computationally efficient? \n",
    "\n",
    "No: It has the huge disadvantage that we predict with the knowledge available at step ```t```. This might be faulty, and the faulty prediction stays as target reference in the batch, and is used to compare the loss for the taken action between the prediction at timestep t and the potenitially long ago target estimation. This will bias the learning process significantly. Thus, we select a batch, calculate the estimations and targets for the complete batch, both with the knowledge of the current steps.\n",
    "\n",
    "### 4) Exploring Parametrizations\n",
    "\n",
    "For the following parameter space exploration, I switched to a local Windows server as well as AWS EC2. The goal is to identify well-working parameter settings, in order to run them afterwards on a intense scale again. \n",
    "\n",
    "The local machine is a Windows Server 2008 R2 Enterprise with a quad core Intel Xeon CPU E5-2680 @ 2.7 GHz and 32 GB RAM. For the cloud computing part, I made my first steps on Ubuntu 16.04 t2.micro free tier instances, and then switch to a Ubuntu 16.04 c4.8xlarge instance. It took me a while to get aquainted with the AWS UX, to set up an IAM user, a Security Group, all the dependencies for this project, and the notebook server, but of course the speed at hand makes up for it easily. \n",
    "\n",
    "At this point, I originally introduced more parameters:\n",
    "\n",
    "- Steps per Action (```SPA```) determines how many steps pass before a fresh action is taken. Deep Mind - Playing Atari with Deep Reinforcement Learning suggests 4 skipping 4 time steps before conducting an action (p.6): ```if step % SPA == 0: a_t = np.argmax(q) if np.random.random() > epsilon else np.random.choice(ACTIONS, 1)[0]```\n",
    "\n",
    "- Reward Clipping (```R_CLIP```) is also an idea taken from Deep Mind - Playing Atari with Deep Reinforcement Learning (p.6), where it was implemented in order to make rewards comparable across the different games: ```if R_CLIP and r_t != 0: r_t = abs(r_t) / r_t```\n",
    "\n",
    "I also wanted to loop over all possible Keras activations and initializations, as well as trying out different ```GAMMA```. I even thought about using different *EPSILON_RANGE*. With that done, I soon realized that the parameter space was truly exploding. Regardless the platform I was running the parameter combination on, it took an extremely long time. \n",
    "\n",
    "So I stepped back. Parameter space exploration will be done on ranges of ```ALPHA```, ```NUM_HIDDEN_NEURON```, and ```STEP_MEM``` only. ```EPSILON_RANGE``` starts at 20% instead of 10%.\n",
    "\n",
    "We will also write down working parameter settings to a home directory subfolder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from os import path, makedirs\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend Hyperparameters\n",
    "\n",
    "- Step Memory ```STEP_MEM```. We replace the fixed hyperparameter by an arbitrary range of ```STEP_MEM``` candidates (```STEP_MEM_RANGE```)\n",
    "- ```SAVE_PATH``` is the working directory where the training epoch models are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_EPISODE = 150\n",
    "NUM_EXPLORATION_STEP = NUM_EPISODE * 15\n",
    "EPSILON_RANGE = [0.2, 0.01] \n",
    "ALPHA_RANGE =  [0.01, 0.1, 1, 10, 100] # Constant over one epoch ##0.01, \n",
    "NUM_HIDDEN_NEURON_RANGE = [100, 200, 300, 400, 500, 600] # Constant over one epoch\n",
    "STEP_MEM_RANGE = np.arange(1, 8) # Constant over one epoch\n",
    "\n",
    "SAVE_PATH = path.join(path.expanduser(\"~\"), \"RL_Saves\")\n",
    "try: makedirs(SAVE_PATH)\n",
    "except OSError: \n",
    "    if not path.isdir(SAVE_PATH): raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Init epoch overview\n",
    "epoch_account = {}\n",
    "\n",
    "# Set output file\n",
    "dqn_stats_file = path.join(SAVE_PATH, \"DQN_Stats.json\")\n",
    "\n",
    "# Apply brute force parameter space exploration\n",
    "for ALPHA in ALPHA_RANGE:\n",
    "    for STEP_MEM in STEP_MEM_RANGE:\n",
    "        for NUM_HIDDEN_NEURON in NUM_HIDDEN_NEURON_RANGE:\n",
    "            # Initialize model\n",
    "            try: model = _create_network() \n",
    "            except Exception: continue\n",
    "\n",
    "            # Create model id\n",
    "            EPOCH = '_'.join([repr(ALPHA), repr(STEP_MEM), str(NUM_HIDDEN_NEURON), INITIALIZATION, ACTIVATION, str(round(GAMMA, 2))])\n",
    "\n",
    "            # Train model / Play epoch\n",
    "            stats = dqn_3(model) # , render=True\n",
    "\n",
    "            # Calculate cumulative epoch statistics\n",
    "            highest_reward = max([ v[0] for v in stats.values() ])\n",
    "            success_episodes = sum([ v[1] for v in stats.values() ])\n",
    "            solved_episodes = sum([ v[2] for v in stats.values() ])\n",
    "\n",
    "            # Visualize\n",
    "            stdout.write(\"\\rEpoch {}, Maximum Reward {}, Successful Episodes {}, Solved Episodes {}\".format(\\\n",
    "                EPOCH, highest_reward, success_episodes, solved_episodes))\n",
    "\n",
    "            ###NEW Write down working settings\n",
    "            if success_episodes > 0: \n",
    "                epoch_account[EPOCH] = [highest_reward, success_episodes, solved_episodes]\n",
    "                with open(dqn_stats_file, \"a\") as outfile: json.dump(epoch_account, outfile)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "When trying to ```json.load``` the resulting ```.json``` file, I realize that I made a mistake when saving the results in this form: I am not having a single json, but a multitude of them. Since I do not want to loose the results and rerun everything, I need to implement a function in order to extract the results properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def jsonToDict(jsonFile):\n",
    "    # Open file\n",
    "    with open(jsonFile, 'r') as f: contents = f.read()\n",
    "    \n",
    "    # Produce json list\n",
    "    records = []\n",
    "    while True:\n",
    "        start = contents.find('{')\n",
    "        end = contents.find('}')\n",
    "        records.append(contents[start:end])       \n",
    "        contents = contents[end+1:]\n",
    "        if end < 0: break\n",
    "    \n",
    "    # Produce intermediate dictionary\n",
    "    intermediate_stats = {}\n",
    "    # Loop through json list\n",
    "    for r in records:\n",
    "        start = r.find('\"')\n",
    "        end = r[start+1:].find('\"')\n",
    "        key =  r[start+1:end+2] \n",
    "        # Re-organize key-value pairs in case of valid key\n",
    "        if len(key) > 0:\n",
    "            valueRaw = r[r.find('[')+1:r.find(']')].split(',')\n",
    "            valueList = [ float(vf) for vf in [ vr.strip() for vr in valueRaw ] if len(vf) > 0 ]\n",
    "            try: \n",
    "                if valueList not in intermediate_stats[key]: intermediate_stats[key].append(valueList) \n",
    "            except KeyError: intermediate_stats[key] = [valueList]\n",
    "    \n",
    "    # Produce final dictionary with summary metrics for every key\n",
    "    stats = {}\n",
    "    for k, v in intermediate_stats.iteritems():\n",
    "        num_trials = len(v)\n",
    "        avg_reward = sum([ subv[0] for subv in v ]) / num_trials\n",
    "        avg_success = sum([ subv[1] for subv in v ]) / num_trials\n",
    "        sum_solved = int(sum([ subv[2] for subv in v ]))\n",
    "        stats[k] = [avg_reward, avg_success, sum_solved, num_trials]\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify working parameter settings\n",
    "\n",
    "Now we are ready to find parameter settings where the problem was solved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01_6_200_glorot_uniform_relu_0.99 [186.83053538728421, 13.0, 1, 2]\n",
      "0.01_6_400_glorot_uniform_relu_0.99 [121.41997221106126, 17.0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "for k, v in jsonToDict(dqn_stats_file).iteritems():\n",
    "    if v[2] > 0: print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, an learning rate of 0.01 in combination with a memory of 6 frames leads to very good results. We just need to decide if we choose a hidden layer of 200 or 400.\n",
    "\n",
    "What remains to be done is a long-term trial TODO\n",
    "\n",
    "save weights in dqn3 with \n",
    "\n",
    "```model.save_weights(path.join(SAVE_PATH, \"DQN_Model.h5\", overwrite=True), model.load_weights(\"model.h5\"), model.compile()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Algorithms 2 & 3: Policy Gradients\n",
    "# Advantage Actor-Critic Policy Gradients\n",
    "# Q Actor-Critic Policy Gradients\n",
    "\n",
    "## A Methododical Overview\n",
    "\n",
    "As we have seen extensively so far, the agent faces the decision of taking one of the available actions at every step ```t```. When applying a policy gradient model, the agent estimates which move is most likely to lead to a good end / a high reward: It assigns probabilities to every action. In order to accomplish this, we need a first ANN block, which outputs probabilities / uses a softmax at the output layer. It is called the *policy estimator*.\n",
    "\n",
    "After the estimation, it samples uniformly from the probability distribution in order to get an action, and observes the consequences of the action. \n",
    "\n",
    "The implemented advantage actor-critic policy gradient model updates the policy in batch mode after having terminated an episode. It so to speak does a retrospection on a completed episode, with ex-post knowledge of the episode's outcome and all its moves and direct consequences.\n",
    "\n",
    "The agent therefore needs to keep track of transition ```s_t, a_t, r_t``` as well as the probability of the action taken ```probs[a_t]``` for every step ```t``` the probability during a completed episode. \n",
    "\n",
    "After having finished, the agent takes this episode experience and walks through it again step by step, computing the relevant metrics at every step ```t```.\n",
    "\n",
    "Policy gradient models operate with non-standard gradients. In general, we need a ```target``` and a ```baseline``` in order to compute the so called ```advantage```. \n",
    "\n",
    "The ```target``` component differs for the different types of policy gradient models: for advantage actor-critic models, it is the cumulated, discounted (again, with discount factor ```GAMMA```) reward for the remaining steps, at step ```t```. Computing this ex post is not difficult!\n",
    "\n",
    "The ```baseline``` is the estimate of is *estimate* of the cumulated, discounted reward for the remaining steps, at step ```t```. For this, we need a second ANN block, the so called *value estimator*, outputting a regression estimate by applying a mean squared error loss function.\n",
    "\n",
    "The ```advantage``` is nothing more than the absolute difference of the ```target``` to its estimation, the ```baseline```.  \n",
    "\n",
    "The loss at ```t``` eventually is the cross-entropy, given the probability of the action taken (```probs[a_t]```), and the ```advantage``` at ```t```.\n",
    "\n",
    "In order to the model on this loss in Keras, I needed to implement a helper function ```get_trainable_params```, as proposed by the helpful minds at https://github.com/fchollet/keras/issues/3062. A big thank you for that! Getting the gradients subsequently is relying directly on TensorFlow, applying ```tf.gradients()```.\n",
    "\n",
    "Having said all this, it is time to implement the model and testing routines TODO:\n",
    "\n",
    "Q ACTOR CRITIC:\n",
    "\n",
    "TODO explanation, difference to Advantage: on the fly, baseline with q \n",
    "\n",
    "!!!!!! model_v muss dasselbe sein wie bei dqn. Tatsächlich? Kann ja mal das andere ausprobieren. Ansprechen bei dqn.\n",
    "\n",
    "#### Extend Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.engine import training\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend Hyperparameters\n",
    "\n",
    "- TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "INITIALIZATIONS_V = ['uniform', 'lecun_uniform', 'normal', 'identity', 'orthogonal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform'] # Constant over one epoch \n",
    "ACTIVATIONS_V = ['softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear'] # Constant over one epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Keras Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _create_network(model_type): \n",
    "    model = Sequential()\n",
    "    \n",
    "    if model_type == 'value':\n",
    "        # Regression function estimate for calculating the advantage\n",
    "        model.add(Dense(NUM_HIDDEN_NEURON, init=INITIALIZATION_V, input_dim=STEP_MEM*NUM_INPUT)) \n",
    "        model.add(Activation(ACTIVATION_V))\n",
    "        model.add(Dense(1, init=INITIALIZATION_V))\n",
    "        model.compile(optimizer='rmsprop', loss='mse')\n",
    "    \n",
    "    if model_type == 'policy':\n",
    "        model.add(Dense(NUM_HIDDEN_NEURON, init=INITIALIZATION_P, input_dim=STEP_MEM*NUM_INPUT)) \n",
    "        model.add(Activation(ACTIVATION_P))\n",
    "        model.add(Dense(NUM_ACTION, init=INITIALIZATION_P))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Helper Function for Non-Standard Gradient Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_trainable_params(model):\n",
    "    params = []\n",
    "    for layer in model.layers:\n",
    "        params += training.collect_trainable_weights(layer)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Training Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def advantage_actor_critic(model_v, model_p, render=False): \n",
    "    # Init epoch stats\n",
    "    stats = {}\n",
    "    \n",
    "    # Play through episodes\n",
    "    for episode in range(NUM_EPISODE): \n",
    "        # Init episode transitions & stats\n",
    "        transitions = []\n",
    "        stats[episode] = [0]*3\n",
    "        \n",
    "        # Init observations\n",
    "        x_t = ENV.reset()\n",
    "        s_t = np.tile(x_t, STEP_MEM)\n",
    "        done = False\n",
    "\n",
    "        # Start an episode\n",
    "        for step in count():\n",
    "            \n",
    "            # Reset the environment for a new gaming step\n",
    "            if render: ENV.render()\n",
    "            \n",
    "            # Estimate probabilities for each possible action\n",
    "            probs = model_p.predict(s_t[np.newaxis])[0]\n",
    "            \n",
    "            # Take an action: Sample an action from the probabilities distribution\n",
    "            a_t = np.random.choice(ACTIONS, p=probs)\n",
    "            \n",
    "            # Observe after action a_t\n",
    "            x_t, r_t, done, info = ENV.step(a_t)\n",
    "            \n",
    "            # Create state at t1: Append x observations, throw away the earliest\n",
    "            s_t1 = np.concatenate((x_t, s_t[:(STEP_MEM-1) * NUM_INPUT,]), axis=0)\n",
    "            \n",
    "            # Keep track of the episode transition and the probability of the action taken\n",
    "            transitions.append((s_t, a_t, r_t, probs[a_t]))\n",
    "            \n",
    "            # Update statistics: Sum up rewards, count steps, successes, solved\n",
    "            stats[episode][0] += r_t\n",
    "            if r_t >= 100: stats[episode][1] += 1\n",
    "            if r_t >= 200: stats[episode][2] += 1\n",
    "                \n",
    "            # Visualize\n",
    "            stdout.write(\"\\rAAC | {} | Step {} @ Episode {}/{}\".format(\\\n",
    "                EPOCH, step, episode, NUM_EPISODE))\n",
    "\n",
    "            # Exit episode after crash or deadlock\n",
    "            if done or step > 10000: break \n",
    "            \n",
    "            # Update state\n",
    "            s_t = s_t1\n",
    "            \n",
    "        # Update policy after each episode \n",
    "        for t, transition in enumerate(transitions):\n",
    "        \n",
    "            # Calculate target for action taken\n",
    "            # -> Total discounted reward after this step during episode\n",
    "            target = sum(GAMMA**i * j[2] for i, j in enumerate(transitions[t:])) \n",
    "\n",
    "            # Update value estimator\n",
    "            model_v.fit(transition[0][np.newaxis], np.asarray([target]), verbose=0)\n",
    "\n",
    "            # Estimate baseline for action taken\n",
    "            baseline = model_v.predict(transition[0][np.newaxis])[0][0]\n",
    "\n",
    "            # Calculate advantage for action taken\n",
    "            advantage = target - baseline\n",
    "\n",
    "            # Caluculate loss\n",
    "            loss = -np.log(transition[3]) * advantage\n",
    "            \n",
    "            # Update policy estimator\n",
    "            network_params = get_trainable_params(model_p)\n",
    "            param_grad = tf.gradients(loss, network_params)\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q_actor_critic(model_v, model_p, render=False): \n",
    "    # Init epoch stats\n",
    "    stats = {}\n",
    "\n",
    "    # Play through episodes\n",
    "    for episode in range(NUM_EPISODE): \n",
    "        # Init episode stats\n",
    "        stats[episode] = [0]*3\n",
    "        \n",
    "        # Init observations\n",
    "        x_t = ENV.reset()\n",
    "        s_t = np.tile(x_t, STEP_MEM)\n",
    "        done = False\n",
    "        \n",
    "        # Start an episode\n",
    "        for step in count():\n",
    "            \n",
    "            # Reset the environment for a new gaming step\n",
    "            if render: ENV.render()\n",
    "\n",
    "            # Estimate probabilities for each possible action\n",
    "            probs = model_p.predict(s_t[np.newaxis])[0]\n",
    "            \n",
    "            # Take an action: Sample an action from the returned probabilities distribution\n",
    "            a_t = np.random.choice(ACTIONS, p=probs)\n",
    "\n",
    "            # Observe after action a_t\n",
    "            x_t, r_t, done, info = ENV.step(a_t)\n",
    "            \n",
    "            # Create state at t1: Append x observations, throw away the earliest\n",
    "            s_t1 = np.concatenate((x_t, s_t[:(STEP_MEM-1) * NUM_INPUT,]), axis=0)\n",
    "\n",
    "            # Calculate target for action taken\n",
    "            # -> Reward for action taken at s_t1\n",
    "            Q_sa = model_v.predict(s_t1[np.newaxis])[0][0]\n",
    "            target = r_t + GAMMA * Q_sa # if not done else r_t ###TODO reinnehmen?\n",
    "\n",
    "            # Update value estimator\n",
    "            model_v.fit(s_t[np.newaxis], np.asarray([target]), verbose=0)\n",
    "\n",
    "            # Estimate baseline for action taken\n",
    "            baseline = model_v.predict(s_t[np.newaxis])[0][0]\n",
    "\n",
    "            # Calculate advantage for action taken\n",
    "            advantage = target - baseline\n",
    "\n",
    "            # Caluculate loss\n",
    "            loss = -np.log(probs[a_t]) * advantage\n",
    "\n",
    "            # Update policy estimator\n",
    "            network_params = get_trainable_params(model_p)\n",
    "            param_grad = tf.gradients(loss, network_params)\n",
    "            \n",
    "            # Update statistics: Sum up rewards, count steps, successes, solved\n",
    "            stats[episode][0] += r_t\n",
    "            if r_t >= 100: stats[episode][1] += 1\n",
    "            if r_t >= 200: stats[episode][2] += 1\n",
    "\n",
    "            # Visualize\n",
    "            stdout.write(\"\\rQAC | {} | Step {} @ Episode {}/{}\".format(\\\n",
    "                EPOCH, step, episode, NUM_EPISODE))\n",
    "\n",
    "            # Exit episode after crash or deadlock\n",
    "            if done or step > 10000: break \n",
    "\n",
    "            # Update state\n",
    "            s_t = s_t1\n",
    "            \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model\n",
    "\n",
    "TODO Cannot be tested on Win server, since TensorFlow presently cannot be run on Windows\n",
    "\n",
    "TODO use same param for policy models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Init epoch overview\n",
    "epoch_account = {}\n",
    "\n",
    "# Set output files\n",
    "stats_files = [path.join(SAVE_PATH, \"AAC_Stats.json\"), path.join(SAVE_PATH, \"QAC_Stats.json\")]\n",
    "\n",
    "# Set policy estimator parameters equal to already known parameters\n",
    "INITIALIZATION_P = INITIALIZATION\n",
    "ACTIVATION_P = ACTIVATION\n",
    "\n",
    "# Apply brute force parameter space exploration\n",
    "for STEP_MEM in STEP_MEM_RANGE:\n",
    "    for NUM_HIDDEN_NEURON in NUM_HIDDEN_NEURON_RANGE:\n",
    "        for INITIALIZATION_V in INITIALIZATIONS_V:\n",
    "            for ACTIVATION_V in ACTIVATIONS_V:\n",
    "                # Initialize models\n",
    "                try: model_v, model_p = _create_network('value'), _create_network('policy') \n",
    "                except Exception: raise\n",
    "\n",
    "                # Create model id\n",
    "                EPOCH = '_'.join([repr(STEP_MEM), str(NUM_HIDDEN_NEURON), INITIALIZATION_V, INITIALIZATION, ACTIVATION_V, ACTIVATION])\n",
    "\n",
    "                # Train model / Play epoch\n",
    "                double_stats = (advantage_actor_critic(model_v, model_p), q_actor_critic(model_v, model_p))\n",
    "\n",
    "                for i, stats in enumerate(double_stats):\n",
    "                    # Calculate cumulative epoch statistics\n",
    "                    highest_reward = max([ v[0] for v in stats.values() ])\n",
    "                    success_episodes = sum([ v[1] for v in stats.values() ])\n",
    "                    solved_episodes = sum([ v[2] for v in stats.values() ])\n",
    "\n",
    "                    # Visualize\n",
    "                    stdout.write(\"\\rEpoch {}, Maximum Reward {}, Successful Episodes {}, Solved Episodes {}\".format(\\\n",
    "                        EPOCH, highest_reward, success_episodes, solved_episodes))\n",
    "                    \n",
    "                    # Write down working settings\n",
    "                    if success_episodes > 0: \n",
    "                        epoch_account[EPOCH] = [highest_reward, success_episodes, solved_episodes]\n",
    "                        with open(stats_files[i], \"a\") as outfile: json.dump(epoch_account, outfile)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify working parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for stats_file in stats_files:\n",
    "    for k, v in jsonToDict(stats_file).iteritems():\n",
    "        if v[2] > 0: print(k, v)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
