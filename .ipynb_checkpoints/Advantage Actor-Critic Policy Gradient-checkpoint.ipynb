{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-10-30 11:37:11,579] Making new env: LunarLander-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'dense_input_6:0' shape=(?, 8) dtype=float32>]\n",
      "[<tf.Tensor 'add_27:0' shape=(?, 1) dtype=float32>]\n",
      "['loss']\n",
      "[<tf.Tensor 'dense_input_5:0' shape=(?, 8) dtype=float32>]\n",
      "[<tf.Tensor 'Softmax_2:0' shape=(?, 4) dtype=float32>]\n",
      "['loss', 'acc']\n",
      "[ 0.27455807  0.20541067  0.24454735  0.27548391] 1.0\n",
      "[ 0.26797563  0.08285315  0.3405714   0.30859989] 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "np.random.seed(3293734)\n",
    "\n",
    "ENV = gym.make(\"LunarLander-v2\")\n",
    "INPUT_DIM = ENV.reset().shape[0]\n",
    "N_ACTIONS = ENV.action_space.n\n",
    "ACTIONS = np.arange(0, N_ACTIONS)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.engine import training ###TODO extension\n",
    "import tensorflow as tf ###TODO extension\n",
    "\n",
    "from sys import stdout\n",
    "\n",
    "def _create_network_pol():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(HIDDEN_DIM, init='glorot_normal', input_dim=D*INPUT_DIM))  # input_shape=(D*INPUT_DIM,)\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(N_ACTIONS, init='glorot_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) # categorical_crossentropy\n",
    "    return model\n",
    "\n",
    "# Regression function estimate for calculating the baseline/advantage\t\n",
    "def _create_network_val():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, init='glorot_uniform', input_dim=D*INPUT_DIM)) # input_shape=(D*INPUT_DIM,)\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(1, init='glorot_uniform')) # input_shape=(D*INPUT_DIM,)\n",
    "    model.compile(loss='mse', optimizer='rmsprop') # Adam(lr=1e-6)\n",
    "    return model\n",
    "\n",
    "# https://github.com/fchollet/keras/issues/3062\n",
    "def get_trainable_params(model):\n",
    "    params = []\n",
    "    for layer in model.layers:\n",
    "        params += training.collect_trainable_weights(layer)\n",
    "    return params\n",
    "\n",
    "D = 1\n",
    "HIDDEN_DIM = 200\n",
    "model_pol = _create_network_pol()\n",
    "model_val = _create_network_val()\n",
    "\n",
    "print model_val.inputs\n",
    "print model_val.outputs\n",
    "print model_val.metrics_names\n",
    "\n",
    "print model_pol.inputs\n",
    "print model_pol.outputs\n",
    "print model_pol.metrics_names\n",
    "\n",
    "\n",
    "for episode in range(50):\n",
    "\n",
    "    # Instantiate interactions list\n",
    "    interactions = list()\n",
    "    discount_factor=0.99 # TODO parametrize brute force\n",
    "\n",
    "    s_t = ENV.reset()  # TODO stack\n",
    "    done = False\n",
    "\n",
    "    # Start an episode\n",
    "    while not done:\n",
    "        ###ENV.render()\n",
    "        probs = model_pol.predict(s_t[np.newaxis])[0]\n",
    "        # Take an action: Sample an action from the returned probabilities distribution\n",
    "        a_t = np.random.choice(ACTIONS, p=probs)\n",
    "        ###stdout.write('\\r'+str(a_t))\n",
    "        ###stdout.flush\n",
    "\n",
    "        # step the environment and get new measurements\n",
    "        s_t1, r_t, done, info = ENV.step(a_t)\n",
    "\n",
    "        # Keep track of the transition and the probability of the action taken\n",
    "        interactions.append((s_t, a_t, r_t, probs[a_t], probs))\n",
    "\n",
    "        # Update state\n",
    "        s_t = s_t1\n",
    "\n",
    "    # Ex post: Go through the episode and make policy updates\n",
    "    for t, transition in enumerate(interactions):\n",
    "        \n",
    "        # The return after this timestep (Discount reward as of frame n of game)\n",
    "        total_return = sum(discount_factor**i * j[2] for i, j in enumerate(interactions[t:])) \n",
    "        \n",
    "        # Get state at t, ex post\n",
    "        ep_s_t = transition[0][np.newaxis]\n",
    "        \n",
    "        # Update value estimator\n",
    "        model_val.fit(ep_s_t, np.asarray([total_return]), nb_epoch=10, verbose=0) # TODO nb_epoch=10\n",
    "        \n",
    "        # Calculate baseline FOR THE PICKED ACTION ONLY (Regression task)\n",
    "        baseline_value = model_val.predict(ep_s_t)[0][0]\n",
    "        \n",
    "        # Calculate target / y_true: advantage for the picked action\n",
    "        target = total_return - baseline_value\n",
    "        \n",
    "        # Update policy estimator\n",
    "        # Define the loss \n",
    "        #self.loss = -tf.log(self.picked_action_prob) * self.target\n",
    "        loss = -np.log(transition[3]) * target \n",
    "        # Compute the gradients\n",
    "        network_params = get_trainable_params(model_pol)\n",
    "        param_grad = tf.gradients(loss, network_params)\n",
    "        # (cannot use standard model.fit keras)\n",
    "        # https://github.com/fchollet/keras/issues/3062 in references on top\n",
    "        ###estimator_policy.update(transition.state, advantage, transition.action)\n",
    "        \n",
    "    print transition[4], np.sum(np.array(transition[4]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
