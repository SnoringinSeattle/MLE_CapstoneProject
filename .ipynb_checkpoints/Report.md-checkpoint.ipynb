{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Project\n",
    "Matthias Wettstein  \n",
    "December 17th, 2016\n",
    "\n",
    "\n",
    "# Reinforcement Learning: Implementing and Comparing three Algorithms\n",
    "\n",
    "\n",
    "## I. Definition\n",
    "\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "Imagine living a complex world, where a myriad of different situations can occur, where the interaction of events are numberless and unpredictable. In this world, we human beings need to adapt, to learn, and to find a way to persue our development and secure self-preservation. In other words, we need to make use of the overwhelming amount of information around us to forge a strategy on how to survive or, at best, to thrive.\n",
    "\n",
    "This is a non-trivial task, since we are forced to include ever new situations when we act. We might know similar situations from previous experience, but hardly exactly the same situation twice (just think of the film \"Groundhog's Day\"). \n",
    "\n",
    "The strategy is then to deduce, induce, take the next best fitting experience, and, most importantly, after having acted, to observe the consequences and *update our experience according to the consequences*. The consequences might not occur right away after an action taken, it may take a long while, and yet we must be able to track a consequence back to a certain action / decision we took beforehand. \n",
    "\n",
    "What we humans do is applicable for machines too. Back in the days (and still), machines have been programmed determinically, i.e., for a finite set of situations, an appropriate action was hard coded. But machine can be brought to learn too, without a pre-defined plethora of \"if - then\" statements, but with the ability to react to changing environments.\n",
    "\n",
    "This is the domain of *Reinforcement Learning (RL)*, which again is a domain of Machine Learning. It embraces a range of methods on how to train an agent to act according to inputs available and a long-term objective. The agent is acting within a chain of decision problems, a so called *Markov Decision Process (MDP)*.\n",
    "\n",
    "This project is about to learn an agent how to learn is such chain of events and actions. The agent for this project is a toy robot called the *Lunar Lander*. \n",
    "\n",
    "Rocket trajectory optimization is a classic topic in Optimal Control. The Lunar Lander appeared as early as 1969 in the hype around Neil Armstrong and the race for the mooon, and starred several games since then. The object of the game is to land a lunar lander savely on the moon surface. In order to obtain this, the player needs to manoeuvre with propulsion properly, or else the lander crashes on the moon. Due to the weak gravitational forces the module is only slowly accelerated, and despite of the vacuum on the moon, the game is including a slowing air resistance.\n",
    "\n",
    "In this project, there will be no player, but only the lander acting as a autonomous agent, taking in inputs, acting, and learning from experience.\n",
    "\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "This project aims to sucessfully train the Lunar Lander toy agent to autonomously land on the toy moon. This serves as a toy example, ready to be extended to real-life tasks, and as a starter for my research on this area.\n",
    "\n",
    "The toy environment is provided by Elon Musk's [Open AI Gym](https://gym.openai.com/). It made available the lunar lander module for training Reinforcement Algorithms. It uses a 2D engine in the background (Box2D) in order to simulate the physics in the vicinity of the moon surface. This will be the agent's environment throughout the project.\n",
    "\n",
    "The project is designed as a storyline, developing and comparing four algorithms:\n",
    "\n",
    "- Algorithm 1 is the standard *Q Learning* algorithm. I hereby wrap up the lessons learned from the *Smartcab* section of the Machine Learning Engineer Nanodegree by applying them onto the Lunar Lander.\n",
    "- Algorithm 2 is *Deep Q Learning (DQN)*, which I will - for reasons reflecting my own learning on the subject - is developed step-by-step away from standard Q Learning up to a fully fledged DQN.\n",
    "- Finally, I will present two Policy Gradient algorithms \n",
    "    - Monte-Carlo Policy Gradients\n",
    "    - Action-Value Actor-Critic Policy Gradients\n",
    "\n",
    "To take the step from algorithm 1 to algorithm 2, I let myself go astray and develop an tiny *Artifical Neural Networks (ANN)*. This again is on account to reflect my learning on the subject.\n",
    "\n",
    "The algorithms need to solve two paramount problems:\n",
    "\n",
    "- Upon what experience / model the agent should take an action? Or: How should the agent determine which action might be beneficial given the circumstances?\n",
    "- How should the agent update the experience / model after an action, in order to make it more accurate when assessing future circumstances?\n",
    "   \n",
    "The algorithms are presented in a standardized, comparable way, in order to make detection of differences and similarities among them easier. \n",
    "\n",
    "They will be trained and benchmarked against one another by clearly defined metrics, with help of non-exhaustive parameter space exploration. \n",
    "\n",
    "Since this is computationally extremely expensive, this project is not only about the algorithms, but also about on *where* to implement them. *High Performance Computing (HPC) on the Cloud* will therefore be discussed too in this project.\n",
    "\n",
    "Please note that a significant share of detailed information is included as comments directly in the code.\n",
    "\n",
    "\n",
    "## II. Analysis\n",
    "\n",
    "\n",
    "### Data Exploration and Metrics\n",
    "\n",
    "A Markov Decision Process over a sequence of steps `t` is characterized by \n",
    "\n",
    "- a set of states `s` an agent can be faced with\n",
    "- a set of actions `a` an agent can take\n",
    "- a set of new states `s'` an agent can end up after having taken an action\n",
    "- a reward / punishment `r` an agent can face after having taken an action\n",
    "- a transition probability `T`/ policy, which indicates the agent which action to take, and which it has to learn\n",
    "\n",
    "A full MDP is called an *Episode*. An episode is a period of steps t during which the agent is in play, i.e. has not yet crashed or succeeded to land properly.\n",
    "\n",
    "An arbitrary number of subsequent Episodes is called an *Epoch*. It is the main block of training for an AI agent.\n",
    "\n",
    "Open AI Gym is taking care of state space (called *observations*), the action space, and the rewards. What is left is to learn a well suited transition policy.\n",
    "\n",
    "The step interaction between environment and agent boils down to the following single code line:\n",
    "\n",
    "    # Observe after action a_t\n",
    "    x_t, r_t, done, info = ENV.step(a_t)\n",
    "\n",
    "where `ENV` is an instance of the Open AI Gym Lunar Lander environment (see below for details [Preparing an Environment]), `x_t` is a state `s'` obtained by action `a_t`, and `r_t` is the associated reward for the transition. `done` is an indication if the Episode has ended, and `info` will be neglected, since not relevant for the environment.\n",
    "\n",
    "The result of above-mentioned step is e.g.:\n",
    "\n",
    "    array([-0.0051157 ,  0.93505154, -0.26596503, -0.19845303,  \n",
    "            0.00819082, 0.10553619,  0. ,  0. ])\n",
    "            , -2.7344571275052219, False, {}\n",
    "\n",
    "- The state space is a 1D vector of 8-digit floating numbers, whereas the coordinates w.r.t. the moon are the first two numbers in the state vector.\n",
    "- The reward is described as follows in Open AI documentation:\n",
    "> Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points.\n",
    "- done results in a boolean, `True` or `False`\n",
    "\n",
    "The agent has four different actions at its disposition:\n",
    "- Fire the main engine, which principally counteracts gravity\n",
    "- Fire the left engine in order to correct insufficient angles\n",
    "- Fire the right engine, likewise\n",
    "- Do not fire at all\n",
    "\n",
    "All four firing actions are discrete, i.e. fire engine at full throttle or do not fire at all. Again, after the documentation:\n",
    "> According to Pontryagin's maximum principle it's optimal to fire engine full throttle or turn it off. That's the reason this environment is OK to have discreet actions (engine on or off).\n",
    "\n",
    "The docs are indicating a few other points worth considering:\n",
    ">Landing pad is always at coordinates (0,0). (...) Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine.\n",
    "\n",
    "\n",
    "### Exploratory Visualization\n",
    "\n",
    "In order to see an agent in heuristic action, run:\n",
    "\n",
    "    git clone https://github.com/openai/gym.git\n",
    "    python gym/envs/box2d/lunar_lander.py\n",
    "\n",
    "In order to see an agent doing random actions, please run __code snippet 02__ in the code section.\n",
    "\n",
    "Comparing heuristic behaviour to the random one, it is immediately visible that a random approach does not bring great merit here. Firing all engines at random lets the agent crash every single time.\n",
    "\n",
    "\n",
    "### Algorithms and Techniques\n",
    "\n",
    "\n",
    "#### Understanding the Gist of Artificial Neural Networks\n",
    "\n",
    "Before starting to implement a Q estimator, I want to understand how an ANN works. According to Andrew Trask's manual, I create a network with one hidden layer, and the output layer, both sigmoid activated, which returns probablities for each of the outputs.\n",
    "\n",
    "I use a sigmoid activation function due to the easy calculation of the derivative and the widespread application in neural nets.\n",
    "\n",
    "In the procedure, the magic happens with `layer_2_weighted_loss`: Each output loss is multiplied by the slope / gradient of the predicted value on the sigmoid curve.\n",
    "\n",
    "In order to get a feeling for changes invoked by changing parametrization, I will alter the learning rate `ALPHA` as well as the number of neurons in the hidden layer `NUM_HIDDEN_NEURON` within an arbitrary range.\n",
    "\n",
    "__Code snippet 03__ shows the implementation of such a toy network.\n",
    "\n",
    "When looking at the output, it is clearly visible that the parametrization of the network has an impaction upon the network performance. Some configurations work well, like e.g. a hidden size of 8 with an `ALPHA` of 100, whereas as of a certain hidden size, the results stay at a bad level.\n",
    "\n",
    "\n",
    "#### Preparing the Environment\n",
    "\n",
    "The environment for the whole project is created once. The initialization procedure is created in a way \n",
    "\n",
    "- I create an instance (ENV) of the current Lunar Lander environment:\n",
    "\n",
    "        ENV = gym.make(\"LunarLander-v2\")\n",
    "        \n",
    "- The input dimension NUM_INPUT is obtained by resetting the environment:\n",
    "        \n",
    "        NUM_INPUT = ENV.reset().shape[0]\n",
    "\n",
    "- Thee actions are obtained by the environment method action_space:\n",
    "\n",
    "        NUM_ACTION = ENV.action_space.n\n",
    "        \n",
    "- The action space then is simply an enumeration:    \n",
    "\n",
    "        ACTIONS = np.arange(0, NUM_ACTION)\n",
    "\n",
    "__Code snippet 01__ shows the implementation of the environment initialization.\n",
    "\n",
    "\n",
    "#### Environment Interaction\n",
    "\n",
    "The main building block describing the interaction of the agent with the environment (during one epoch) is the following:\n",
    "\n",
    "It contains more than zero episodes. \n",
    "The function records statistics (stats) along the way:\n",
    "The reward sum per episode\n",
    "The number of times the lander was successful during the episode (with a reward greater or equal 100)\n",
    "The number of times the game was solved (with a reward greater or equal 200)\n",
    "The number of times the agent has revisited states, compared to the size of the Q table.\n",
    "\n",
    "    # Define a training epoch\n",
    "    def training_epoch():\n",
    "        # Play through a pre-defined number of episodes\n",
    "        for episode in range(NUM_EPISODE):         \n",
    "            # Initialize observations randomly and set the episode state to not done\n",
    "            x_t = ENV.reset()\n",
    "            done = False\n",
    "            # Start an episode, let it run as long as it is not done - each step is denoted with `t`\n",
    "            for step in count()\n",
    "                # Take an action based upon experience / a suitable model\n",
    "                ## [X]\n",
    "                # Observe the state space, the reward, and the episode state after action a_t\n",
    "                x_t, r_t, done, info = ENV.step(a_t)\n",
    "                # Train the model\n",
    "                ## [Y]\n",
    "                # Exit episode after crash or deadlock\n",
    "                if done or step > 10000: break  \n",
    "                \n",
    "[X] and [Y] are denoting the crucial moments of choosing an action and updating the experience.\n",
    "\n",
    "\n",
    "### Benchmark\n",
    "\n",
    "The benchmark metrics for comparing the different algorithms can be directly derived from the reward obtained during the course of an epoch. The main question is: *Is the problem solved?* A solved problem will be rewarded with 200 points. As a secondary measure, I measure *successful episodes*, with a reward of >= 100. If the environment is solved, the task is done. If not, I will have a look on the successful episodes in order to establish if I am on the right track.\n",
    "\n",
    "Throughout the project, sucesses and solutions are tracked for each episode, for each epoch. \n",
    "\n",
    "An epoch will entail 150 episodes each. The lower benchmark is set by the Q learning algorithm, followed by epochs which implement step-by-step to a fully fledged Deep Q learning algorithm. The latter then sets the higher benchmark for the following Policy Gradient algorithms.\n",
    "\n",
    "\n",
    "## III. Methodology\n",
    "\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "In its basic form, the input data / state space provided by Open AI Gym does not need preprocessing, it can be used as-is.\n",
    "\n",
    "However, I argue that it might make sense to let the agent take into account more than just the current state, when taking an action. As such, the \"memory\" of the agent will extend to `n` past state spaces, which requires stacking of state spaces. It becomes a parameter which can be tuned during experimentation.\n",
    "\n",
    "The random initialization of observations becomes\n",
    "\n",
    "        # Pile up STEP_MEM times the same init observation, in order to be consistent with the model input\n",
    "        s_t = np.tile(x_t, STEP_MEM)\n",
    "        \n",
    "After having observed a new state space `x_t`, I stack the new state space on top of the state space queue:\n",
    "\n",
    "        # Create state at t1: Append x observations, throw away the earliest\n",
    "        s_t1 = np.concatenate((x_t, s_t[:(STEP_MEM-1) * NUM_INPUT,]), axis=0) \n",
    "\n",
    "\n",
    "### Implementation and Refinement\n",
    "\n",
    "\n",
    "#### The Starting Point: Evaluating Bellman Equations From Data (Q-Learning)\n",
    "\n",
    "Q learning is about revisiting states. The agent is given a *memory* in form of a lookup table, where it stores states `s`, and updates the punishment or reward for each action it has taken. \n",
    "\n",
    "The number of possible combinations of the relevant inputs which constitute a state is called the state space. Whenever the state space is sufficiently small, the agent - at step `t` - might discover that it has already been in a specific state `s`. It therefore has made an experience by taking an action.\n",
    "\n",
    "If the agent should take advantage of its \"memory\" (possibly a dictionary of dictionaries, main keys being the states, values being the actions / keys and their values), it should use previous experiences and update them with new experiences:\n",
    "\n",
    "- It looks up the expected lifetime rewards per each possible action in `s` (a.k.a. action-value function, `q` values), selects the maximum `q` value, and executes the chosen action. \n",
    "- It obtains fresh evidence about the consequence of a specific action in a specific state: It knows the initial state `s_t`, the selected action `a_t`, the reward `r_t`, and the new state this all lead to, `s_t1`. \n",
    "- This knowledge it now uses to calculate a new `q` value for `s_t` by taking the observed reward `r_t`, and adding to it the discounted maximum `q` value for `s_t1`. The difference to the old `q` value is the new `q` value for action `a_t` in state `s_t`.\n",
    "\n",
    "It then chooses the action with the highest associated `q` value (<=> the maximum of the `q` function, or: the highest expected lifetime reward at step `t`) and moves onward another step.\n",
    "\n",
    "__Code snippet 04__ shows the implementation of a Q Learning algorithm.\n",
    "\n",
    "\n",
    "#### Why Q-Learning Often Does Not Work: Exploding State Spaces\n",
    "\n",
    "The crux about state spaces is, that they can get very large very quickly. Just think of a small number of inputs, each input being a floating number with 4 digits. Even this small setting is creating a large amount of combinations: the state space explodes. Revisiting states will become very unlikely, even with a large amount of trials. As a consequence, learning is slow, or even not happening.\n",
    "\n",
    "=> Code snippet 02 illustrates the point by setting up a basic q-learning algorithm. \n",
    "    \n",
    "One clearly can see that a simple table lookup is not adequate anymore. There is most often simply nothing to look up, since the state space is so huge. \n",
    "\n",
    "\n",
    "#### What to Do? Do Not Update the Q Function, But the Q Function Estimator: Deep Q Learning\n",
    "\n",
    "With the fast growth of the state space for the Lunar Lander game in mind, I intend to replace the table lookup by a function approximator. An Artifical Neural Net (ANN) will estimate the `q` function for the state `s` at step `t`.\n",
    "\n",
    "Once the agent has performed an action, it will know the reward `r_t` and the subsequent state `s_t1`. Based on this, it is able to update its memory. But this time, it does not update the `q` function directly. Instead, it is updating the ANN, which means that is updating the weights used in the ANN:\n",
    "\n",
    "- At time `t`, the agent already knows the *estimation* of the `q` function for state `s_t`: It used it to pick an action `a_t` accordingly. \n",
    "\n",
    "- After action `a_t`, the agent knows `s_t`, `a_t`, `r_t` and `s_t1`. This allows us to update the `q` function, *but only for the action taken*:  The discounted expected lifetime reward is added to `r_t`. In other words: The ANN estimates the `q` function for state `s_t1`. \n",
    "\n",
    "- For the action taken, the agent now updates the `q` value. All the other actions are not performed, the agent does not know about the reward, or a subsequent state `s_t1`. So, the agent cannot learn for those. This updated `q` function is the *target*.\n",
    "\n",
    "The error, which is the difference between the estimation and the target, is backpropagated through the network, such that the weights are updated.\n",
    "\n",
    "Next time the agent lets estimate the `q` function for another state `s`, it will have updated weights at hand.\n",
    "\n",
    "\n",
    "#### Deep Q Learning from Single Current Observations\n",
    "\n",
    "In order to implement the Q estimator, I need to change the training epoch function described in => code snippet 02. I first get rid of the lookup table `Q_TABLE` and the table's `q` value initialization `Q_INIT`. In plus, I do not need the learning rate `ALPHA` anymore. \n",
    "\n",
    "The core routine implemented for the function estimate is an ANN block. For this task I chose Keras and its `Sequential` model architecture. For the Deep Q Learning part of the project, the sequential model consists of \n",
    "\n",
    "- one fully-connected hidden layer (Keras terminology is `Dense`) plus an non-linear `relu` Activation, \n",
    "- the output layer with softmax activation, which produces probabilities for each of the possible actions given a certain state input. Here, I am not entirely sure if using probabilities in stead of a regression output is against some rules in Deep Q Learning. I tried also a linear activation and thereby found out that the probabilities are working very nicely. (Comments on this are very welcome.)\n",
    "\n",
    "This set-up I basically found after some experimentation in the parameter space.\n",
    "\n",
    "__Code snippet 05__ shows the exact implementation of a DQN including the ANN model block.\n",
    "\n",
    "Some highlights to special portions of the basic DQN:\n",
    "\n",
    "- For action selection, Q is estimated by the ANN based upon the current state space `s_t`, using the Keras function `model.predict`:\n",
    "\n",
    "            # Estimate q for each action at s_t\n",
    "            q = model.predict(s_t[np.newaxis])[0] \n",
    "\n",
    "- For the model update, Q is estimated by the ANN based upon the subsequent state space `s_t+1`, using the Keras function `model.predict`:\n",
    "\n",
    "            # Estimate q for each action at s_t1 (again a forward pass)\n",
    "            Q_sa = model.predict(s_t1[np.newaxis])[0] \n",
    "\n",
    "- For the model update, standard Q learning acts as the target, upon which (supervised learning) the ANN is trained using the Keras function `model.fit` (Note that the target is set to the reward in case the episode is terminated):\n",
    "\n",
    "           targets = q\n",
    "           targets[a_t] = r_t + GAMMA * np.max(Q_sa) if not done else r_t\n",
    "           model.fit(s_t[np.newaxis], targets[np.newaxis], verbose=0)\n",
    "           \n",
    "           \n",
    "#### *Epsilon Greedy* Deep Q Learning from Single Observations\n",
    "\n",
    "At this point, I want to tackle the issue of getting stuck in local minima. As a remedy, Deep Q Learning makes use the so called *epsilon greedy* action selection policy. It allows for a random move with probability `epsilon`, and by that introduces the notion of exploration (random moves) vs. exploitation (act on estimation of the q function). \n",
    "\n",
    "*Exploration* will reduce the probability of getting stuck in local minima, which are *not* reflecting the best action given a certain experience level of the agen. It's just like fresh air for the AI brain, introducing random ideas from outside. \n",
    "\n",
    "On the other hand, the agent needs to train and get experience with his selected moves. It needs evidence that one decision was (not) the right one, and to update its decicion finding process (the weights of the ANN. It only gets it by acting according to its own decisions undisturbed by random inputs. This is where *exploitation* comes in.\n",
    "\n",
    "In RL, usually `epsilon` decreases over a certain exploration period. This reflects the idea that the agent will start with many random moves to fathom the environment by just observing. With time and growing experience, it will decrease the share of random moves, since it feels more confident in its own decisions. \n",
    "\n",
    "I will follow th custom of allowing random moves at a linearly decreasing exploration rate during the exploration period. Following Deep Mind: Playing Atari with Deep Reinforcement Learning (p.6), I define a interval between the maximum and minimum `epsilon` allowed (`EPSILON_RANGE`). The exploration period (`NUM_EXPLORATION_STEP`) is set to roughly 1/10 of the total number of steps. Assuming an average 100 steps per episode lets `epsilon` decrease until Episode 6. If I increase the average to 150, I end up with the desired 10 episodes until minimum `epsilon`. \n",
    "\n",
    "During training, the agent will run on the minimum `epsilon` constantly.\n",
    "\n",
    "__Code snippet 06__ shows the exact implementation of a DQN including an epsilon greedy policy.\n",
    "\n",
    "Some highlights on the changes in the code:\n",
    "\n",
    "- Epsilon is annealed linearly over exploration period: \n",
    "         \n",
    "        epsilon = max(epsilon - (EPSILON_RANGE[0] - EPSILON_RANGE[1]) / NUM_EXPLORATION_STEP, EPSILON_RANGE[1])\n",
    "\n",
    "- Exploitation is done with probability `1-epsilon` (\"epsilon greedy\" policy):\n",
    "\n",
    "        # Take action with highest estimated reward \n",
    "        a_t = np.argmax(q) if np.random.random() > epsilon else np.random.choice(ACTIONS, 1)[0]\n",
    "\n",
    "\n",
    "#### Deep Q Learning *from Stored Experiences* \n",
    "\n",
    "Deep Mind: Playing Atari with Deep Reinforcement Learning claims that \n",
    "\n",
    ">learning directly from consecutive samples is inefficient, due to the strong correlations between the samples\n",
    "\n",
    "(p.5). Instead, the trick is to learn from a memory storage, which I will call Experience Replay Memory (`ERM`), in batches. The `ERM` is set up once per epoch. It is fed at each step with fresh transition evidence, and it is cropped again when its maximum size is reached.\n",
    "\n",
    "The `ERM` is the main database of the agent: It is the place where it \n",
    "- stores states and its experiences with the states (transitions `s_t`, `a_t`, `r_t`, and `s_t1`)\n",
    "- recalls on the memory, collects a memory sample, trains on the sample, and updates the `q` function estimator.\n",
    "\n",
    "`ERM_SIZE` is setting the size of the experience replay memory. Following the recommendation of Deep Mind: Playing Atari with Deep Reinforcement Learning (p.6), I set it equal to the number of exploration steps. `BATCH_SIZE` denotes the size of the transition sample which is drawn uniformly without replacement from the `ERM` at each step. Again, its size is following the recommendations of Deep Mind: Playing Atari with Deep Reinforcement Learning (p.6).\n",
    "\n",
    "Each memory update operates on a batch size of 32 randomly samples experiences. From what I [read](http://www.bitfusion.io/2016/11/03/quick-comparison-of-TensorFlow-gpu-performance-on-aws-p2-and-g2-instances/), larger batch sizes are increasing the probability of the algorithm getting caught in local minima, so I stick with the DeepMind indication here, and do not test out this parameter.\n",
    "\n",
    "__Code snippet 07__ shows the exact implementation of a DQN including an epsilon greedy policy and learning from stored experiences.\n",
    "\n",
    "The new features of the code:\n",
    "\n",
    "- Exploration is only started when the `ERM` is filled up with transitions completely:\n",
    "\n",
    "        # Linarily anneal random exploration rate epsilon over exploration period    \n",
    "        if len(ERM) < ERM_SIZE: epsilon = EPSILON_RANGE[0]\n",
    "        else: epsilon = max(epsilon - (EPSILON_RANGE[0] - EPSILON_RANGE[1]) / NUM_EXPLORATION_STEP, EPSILON_RANGE[1])\n",
    "\n",
    "- Transitions are stored in `ERM`\n",
    "\n",
    "        ERM.append((s_t, a_t, r_t, s_t1))\n",
    "\n",
    "- Randomly draw a minibatch out of the transitions available in the `ERM` of size `BATCH_SIZE`:\n",
    "\n",
    "        minibatch = np.array([ ERM[i] for i in np.random.choice(np.arange(0, len(ERM)), min(len(ERM), BATCH_SIZE)) ])\n",
    "        \n",
    "- Compute targets/references for each transition in minibatch\n",
    "\n",
    "        inputs = deque(); targets = deque()\n",
    "        for m in minibatch:\n",
    "            inputs.append(m[0]) # Append s_t of batch transition m to inputs\n",
    "            m_q = model.predict(m[0][np.newaxis])[0] # Estimate rewards for each action (targets), at s_t\n",
    "            m_Q_sa = model.predict(m[3][np.newaxis])[0] # Estimate rewards for each action (targets), at s_t1\n",
    "            m_targets = m_q\n",
    "            m_targets[m[1]] = m[2] + GAMMA * np.max(m_Q_sa)\n",
    "            targets.append(m_targets) # Append target of batch transition m to targets\n",
    "            \n",
    "- Train the model by backpropagating the errors and update weights\n",
    "            \n",
    "        model.train_on_batch(np.array(inputs), np.array(targets))  \n",
    "        \n",
    "        \n",
    "#### Policy Gradients       \n",
    "\n",
    "##### A Methododical Overview\n",
    "So far, I have performed Value-based Reinforcement Learning by trying to find the maximum value over all possible actions in a given state. The policy thereby was generated from the value function. Another approach would be to directly parametrize the policy.\n",
    "\n",
    "Both approaches have pros and cons. Policy-based RL has better convergence properties. It does not need a maximum value, which is of great interest in continuous action spaces. On the other hand, policy-based RL often converges towards local in stead of global optima, and they suffer from high variance.\n",
    "\n",
    "##### Taking An Action\n",
    "\n",
    "The agent faces the decision of taking one of the available actions at every step t. When applying a policy gradient model, the agent estimates which move is most likely to lead to a good end / a high reward: It assigns probabilities to every action. In order to accomplish this, I need a first ANN block, which outputs probabilities / uses a softmax at the output layer. It is called the policy estimator.\n",
    "\n",
    "After the estimation, it samples uniformly from the probability distribution in order to get an action, and observes / learns from the consequences of the action.\n",
    "\n",
    "##### Evaluating An Action\n",
    "\n",
    "An example for Policy-based RL is the Monte-Carlo Policy Gradient algorithm, which is also known as REINFORCE. It is the first example I am presenting here.\n",
    "\n",
    "The basic principle of the REINFORCE algorithm is to sample from the action-value function q by using the collected returns of a full episode: After each episode, the algorithm runs through each time step again calculates the target (a value function), which is the total discounted reward after a specific step, representing a sample of the action-value function multiplies the target with the score function at each time step to get the gradient. The score function is the negative log probability of the chosen action.\n",
    "\n",
    "It is important to see that during the trajectory (the time steps of an episode), the transitions are collected for the later adaption of the policy after the episode.\n",
    "\n",
    "In order to reduce the high variance of the method, it is common to subtract a baseline function B(s) from the target. Over all states s in the state space S, the expectation of the policy gradient is still 0, so expectations do not change.\n",
    "\n",
    "Instead of the target, I now multiply the score with the advantage function, which is simply the target minus the baseline. A suitable baseline is the state value function, which is a value function estimate. For this, I need a second ANN block, the so called value estimator, outputting a regression estimate by applying a mean squared error loss function.\n",
    "\n",
    "Intuitively, the advantage captures how much better than expected the action at time step t was doing.\n",
    "Another method to reduce the variance is to replace the Monte-Carlo Policy Gradient by an Actor-Critic algorithm. The last presented algorithm, the Action-Value (or Q) Actor-Critic algorithm, belongs to this family.\n",
    "\n",
    "Actor-critic are a mix between value based and policy based frameworks. The actor is taking the action, the critic is evaluating the action. In contrast to REINFORCE, the latter is done during the episode. This bootstrapping from the value function leads to lower variance, but also to higher bias, compared with REINFORCE.\n",
    "\n",
    "The target / value function for Q Actor-Critic algorithms is the estimated cumulated reward, which consists of the actual reward at s_t plus a discounted estimated reward for the action at s_t1: target = r_t + GAMMA * Q_sa.\n",
    "Again, value function estimate is subtracted from the target. The resulting advantage is then multiplied with the score function.\n",
    "\n",
    "##### Implementation\n",
    "\n",
    "Policy gradient methods need two kinds of different estimators:\n",
    "\n",
    "- The value estimator\n",
    "- The policy estimator\n",
    "\n",
    "For this purpose, I recreate the existing `_create_network` function. I will use a different set of initializations as well as hidden layer activations for the two models. The policy estimator has a softmax activation for the output layer, the value estimator a linear activation.\n",
    "\n",
    "In order to the model the non-standard gradients in Keras, I need to implement a helper function. Getting the gradients subsequently is relying directly on TensorFlow, applying tf.gradients().\n",
    "\n",
    "__Code snippet 08__ is showing the new `create_network` function as well as the gradient helper function.\n",
    "\n",
    "The Monte Carlo Policy Gradient method is implemented in __code snippet 09__. Note that the agent learns only after having finished an episode, which results in low bias and high variance.\n",
    "\n",
    "The Action-Value (Q) Actor-Critic is implemented in __code snippet 10__. In contrary to the Monte Carlo Policy Gradient implementation, the algorithm bootstraps from the value function during the episode, which results in higher bias, but lower variance.\n",
    "\n",
    "\n",
    "## IV. Results\n",
    "\n",
    "\n",
    "### Model Evaluation and Validation\n",
    "\n",
    "#### Evaluation Infrastructure\n",
    "\n",
    "First evaluation steps are done on a MacBook Air from early 2014. For tasks not relying directly on TensorFlow, I used  an on-site Windows Server 2008 R2 Enterprise with a quad core Intel Xeon CPU E5-2680 @ 2.7 GHz and 32 GB RAM. When performing HPC on AWS EC2, I started with Ubuntu 16.04 t2.micro free tier instances, then switched to Ubuntu 16.04 c4.8xlarge instances. The insufficient speed forced me to move on to bare metal GPU instances on AWS. I finally ended up using pre-configured Bitfusion Ubuntu 14 TensorFlow instances available on Amazon Marketplace, which spared me hours of setting up software and drivers, but still needed some tweaking.\n",
    "\n",
    "Processing the model evaluations took more than a month, included still too slow computation, set-ups, and trial and error with the test design.\n",
    "\n",
    "__Code snippet 11__ contains the shell script to set up the remote infrastructure on AWS EC2.\n",
    "\n",
    "#### Evaluation Design\n",
    "\n",
    "Model evaluation is done along the following generic steps:\n",
    "\n",
    "- Generate a model id for identification. Every id consists of the exact parametrization of the model\n",
    "        \n",
    "        EPOCH = '_'.join([repr(STEP_MEM), str(NUM_HIDDEN_NEURON), INITIALIZATION, ACTIVATION, str(round(GAMMA, 2))])\n",
    "\n",
    "- Train the model, i.e. call the function containing the model epoch, and save the output to variable\n",
    "    \n",
    "        stats = train_ql(render=True)     \n",
    "\n",
    "- Calculate the summary statistics for each epoch. `highest_reward` is of an informative nature, `success_episodes` and `solved_episodes` are the metrics for model benchmarking. \n",
    "\n",
    "        highest_reward = max([ v[0] for v in stats.values() ])\n",
    "        success_episodes = sum([ v[1] for v in stats.values() ])\n",
    "        solved_episodes = sum([ v[2] for v in stats.values() ])\n",
    "\n",
    "- Visualize the model performance, episode by episode\n",
    "\n",
    "        stdout.write(\"\\rEpoch {}, Maximum Reward {}, Successful Episodes {}, Solved Episodes {}\".format(\\\n",
    "            EPOCH, highest_reward, success_episodes, solved_episodes))\n",
    "            \n",
    "\n",
    "The quality of the algorithms is assessed with a handy 150 episodes each during the development phase away from Q Learning towards Deep Q Learning.\n",
    "\n",
    "Nothing is changed regarding the algorithm parametrization:\n",
    "\n",
    "- STEP_MEM = 1\n",
    "- NUM_HIDDEN_NEURON = 200\n",
    "- INITIALIZATION = 'glorot_uniform'\n",
    "- ACTIVATION = 'relu'\n",
    "- GAMMA = 0.00\n",
    "\n",
    "\n",
    "#### Evaluating the Phase from Q Learning (lower benchmark) to Deep Q Learning\n",
    "\n",
    "##### Q Learning\n",
    "\n",
    "__Code snippet 12__ with model *train_ql*, results:\n",
    "\n",
    "    Q Table size 14005, # Revisited States 0\n",
    "    Epoch Q-Learning, Maximum Reward -0.780001726748, Successful Episodes 3, Solved Episodes 0\n",
    "    \n",
    "The state space has swollen up to 15k states during 150 episodes. The lower benchmark is set. Not a single state is revisited by the procedure. Q Learning is not able to solve the environment, but produces three successes out of 150 in the trial.\n",
    " \n",
    "##### Deep Q Learning from Single Current Observations \n",
    "\n",
    "__Code snippet 12__ with model *dqn_1*, results:\n",
    "\n",
    "    Epoch 10_1_200_glorot_uniform_relu_0.99, Maximum Reward 177.884551583, Successful Episodes 5, Solved Episodes 0\n",
    "    \n",
    "No solution so far, but a much, much nicer landing behaviour compared to Q Learning. The frantic, purpose-less behaviour is gone most of the time, or is vanishing quickly within a few episodes only.\n",
    "\n",
    "- Most of the time, I observe an extensive swinging movement. The agent tries to counter-act skewed positions by engaging the lateral engines, and overdoes it. Then, it corrects again, and again, and from all these corrections forgets to fire against the moon's gravity, and then crashes into the surface.\n",
    "- There are plenty of times I can see the agent trapped into a locally optimal policy. For example, it stays on the ground, engaging left and right engine forever, perfectly stable, but not reaching the ultimate goal. Or a setting where left and right engines are engaged, but the lower engine does not fire at all, over long episodes.\n",
    "\n",
    "What is to be learned from this basic implementation?\n",
    "\n",
    "- Does the weight initializations make sense? What are the alternatives?\n",
    "- Does the nonlinear activation function for the hidden layer make sense? What are the alternatives? \n",
    "- What about the number of neurons in the hidden layer?\n",
    "- What is the optimal step memory `STEP_MEM`: Does it make sense to let the agent know only its current state, or shall I allow him to take into consideration also some of the states before? If yes, how far back should it remember? \n",
    "\n",
    "\n",
    "##### *Epsilon Greedy* Deep Q Learning from Single Observations\n",
    "\n",
    "__Code snippet 12__ with model *dqn_2*, results:\n",
    "\n",
    "    Epoch 10_1_200_glorot_uniform_relu_0.99, Maximum Reward 189.44243222, Successful Episodes 7, Solved Episodes 0\n",
    "\n",
    "TODO Observations\n",
    "\n",
    "##### Deep Q Learning *from Stored Experiences* \n",
    "\n",
    "__Code snippet 12__ with model *dqn_3*, results:\n",
    "\n",
    "    Epoch 10_1_200_glorot_uniform_relu_0.99, Maximum Reward 153.51536308, Successful Episodes 10, Solved Episodes 0\n",
    "\n",
    "A notion crosses my mind, during experimenting around with the minibatch: Why not perform a prediction for each sample transition of the `ERM`, during iterating through the batch? \n",
    "\n",
    "Probably, because it has the disadvantage that I predict with the knowledge available at step `t`. This might be faulty, and the faulty prediction stays as target reference in the batch, and is used to compare the loss for the taken action between the prediction at timestep `t` and the potenitially long ago target estimation. This will bias the learning process significantly. Thus, `model.train_on_batch` is done only after the batch has been created and computed.\n",
    "    \n",
    "\n",
    "### Justification\n",
    "\n",
    "#### Brute Force Parameter Space Exploration\n",
    "\n",
    "This section covers the experimental parameter space exploration for the three algorithms Deep Q Learning, Monte Carlo Policy Gradients, and Action-Value (Q) Actor-Critic. The Deep Q Learning thereby is algorithm *dqn_3*, which finalized the development phase of Deep Q Learning.\n",
    "\n",
    "The goal is to identify well-working parameter settings. \n",
    "\n",
    "The parameter space will entail ranges of `NUM_HIDDEN_NEURON`, and `STEP_MEM`. `EPSILON_RANGE` for Deep Q Learning starts at 20% instead of 10%, in order to allow more exploration than before.\n",
    "\n",
    "The original idea was to include more parameters into the routine:\n",
    "\n",
    "- Steps per Action (`SPA`) determines how many steps pass before a fresh action is taken. Deep Mind - Playing Atari with Deep Reinforcement Learning suggests 4 skipping 4 time steps before conducting an action (p.6): `if step % SPA == 0: a_t = np.argmax(q) if np.random.random() > epsilon else np.random.choice(ACTIONS, 1)[0]`.\n",
    "\n",
    "- Reward Clipping (`R_CLIP`) is also an idea taken from Deep Mind - Playing Atari with Deep Reinforcement Learning (p.6), where it was implemented in order to make rewards comparable across the different games: `if R_CLIP and r_t != 0: r_t = abs(r_t) / r_t`.\n",
    "\n",
    "- Looping over a range of `GAMMA` candidates.\n",
    "\n",
    "- Looping over a multitude of `EPSILON_RANGE`.\n",
    "\n",
    "- Looping over all possible Keras activations and initializations.\n",
    "\n",
    "In order to stay within a feasible time frame w.r.t. computation and a exploding parameter space, I decided to test only a subset of parameters.\n",
    "\n",
    "Each parameter setting is tested within an epoch consisting of 150 episodes. Parameter settings with at least one successful episode are written down to `.json` files.\n",
    "\n",
    "__Code snippet 13__ prepares the brute force parameter exploration. The presented helper function extracts the multitude of `.json` files into a readable form.\n",
    "\n",
    "__Code snippet 14__ implements the brute force parameter exploration for Deep Q Learning. This exploration entails 42 epochs over 150 episodes each. Reading the cumulative statistics from the `DQN_Stats.json`:\n",
    "\n",
    "    Solved! | 0.01_6_200_glorot_uniform_relu_0.99 | [186.83053538728421, 13.0, 1, 2]\n",
    "    Solved! | 0.01_6_400_glorot_uniform_relu_0.99 | [121.41997221106126, 17.0, 1, 2]\n",
    "    \n",
    "Obviously, an learning rate of 0.01 in combination with a memory of 6 frames leads to very good results with solved envirionments. It is just a question of choosing a hidden layer size of 200 or 400.\n",
    "\n",
    "__Code snippet 15__ implements step 1 of the brute force parameter exploration for Policy Gradient methods. The 235'200 epochs -  each 150 episodes for each model - is extremely ambitious in terms of the computational capacities at hand.\n",
    "\n",
    "In fact, a first trial with this full set up had to be aborted after very long runtimes on one single GPU / CPU combination. It resulted in a first indication in which direction to tune at least some of the parameters (`MCPG_Stats.json`, `QAC_Stats.json`):\n",
    "\n",
    "    /Users/matthiaswettstein/RL_Saves/MCPG_Stats.json | ('5_200_normal_normal_softplus_softsig', -7.553130756059716, 5.0)\n",
    "    /Users/matthiaswettstein/RL_Saves/QAC_Stats.json | ('1_200_he_uniform_he_uniform_softplus_softplus', -8.638336724713128, 7.0)\n",
    "\n",
    "For the consecutive exploration, I decided to fix the following parameter subspace\n",
    "\n",
    "- Value estimator initialization `normal`\n",
    "- Policy estimator intialization `normal`\n",
    "- Value estimator activation function `softplus`\n",
    "- Policy estimator activation function `softsig` \n",
    "\n",
    "for Monte Carlo Policy Gradients, and \n",
    "\n",
    "- Value estimator initialization `he_uniform`\n",
    "- Policy estimator intialization `he_uniform`\n",
    "- Value estimator activation function `softplus`\n",
    "- Policy estimator activation function `softplus` \n",
    "\n",
    "for Action-Value (Q) Actor-Critic, based on the available *non-exhaustive* activation and initialization exploration resulting in 5 resp. 7 successful episodes out of 150.\n",
    "\n",
    "__Code snippet 16__ implements step 2 of the brute force parameter exploration for Policy Gradient methods. The parameter space shinks 48 combinations.\n",
    "\n",
    "\n",
    "\n",
    "## V. Conclusion\n",
    "\n",
    "### Free-Form Visualization\n",
    "In this section, you will need to provide some form of visualization that emphasizes an important quality about the project. It is much more free-form, but should reasonably support a significant result or characteristic about the problem that you want to discuss. Questions to ask yourself when writing this section:\n",
    "- _Have you visualized a relevant or important quality about the problem, dataset, input data, or results?_\n",
    "- _Is the visualization thoroughly analyzed and discussed?_\n",
    "- _If a plot is provided, are the axes, title, and datum clearly defined?_\n",
    "\n",
    "### Reflection\n",
    "In this section, you will summarize the entire end-to-end problem solution and discuss one or two particular aspects of the project you found interesting or difficult. You are expected to reflect on the project as a whole to show that you have a firm understanding of the entire process employed in your work. Questions to ask yourself when writing this section:\n",
    "- _Have you thoroughly summarized the entire process you used for this project?_\n",
    "- _Were there any interesting aspects of the project?_\n",
    "- _Were there any difficult aspects of the project?_\n",
    "- _Does the final model and solution fit your expectations for the problem, and should it be used in a general setting to solve these types of problems?_\n",
    "\n",
    "### Improvement\n",
    "In this section, you will need to provide discussion as to how one aspect of the implementation you designed could be improved. As an example, consider ways your implementation can be made more general, and what would need to be modified. You do not need to make this improvement, but the potential solutions resulting from these changes are considered and compared/contrasted to your current solution. Questions to ask yourself when writing this section:\n",
    "- _Are there further improvements that could be made on the algorithms or techniques you used in this project?_\n",
    "- _Were there algorithms or techniques you researched that you did not know how to implement, but would consider using if you knew how?_\n",
    "- _If you used your final solution as the new benchmark, do you think an even better solution exists?_\n",
    "\n",
    "\n",
    "### Credits and Thanks\n",
    "\n",
    "- Neural Networks\n",
    "    - [Andrew Trask](https://iamtrask.github.io)\n",
    "    - [Sebastian Raschka](http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html)\n",
    "- Deep Q Learning\n",
    "    - [Deep Mind - Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)\n",
    "    - Tambet Matiisen ([here](https://www.nervanasys.com/demystifying-deep-reinforcement-learning/) and [here](https://github.com/tambetm/simple_dqn/blob/master/src/replay_memory.py))\n",
    "    - [Eder Santana](http://edersantana.github.io/articles/keras_rl/)\n",
    "    - [Ben Lau](https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html)\n",
    "- Policy Gradients\n",
    "    - [Andrej Karpathy](http://karpathy.github.io/2016/05/31/rl/)\n",
    "    - [Denny Britz](https://github.com/dennybritz/reinforcement-learning/tree/master/PolicyGradient/)\n",
    "    - [Open AI Gym Documentation](https://gym.openai.com/docs/rl#policy-gradients)\n",
    "    - [David Silver](https://www.youtube.com/watch?v=KHZVXao4qXs)\n",
    "    - [Non-standard gradient optimization](https://github.com/fchollet/keras/issues/3062)\n",
    "- Keras\n",
    "    - Francois Chollet ([here](https://keras.io) and [here](https://github.com/fchollet/keras/tree/master/examples))\n",
    "- Box2D & Swig\n",
    "    - [Forums](https://github.com/openai/gym/issues/100)\n",
    "- Cloud Computing on Amazon Web Services EC2 / GPU activation\n",
    "    - [Amazon Tutorials](https://aws.amazon.com/de/ec2/)\n",
    "    - [Jie Yang](http://yangjie.me/2015/08/26/Run-Jupyter-Notebook-Server-on-AWS-EC2/)\n",
    "    - [David Sanwald](https://davidsanwald.github.io/ec2-openAI-gym-TensorFlow-GPU-cuda-deep-learning.html#ec2-openAI-gym-TensorFlow-GPU-cuda-deep-learning)\n",
    "    - [TensorFlow Instructions](https://www.TensorFlow.org/versions/r0.7/get_started/os_setup.html#optional-install-cuda-gpus-on-linux)\n",
    "\n",
    "-----------\n",
    "\n",
    "__Before submitting, ask yourself. . .__\n",
    "\n",
    "- Does the project report you’ve written follow a well-organized structure similar to that of the project template?\n",
    "- Is each section (particularly __Analysis__ and __Methodology__) written in a clear, concise and specific fashion? Are there any ambiguous terms or phrases that need clarification?\n",
    "- Would the intended audience of your project be able to understand your analysis, methods, and results?\n",
    "- Have you properly proof-read your project report to assure there are minimal grammatical and spelling mistakes?\n",
    "- Are all the resources used for this project correctly cited and referenced?\n",
    "- Is the code that implements your solution easily readable and properly commented?\n",
    "- Does the code execute without error and produce results similar to those reported?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
