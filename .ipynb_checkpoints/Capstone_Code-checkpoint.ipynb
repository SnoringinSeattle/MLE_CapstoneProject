{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Snippet 01: Preparations\n",
    "\n",
    "#### Set Namespace\n",
    "\n",
    "I need numerical computing, an environment, a decent iterator, and proper Python 2 printout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from itertools import count\n",
    "from sys import stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Global Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ENV = gym.make(\"LunarLander-v2\")\n",
    "NUM_INPUT = ENV.reset().shape[0]\n",
    "NUM_ACTION = ENV.action_space.n\n",
    "ACTIONS = np.arange(0, NUM_ACTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Snippet 02: Exploratory Visualization\n",
    "\n",
    "#### Create a random action agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ENV.reset()\n",
    "for _ in range(1000):\n",
    "    ENV.render() # comment out for visual assessment, if possible\n",
    "    ENV.step(ENV.action_space.sample()) # take a random action\n",
    "    print ENV.step(ENV.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code snippet 03: Create A Toy Artificial Neural Network (ANN)\n",
    "\n",
    "#### Set Hyperparameters\n",
    "\n",
    "- NUM_EPISODE denotes the maximum number of episodes an epoch will embrace.\n",
    "- ALPHA_RANGE is the range of learning rates to be considered.\n",
    "- NUM_HIDDEN_NEURON_RANGE is the range of hidden layer sizes to be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_EPISODE = 150\n",
    "ALPHA_RANGE = [10**-4, 10**-2, 1, 10**2, 10**4] # Constant over one epoch\n",
    "NUM_HIDDEN_NEURON_RANGE = [8, 16, 32, 64, 128, 256, 512] # Constant over one epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the helper functions/ sigmoid functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Create the sigmoid function's derivative\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build training epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize weights with the correct dimensionality to fit in the input layer\n",
    "X = np.random.randint(2, size=(4, 3))\n",
    "y = np.random.randint(2, size=(4, 1))\n",
    "\n",
    "# Test the hidden dimensions\n",
    "for NUM_HIDDEN_NEURON in NUM_HIDDEN_NEURON_RANGE: \n",
    "    \n",
    "    # Initialize 1st set of weights\n",
    "    W1 = np.random.rand(X.shape[1], NUM_HIDDEN_NEURON) \n",
    "\n",
    "    # Initialize 2nd set of weights\n",
    "    W2 = np.random.rand(NUM_HIDDEN_NEURON, y.shape[1]) \n",
    "    \n",
    "    # Test the alphas\n",
    "    for ALPHA in ALPHA_RANGE: \n",
    "        \n",
    "        for episode in range(NUM_EPISODE):\n",
    "            # Forward propagate\n",
    "            \n",
    "            # Initialize hidden layer (fully connected)\n",
    "            layer_1 = np.dot(X, W1)\n",
    "            \n",
    "            # Apply sigmoid activation\n",
    "            layer_1 = sigmoid(layer_1) \n",
    "            \n",
    "            # Initialize output layer(fully connected)\n",
    "            layer_2 = np.dot(layer_1, W2) \n",
    "            \n",
    "            # Apply sigmoid activation \n",
    "            layer_2 = sigmoid(layer_2) \n",
    "\n",
    "            # Calculate loss\n",
    "            layer_2_loss = y - layer_2 \n",
    "\n",
    "            ''' Apply SGD to the loss: the more certain the estimate, the less weighted it will get: \n",
    "                The gradient at the extremes is smaller than in the middle\n",
    "            '''\n",
    "            layer_2_weighted_loss = layer_2_loss * sigmoid_derivative(layer_2) # element-wise multiplication!\n",
    "            \n",
    "            # Backpropagate\n",
    "            \n",
    "            # Compute the effect of the hidden layer to the weighted loss\n",
    "            layer_1_loss = np.dot(layer_2_weighted_loss, W2.T) \n",
    "\n",
    "            # Apply SGD\n",
    "            layer_1_weighted_loss = layer_1_loss * sigmoid_derivative(layer_1) \n",
    "                \n",
    "            # Update weights\n",
    "            W2 += ALPHA * np.dot(layer_1.T, layer_2_weighted_loss)\n",
    "            W1 += ALPHA * np.dot(X.T, layer_1_weighted_loss)\n",
    "            \n",
    "            # Visualize\n",
    "            if episode == NUM_EPISODE - 1: print \"Hidden Size {}, alpha {}: Avg loss {}\".format(\n",
    "                                                '%4s' % NUM_HIDDEN_NEURON, \\\n",
    "                                                '%7s' % ALPHA, \\\n",
    "                                                '%14s' % np.mean(np.abs(layer_2_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Snippet 04: Implementation and Refinement:: Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend Hyperparameters\n",
    "\n",
    "- GAMMA is the factor by which future expected rewards are discounted.\n",
    "- ALPHA is the learning rate.\n",
    "- Q_TABLE is the agent's memory. The states are the keys, and dictionaries of actions and their respective values are the values.\n",
    "- VALUE_INIT is the initial value for the actions of a state, once the state is visited the first time. It is set to zero.\n",
    "- MAX_NUM_STEPS is an arbitrary maximum of allowed steps in case of deadlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "ALPHA = 0.1\n",
    "Q_TABLE = {}\n",
    "VALUE_INIT = 0\n",
    "MAX_NUM_STEPS = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build training epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_ql(render=False):\n",
    "    # Init epoch stats\n",
    "    stats = {}\n",
    "    \n",
    "    # Init counter for how many times the agent revisited states\n",
    "    revisiting_states = 0 \n",
    "\n",
    "    # Play through episodes\n",
    "    for episode in range(NUM_EPISODE):\n",
    "        # Init episode stats\n",
    "        stats[episode] = [0]*3\n",
    "        \n",
    "        # Init observations\n",
    "        x_t = ENV.reset() \n",
    "        # Take only the current observation as state, in order to keep the state space as small as possible\n",
    "        s_t = tuple(x_t) \n",
    "        done = False\n",
    "\n",
    "        # Start an episode\n",
    "        for step in count():\n",
    "            # Reset the environment for a new gaming step\n",
    "            if render: ENV.render()\n",
    "            \n",
    "            # Look up the action with the highest value at s_t, as well as its value\n",
    "            a_t, q, r_s  = best_action(s_t)\n",
    "\n",
    "            # Observe after action a_t\n",
    "            x_t1, r_t, done, info = ENV.step(a_t) \n",
    "            # Again, take only the current observation\n",
    "            s_t1 = tuple(x_t1) \n",
    "            \n",
    "            # Look up the action with the highest value at s_t1, as well as its value\n",
    "            a_t1, Q_sa, _ = best_action(s_t1) \n",
    "            \n",
    "            # Look up the action with the highest value at s_t1, as well as its value\n",
    "            Q_TABLE[s_t][a_t] = q + ALPHA * (r_t + GAMMA * Q_sa - q) \n",
    "            \n",
    "            # Update statistics: Sum up rewards, count steps, successes, solved\n",
    "            stats[episode][0] += r_t\n",
    "            if r_t >= 100: stats[episode][1] += 1\n",
    "            if r_t >= 200: stats[episode][2] += 1\n",
    "            \n",
    "            # Visualize\n",
    "            stdout.write(\"\\r{} | Step {} @ Episode {}/{} | Step Reward {}\".format(\\\n",
    "                EPOCH, step, episode, NUM_EPISODE, round(r_t, 1)))\n",
    "            \n",
    "            # Exit episode after crash or deadlock\n",
    "            if done or step > MAX_NUM_STEPS: break\n",
    "            \n",
    "            # Update state\n",
    "            s_t = s_t1\n",
    "            revisiting_states += r_s\n",
    "            \n",
    "    # Visualize revisiting states issue\n",
    "    print \"\\rQ Table size {}, # Revisited States {}\".format(len(Q_TABLE), revisiting_states)\n",
    "    \n",
    "    return stats\n",
    "    \n",
    "    \n",
    "# Create helper function to initialize and query Q-table\n",
    "def best_action(state):\n",
    "    # Create init scenario for table queries at t and t1\n",
    "    if state not in Q_TABLE or sum(Q_TABLE[state].values()) == 0: \n",
    "        # Bookkeeping: Init revisiting states counter\n",
    "        revisit_state = 0 \n",
    "        \n",
    "        # Init q function\n",
    "        q_function = {} \n",
    "        for A in ACTIONS: q_function[A] = VALUE_INIT\n",
    "        Q_TABLE[state] = q_function \n",
    "        \n",
    "        # Do random action\n",
    "        action = np.random.choice(ACTIONS, 1)[0] \n",
    "    else: \n",
    "        revisit_state = 1\n",
    "        \n",
    "        # Select action according to max q\n",
    "        action = max(Q_TABLE[state], key=Q_TABLE[state].get) \n",
    "    \n",
    "    # Get q value for action selected\n",
    "    q = Q_TABLE[state][action] \n",
    "    \n",
    "    return action, q, revisit_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Snippet 05: Implementation and Refinement:: Deep Q Learning from Single Current Observations\n",
    "\n",
    "#### Extend Namespace\n",
    "\n",
    "From the Keras library, import the basic `Sequential` model, with two different layer types: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend Hyperparameters\n",
    "- STEP_MEM is the number of steps the agent should take into account as the current state it is in: This is the \"operational\" memory of the agent.\n",
    "- NUM_HIDDEN_NEURON is the number of neurons in the hidden layer.\n",
    "- INITIALIZATION is one of the available weight initializations in Keras.\n",
    "- ACTIVATION is one of the available activation functions in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "STEP_MEM = 1\n",
    "NUM_HIDDEN_NEURON = 200\n",
    "INITIALIZATION = 'glorot_uniform'\n",
    "ACTIVATION = 'relu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the ANN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _create_network_1():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(NUM_HIDDEN_NEURON, init=INITIALIZATION, input_shape=(STEP_MEM*NUM_INPUT,))) \n",
    "    model.add(Activation(ACTIVATION))\n",
    "    model.add(Dense(NUM_ACTION, init=INITIALIZATION))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build training epoch\n",
    "The epoch build will always follow the same pattern. New elements or changes w.r.t. the previous epoch are indicated in the code comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dqn_1(model, render=False):\n",
    "    # Init epoch stats\n",
    "    stats = {}\n",
    "\n",
    "    # Play through episodes\n",
    "    for episode in range(NUM_EPISODE): \n",
    "        # Init episode stats\n",
    "        stats[episode] = [0]*3\n",
    "        \n",
    "        # Init observations\n",
    "        x_t = ENV.reset()\n",
    "        # Pile up STEP_MEM times the same init observation, in order to be consistent with the model input\n",
    "        s_t = np.tile(x_t, STEP_MEM)\n",
    "        done = False\n",
    "\n",
    "        # Start an episode\n",
    "        for step in count():\n",
    "            \n",
    "            # Reset the environment for a new gaming step\n",
    "            if render: ENV.render()\n",
    "    \n",
    "            # Estimate q for each action at s_t\n",
    "            q = model.predict(s_t[np.newaxis])[0] \n",
    "\n",
    "            # Take action with highest estimated reward (argmax returns index)\n",
    "            a_t = np.argmax(q) \n",
    "            \n",
    "            # Observe after action a_t\n",
    "            x_t, r_t, done, info = ENV.step(a_t) \n",
    "            \n",
    "            # Create state at t1: Append x observations, throw away the earliest\n",
    "            s_t1 = np.concatenate((x_t, s_t[:(STEP_MEM-1) * NUM_INPUT,]), axis=0) \n",
    "\n",
    "            # Estimate q for each action at s_t1 (again a forward pass)\n",
    "            Q_sa = model.predict(s_t1[np.newaxis])[0] \n",
    "\n",
    "            ''' Create reference/targets by updating estimated reward for chosen action:\n",
    "                For action taken, replace estimated reward by remaining cumulative lifetime reward\n",
    "            ''' \n",
    "            targets = q\n",
    "            targets[a_t] = r_t + GAMMA * np.max(Q_sa) if not done else r_t\n",
    "\n",
    "            ''' Learn!\n",
    "                - Again, predict q values for state s_t\n",
    "                - Calculate loss by comparing predictions to targets: they will differ only for the action taken\n",
    "                - backpropagate error for action taken, update weights\n",
    "            ''' \n",
    "            model.fit(s_t[np.newaxis], targets[np.newaxis], verbose=0)\n",
    "            \n",
    "            # Update statistics: Sum up rewards, count steps, successes, solved\n",
    "            stats[episode][0] += r_t\n",
    "            if r_t >= 100: stats[episode][1] += 1\n",
    "            if r_t >= 200: stats[episode][2] += 1\n",
    "            \n",
    "            # Visualize\n",
    "            stdout.write(\"\\r{} | Step {} @ Episode {}/{} | Epsilon {} | Step Reward {}\".format(\\\n",
    "                EPOCH, step, episode, NUM_EPISODE, epsilon, round(r_t, 1)))\n",
    "   \n",
    "            # Exit episode after crash or deadlock\n",
    "            if done or step > MAX_NUM_STEPS: break\n",
    "\n",
    "            # Update state\n",
    "            s_t = s_t1\n",
    "            \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Snippet 06: Implementation and Refinement::  *Epsilon Greedy* Deep Q Learning from Single Observations\n",
    "\n",
    "#### Extend Hyperparameters\n",
    "\n",
    "- `EPSILON_RANGE` is an interval between the maximum and minimum `epsilon` allowed\n",
    "- The exploration period (`NUM_EXPLORATION_STEP`) is set to roughly 1/10 of the total number of steps. Assuming an average 100 steps per episode lets `epsilon` decrease until Episode 6. If I increase the average to 150, I end up with the desired 10 episodes until minimum `epsilon`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPSILON_RANGE = [0.1, 0.0001]\n",
    "NUM_EXPLORATION_STEP = NUM_EPISODE * 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Training Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dqn_2(model, render=False): \n",
    "    ##NEW Initialize epsilon at its maximum value\n",
    "    epsilon = EPSILON_RANGE[0] \n",
    "    \n",
    "    # Init epoch stats\n",
    "    stats = {}\n",
    "\n",
    "    # Play through episodes\n",
    "    for episode in range(NUM_EPISODE): \n",
    "        # Init episode stats\n",
    "        stats[episode] = [0]*3\n",
    "        \n",
    "        # Init observations        \n",
    "        x_t = ENV.reset()\n",
    "        s_t = np.tile(x_t, STEP_MEM)\n",
    "        done = False\n",
    "\n",
    "        # Start an episode\n",
    "        for step in count():\n",
    "            \n",
    "            # Reset the environment for a new gaming step\n",
    "            if render: ENV.render()\n",
    "            \n",
    "            ###NEW Linarily anneal random exploration rate epsilon over exploration period            \n",
    "            epsilon = max(epsilon - (EPSILON_RANGE[0] - EPSILON_RANGE[1]) / NUM_EXPLORATION_STEP, EPSILON_RANGE[1])\n",
    "    \n",
    "            # Estimate q for each action at s_t\n",
    "            q = model.predict(s_t[np.newaxis])[0]\n",
    "\n",
    "            # Take action with highest estimated reward \n",
    "            ###NEW Do this with probability 1-epsilon (\"epsilon greedy\" policy)\n",
    "            a_t = np.argmax(q) if np.random.random() > epsilon else np.random.choice(ACTIONS, 1)[0]\n",
    "\n",
    "            # Observe after action a_t\n",
    "            x_t, r_t, done, info = ENV.step(a_t)\n",
    "        \n",
    "            # Create state at t1: Append x observations, throw away the earliest\n",
    "            s_t1 = np.concatenate((x_t, s_t[:(STEP_MEM-1) * NUM_INPUT,]), axis=0)\n",
    "\n",
    "            # Estimate q for each action at s_t1 (again a forward pass)\n",
    "            Q_sa = model.predict(s_t1[np.newaxis])[0]\n",
    "\n",
    "            ''' Create reference/targets by updating estimated reward for chosen action\n",
    "                For action taken, replace estimated reward by remaining cumulative lifetime reward\n",
    "            ''' \n",
    "            targets = q\n",
    "            targets[a_t] = r_t + GAMMA * np.max(Q_sa) if not done else r_t\n",
    "\n",
    "            ''' Learn!\n",
    "                - Again, predict q values for state s_t\n",
    "                - Calculate loss by comparing predictions to targets: they will differ only for the action taken\n",
    "                - backpropagate error for action taken, update weights\n",
    "            ''' \n",
    "            model.fit(s_t[np.newaxis], targets[np.newaxis], verbose=0)\n",
    "            \n",
    "            # Update statistics: Sum up rewards, count steps, successes, solved\n",
    "            stats[episode][0] += r_t\n",
    "            if r_t >= 100: stats[episode][1] += 1\n",
    "            if r_t >= 200: stats[episode][2] += 1\n",
    "            \n",
    "            # Visualize\n",
    "            stdout.write(\"\\r{} | Step {} @ Episode {}/{} | Epsilon {} | Step Reward {}\".format(\\\n",
    "                EPOCH, step, episode, NUM_EPISODE, epsilon, round(r_t, 1)))\n",
    "   \n",
    "            # Exit episode after crash or deadlock\n",
    "            if done or step > MAX_NUM_STEPS: break            \n",
    "\n",
    "            # Update state\n",
    "            s_t = s_t1\n",
    "            \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Snippet 07: Implementation and Refinement::   Deep Q Learning *from Stored Experiences* \n",
    "\n",
    "#### Extend namespace\n",
    "\n",
    "For the ERM, I need a fast queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend Hyperparameters\n",
    "\n",
    "- `ERM_SIZE` denotes to size of the memory to sample from. It is set equal to the number of exploration steps.\n",
    "- `BATCH_SIZE` denotes the size of the sample drawn from `ERM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ERM_SIZE = NUM_EXPLORATION_STEP\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Training Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dqn_3(model, render=False):\n",
    "    # Init epsilon and ERM\n",
    "    epsilon = EPSILON_RANGE[0]\n",
    "    ERM = deque(maxlen=ERM_SIZE)\n",
    "    \n",
    "    # Init epoch stats\n",
    "    stats = {}\n",
    "    \n",
    "    # Play through episodes\n",
    "    for episode in range(NUM_EPISODE): \n",
    "        # Init episode stats\n",
    "        ### NEW: Add episode length stats\n",
    "        stats[episode] = [0]*3 + [1]\n",
    "        \n",
    "        # Init observations        \n",
    "        x_t = ENV.reset()\n",
    "        s_t = np.tile(x_t, STEP_MEM)\n",
    "        done = False\n",
    "\n",
    "        # Start an episode\n",
    "        for step in count():\n",
    "            \n",
    "            # Reset the environment for a new gaming step\n",
    "            if render: ENV.render()\n",
    "            \n",
    "            # Linarily anneal random exploration rate epsilon over exploration period    \n",
    "            ###NEW Exploration starts only when ERM is complete\n",
    "            if len(ERM) < ERM_SIZE: epsilon = EPSILON_RANGE[0]\n",
    "            else: epsilon = max(epsilon - (EPSILON_RANGE[0] - EPSILON_RANGE[1]) / NUM_EXPLORATION_STEP, EPSILON_RANGE[1])\n",
    "    \n",
    "            # Estimate q for each action at s_t\n",
    "            q = model.predict(s_t[np.newaxis])[0]\n",
    "\n",
    "            # Take action with highest estimated reward with probability 1-epsilon (\"epsilon greedy\" policy)\n",
    "            a_t = np.argmax(q) if np.random.random() > epsilon else np.random.choice(ACTIONS, 1)[0]\n",
    "\n",
    "            # Observe after action a_t\n",
    "            x_t, r_t, done, info = ENV.step(a_t)\n",
    "        \n",
    "            # Create state at t1: Append x observations, throw away the earliest\n",
    "            s_t1 = np.concatenate((x_t, s_t[:(STEP_MEM-1) * NUM_INPUT,]), axis=0)\n",
    "            \n",
    "            ###NEW Store transition in experience replay memory\n",
    "            ERM.append((s_t, a_t, r_t, s_t1))\n",
    "\n",
    "            ###NEW Choose a batch of maximum length BATCH_SIZE\n",
    "            minibatch = np.array([ ERM[i] for i in np.random.choice(np.arange(0, len(ERM)), min(len(ERM), BATCH_SIZE)) ])\n",
    "            \n",
    "            ###NEW Compute targets/reference for each transition in minibatch\n",
    "            inputs = deque(); targets = deque()\n",
    "            for m in minibatch:\n",
    "                inputs.append(m[0]) # Append s_t of batch transition m to inputs\n",
    "                m_q = model.predict(m[0][np.newaxis])[0] # Estimate rewards for each action (targets), at s_t\n",
    "                m_Q_sa = model.predict(m[3][np.newaxis])[0] # Estimate rewards for each action (targets), at s_t1\n",
    "                m_targets = m_q\n",
    "                m_targets[m[1]] = m[2] + GAMMA * np.max(m_Q_sa)\n",
    "                targets.append(m_targets) # Append target of batch transition m to targets\n",
    "                \n",
    "            ###NEW Train the model by backpropagating the errors and update weights\n",
    "            model.train_on_batch(np.array(inputs), np.array(targets))\n",
    "            \n",
    "            # Update statistics: Sum up rewards, count steps, successes, solved\n",
    "            stats[episode][0] += r_t\n",
    "            if r_t >= 100: stats[episode][1] += 1\n",
    "            if r_t >= 200: stats[episode][2] += 1\n",
    "            stats[episode][3] = step\n",
    "            \n",
    "            # Visualize\n",
    "            stdout.write(\"\\r{} | Step {} @ Episode {}/{} | Epsilon {} | Step Reward {}\".format(\\\n",
    "                EPOCH, step, episode, NUM_EPISODE, epsilon, round(r_t, 1)))\n",
    "            \n",
    "            # Exit episode after crash or deadlock\n",
    "            if done or step > MAX_NUM_STEPS: break           \n",
    "            \n",
    "            # Update state\n",
    "            s_t = s_t1  \n",
    "            \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Snippet 08: Implementation and Refinement::   Preparations for Policy Gradient Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Extend Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.engine import training\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Keras Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _create_network_2(model_type): \n",
    "    model = Sequential()\n",
    "    \n",
    "    if model_type == 'value':\n",
    "        # Regression function estimate for calculating the advantage\n",
    "        model.add(Dense(NUM_HIDDEN_NEURON, init=INITIALIZATION_V, input_dim=STEP_MEM*NUM_INPUT)) \n",
    "        model.add(Activation(ACTIVATION_V))\n",
    "        model.add(Dense(1, init=INITIALIZATION_V))\n",
    "        model.compile(optimizer='rmsprop', loss='mse')\n",
    "    \n",
    "    if model_type == 'policy':\n",
    "        model.add(Dense(NUM_HIDDEN_NEURON, init=INITIALIZATION_P, input_dim=STEP_MEM*NUM_INPUT)) \n",
    "        model.add(Activation(ACTIVATION_P))\n",
    "        model.add(Dense(NUM_ACTION, init=INITIALIZATION_P))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Helper Function for Non-Standard Gradient Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_trainable_params(model):\n",
    "    params = []\n",
    "    for layer in model.layers:\n",
    "        params += training.collect_trainable_weights(layer)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Snippet 09: Implementation and Refinement:: Monte Carlo Policy Gradient\n",
    "\n",
    "#### Build Training Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def monte_carlo_policy_gradient(model_v, model_p, render=False): \n",
    "    # Init epoch stats\n",
    "    stats = {}\n",
    "    \n",
    "    # Play through episodes\n",
    "    for episode in range(NUM_EPISODE): \n",
    "        # Init episode transitions & stats\n",
    "        transitions = []\n",
    "        stats[episode] = [0]*3 + [1]\n",
    "        \n",
    "        # Init observations\n",
    "        x_t = ENV.reset()\n",
    "        s_t = np.tile(x_t, STEP_MEM)\n",
    "        done = False\n",
    "\n",
    "        # Start an episode\n",
    "        for step in count():\n",
    "            \n",
    "            # Reset the environment for a new gaming step\n",
    "            if render: ENV.render()\n",
    "            \n",
    "            # Estimate probabilities for each possible action\n",
    "            probs = model_p.predict(s_t[np.newaxis])[0]\n",
    "            \n",
    "            # Take an action: Sample an action from the probabilities distribution\n",
    "            a_t = np.random.choice(ACTIONS, p=probs)\n",
    "            \n",
    "            # Observe after action a_t\n",
    "            x_t, r_t, done, info = ENV.step(a_t)\n",
    "            \n",
    "            # Create state at t1: Append x observations, throw away the earliest\n",
    "            s_t1 = np.concatenate((x_t, s_t[:(STEP_MEM-1) * NUM_INPUT,]), axis=0)\n",
    "            \n",
    "            # Keep track of the episode transition and the probability of the action taken\n",
    "            transitions.append((s_t, a_t, r_t, probs[a_t]))\n",
    "            \n",
    "            # Update statistics: Sum up rewards, count steps, successes, solved\n",
    "            stats[episode][0] += r_t\n",
    "            if r_t >= 100: stats[episode][1] += 1\n",
    "            if r_t >= 200: stats[episode][2] += 1\n",
    "            stats[episode][3] = step\n",
    "                \n",
    "            # Visualize\n",
    "            stdout.write(\"\\r{} | Step {} @ Episode {}/{} | Epsilon {} | Step Reward {}\".format(\\\n",
    "                EPOCH, step, episode, NUM_EPISODE, epsilon, round(r_t, 1)))\n",
    "\n",
    "            # Exit episode after crash or deadlock\n",
    "            if done or step > 10000: break \n",
    "            \n",
    "            # Update state\n",
    "            s_t = s_t1\n",
    "            \n",
    "        # Update policy after each episode \n",
    "        # Low bias, high variance\n",
    "        for t, transition in enumerate(transitions):\n",
    "        \n",
    "            # Calculate target for action taken\n",
    "            # -> Total discounted reward after this step during episode\n",
    "            target = sum(GAMMA**i * j[2] for i, j in enumerate(transitions[t:])) \n",
    "\n",
    "            # Update value estimator\n",
    "            model_v.fit(transition[0][np.newaxis], np.asarray([target]), verbose=0)\n",
    "\n",
    "            # Estimate baseline for action taken\n",
    "            baseline = model_v.predict(transition[0][np.newaxis])[0][0]\n",
    "\n",
    "            # Calculate advantage for action taken\n",
    "            advantage = target - baseline\n",
    "\n",
    "            # Caluculate loss\n",
    "            loss = -np.log(transition[3]) * advantage\n",
    "            \n",
    "            # Update policy estimator\n",
    "            network_params = get_trainable_params(model_p)\n",
    "            param_grad = tf.gradients(loss, network_params)\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Snippet 10: Implementation and Refinement:: Action-Value (Q) Actor-Critic\n",
    "\n",
    "#### Build Training Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def q_actor_critic(model_v, model_p, render=False): \n",
    "    # Init epoch stats\n",
    "    stats = {}\n",
    "\n",
    "    # Play through episodes\n",
    "    for episode in range(NUM_EPISODE): \n",
    "        # Init episode stats\n",
    "        stats[episode] = [0]*3 + [1]\n",
    "        \n",
    "        # Init observations\n",
    "        x_t = ENV.reset()\n",
    "        s_t = np.tile(x_t, STEP_MEM)\n",
    "        done = False\n",
    "        \n",
    "        # Start an episode\n",
    "        for step in count():\n",
    "            \n",
    "            # Reset the environment for a new gaming step\n",
    "            if render: ENV.render()\n",
    "\n",
    "            # Estimate probabilities for each possible action\n",
    "            probs = model_p.predict(s_t[np.newaxis])[0]\n",
    "            \n",
    "            # Take an action: Sample an action from the returned probabilities distribution\n",
    "            a_t = np.random.choice(ACTIONS, p=probs)\n",
    "\n",
    "            # Observe after action a_t\n",
    "            x_t, r_t, done, info = ENV.step(a_t)\n",
    "            \n",
    "            # Create state at t1: Append x observations, throw away the earliest\n",
    "            s_t1 = np.concatenate((x_t, s_t[:(STEP_MEM-1) * NUM_INPUT,]), axis=0)\n",
    "\n",
    "            # Bootstrapping from a value function during the episode: high bias, low variance\n",
    "            # Calculate target for action taken\n",
    "            # -> Reward for action taken at s_t1\n",
    "            Q_sa = model_v.predict(s_t1[np.newaxis])[0][0]\n",
    "            target = r_t + GAMMA * Q_sa\n",
    "\n",
    "            # Update value estimator\n",
    "            model_v.fit(s_t[np.newaxis], np.asarray([target]), verbose=0)\n",
    "\n",
    "            # Estimate baseline for action taken\n",
    "            baseline = model_v.predict(s_t[np.newaxis])[0][0]\n",
    "\n",
    "            # Calculate advantage for action taken\n",
    "            advantage = target - baseline\n",
    "\n",
    "            # Caluculate loss\n",
    "            loss = -np.log(probs[a_t]) * advantage\n",
    "\n",
    "            # Update policy estimator\n",
    "            network_params = get_trainable_params(model_p)\n",
    "            param_grad = tf.gradients(loss, network_params)\n",
    "            \n",
    "            # Update statistics: Sum up rewards, count steps, successes, solved, measure episode length\n",
    "            stats[episode][0] += r_t\n",
    "            if r_t >= 100: stats[episode][1] += 1\n",
    "            if r_t >= 200: stats[episode][2] += 1\n",
    "            stats[episode][3] = step\n",
    "\n",
    "            # Visualize\n",
    "            stdout.write(\"\\r{} | Step {} @ Episode {}/{} | Epsilon {} | Step Reward {}\".format(\\\n",
    "                EPOCH, step, episode, NUM_EPISODE, epsilon, round(r_t, 1)))\n",
    "\n",
    "            # Exit episode after crash or deadlock\n",
    "            if done or step > 10000: break \n",
    "\n",
    "            # Update state\n",
    "            s_t = s_t1\n",
    "            \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Snippet 11: Model Evaluation and Validation:: Setting up the infrastructure\n",
    "\n",
    "This shell file represents the last version which I used to set up the infrastructure on a pre-configured Bitfusion Ubuntu 14 TensorFlow instances. Setting up Jupyter, TensorFlow, and NVIDIA drivers are dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ## AWS Marketplace\n",
    "    ## Bitfusion Ubuntu 14 TensorFlow - Ubuntu 14.04 LTS (GNU/Linux 3.13.0-95-generic x86_64)\n",
    "    ## awsmrkt-bfboost-ubuntu14-cuda75-TensorFlow-2016-09-13-130716-dd1e96f9-9ede-4ff5-be40-3419bfca03a3-ami-ac4635bb.3 (ami-94b14cfb)\n",
    "\n",
    "    ## GO TO SERVER\n",
    "    ssh -i \"my_aws_keys.pem\" ubuntu@ec2-35-156-49-218.eu-central-1.compute.amazonaws.com\n",
    "\n",
    "    # Install the basics\n",
    "    sudo apt-get update\n",
    "    sudo apt-get upgrade\n",
    "    sudo pip install --upgrade pip\n",
    "    sudo apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb libav-tools xorg-dev python-opengl libboost-all-dev\n",
    "    \n",
    "    # Install the proper swig for Box2D to work\n",
    "    sudo apt-get install swig3.0\n",
    "    sudo rm /usr/bin/swig\n",
    "    sudo ln -s /usr/bin/swig3.0 /usr/bin/swig\n",
    "    \n",
    "    # Re-install Box2D\n",
    "    git clone https://github.com/pyBox2D/pyBox2D.git\n",
    "    cd pyBox2D\n",
    "    sudo python setup.py clean\n",
    "    sudo python setup.py build\n",
    "    sudo python setup.py install\n",
    "    \n",
    "    # Install Gym\n",
    "    cd ~\n",
    "    git clone https://github.com/openai/gym.git\n",
    "    cd gym\n",
    "    sudo pip install -e '.[all]'\n",
    "\n",
    "    # Modify the Box2D init in the Gym envs\n",
    "    cd ~\n",
    "    nano /gym/gym/envs/Box2D/__init__.py\n",
    "    # comment all but lunar lander out\n",
    "    \n",
    "    # Delete pip version of Box2D\n",
    "    sudo pip uninstall Box2D-py\n",
    "\n",
    "    ## GO TO LOCAL\n",
    "    # Send files to server\n",
    "    scp -i my_aws_keys.pem ~/CloudStation/Hack/Python/Udacity/MLE/06P_Capstone/04_DQN_PG/Capstone__Reinforcement_Learning.ipynb ubuntu@ec2-35-156-49-218.eu-central-1.compute.amazonaws.com:~/pynb/Capstone__Reinforcement_Learning.ipynb\n",
    "\n",
    "    ## GO TO SERVER\n",
    "    # Start notebook\n",
    "    cd pynb\n",
    "    jupyter notebook\n",
    "    #xvfb-run -s \"-screen 0 1400x900x24\" /bin/bash\n",
    "    # run without render()\n",
    "\n",
    "    ## GO TO LOCAL\n",
    "    # Open notebook (NOT IN SAFARI; use instance ID as password)\n",
    "    http://ec2-35-156-49-218.eu-central-1.compute.amazonaws.com:8888\n",
    "\n",
    "    # Send files to local\n",
    "    scp -i my_aws_keys.pem ubuntu@ec2-35-156-49-218.eu-central-1.compute.amazonaws.com:~/pynb/Capstone__Reinforcement_Learning.ipynb ~/CloudStation/Hack/Python/Udacity/MLE/06P_Capstone/04_DQN_PG/Capstone__Reinforcement_Learning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from os import path, makedirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Save Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAVE_PATH = path.join(path.expanduser(\"~\"), \"RL_Saves\")\n",
    "try: makedirs(SAVE_PATH)\n",
    "except OSError: \n",
    "    if not path.isdir(SAVE_PATH): raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Helper Function to Produce Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _produce_statistics(stats, stats_file=None, epoch_account=None):\n",
    "    \n",
    "    # Calculate cumulative epoch statistics\n",
    "    highest_reward = max([ v[0] for v in stats.values() ])\n",
    "    successful_steps = sum([ v[1] for v in stats.values() ])\n",
    "    solved_steps = sum([ v[2] for v in stats.values() ])\n",
    "    \n",
    "    # Visualize\n",
    "    stdout.write(\"\\rEpoch {}, Maximum Reward {}, Successful Episodes {}, Solved Episodes {}\".format(\\\n",
    "        EPOCH, highest_reward, successful_steps, solved_steps))\n",
    "\n",
    "    # Write down working settings\n",
    "    if stats_file:\n",
    "        if successful_steps > 0: \n",
    "            epoch_account[EPOCH] = [highest_reward, successful_steps, solved_steps]\n",
    "            with open(stats_file, \"w\") as outfile: json.dump(epoch_account, outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Helper Function to Get Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_statistics(stats_file):\n",
    "    stats_file_name = path.basename(stats_file)\n",
    "    best_trial = ('NA', -10000, 0)\n",
    "    with open(stats_file) as json_file: \n",
    "        stats = json.load(json_file)\n",
    "        for k, v in stats.items():\n",
    "            if v[2] > 0: print \"\\nSolved!: {} | {} | {}\".format(stats_file_name, k, v)\n",
    "            if v[1] > best_trial[2] and v[0] > best_trial[1]: best_trial = (k, v[0], v[1])\n",
    "    return \"\\nBest trial without solved: {} | {}\".format(stats_file_name, best_trial)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Snippet 12: Model Evaluation and Validation:: From Q Learning to Deep Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = _create_network_1() \n",
    "\n",
    "# Create model id\n",
    "# Insert <EPOCH = \"Q-Learning\"> for Q Learning here\n",
    "EPOCH = '_'.join([repr(STEP_MEM), str(NUM_HIDDEN_NEURON), INITIALIZATION, ACTIVATION, str(round(GAMMA, 2))])\n",
    "#EPOCH = \"Q-Learning\"\n",
    "                                \n",
    "# Train model / Play epoch\n",
    "# insert train_ql, dqn_1, dqn_2, dqn_3 according to the model to be evaluated\n",
    "stats = dqn_3(model) \n",
    "\n",
    "_produce_statistics(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Snippet 13: Justification:: Parameter Space Exploration for Deep Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPSILON_RANGE = [0.2, 0.01] \n",
    "NUM_HIDDEN_NEURON_RANGE = [100, 200, 300, 400, 500, 600] # Constant over one epoch\n",
    "STEP_MEM_RANGE = np.arange(1, 8) # Constant over one epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the Parameter Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(STEP_MEM_RANGE)*len(NUM_HIDDEN_NEURON_RANGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Space Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Init epoch overview\n",
    "epoch_account = {}\n",
    "\n",
    "# Set output file\n",
    "stats_file = path.join(SAVE_PATH, \"DQN_Stats.json\")\n",
    "\n",
    "# Apply brute force parameter space exploration\n",
    "for STEP_MEM in STEP_MEM_RANGE:\n",
    "    for NUM_HIDDEN_NEURON in NUM_HIDDEN_NEURON_RANGE:\n",
    "        # Initialize model\n",
    "        try: model = _create_network_1() \n",
    "        except Exception: continue\n",
    "\n",
    "        # Create model id\n",
    "        EPOCH = '_'.join([repr(STEP_MEM), str(NUM_HIDDEN_NEURON), INITIALIZATION, ACTIVATION])\n",
    "\n",
    "        # Train model / Play epoch\n",
    "        stats = dqn_3(model) # , render=True\n",
    "        \n",
    "        # Produce statistics\n",
    "        _produce_statistics(stats, stats_file, epoch_account)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify Working Parameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print _get_statistics(stats_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Snippet 14: Justification:: Parameter Space Exploration for Policy Gradient Methods, Step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extend Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select all available initializations and activations of Keras\n",
    "INITIALIZATIONS = ['normal', 'he_normal', 'glorot_uniform', 'uniform', 'lecun_uniform', 'identity', 'orthogonal', 'zero', 'glorot_normal', 'he_uniform']\n",
    "ACTIVATIONS = ['softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the Parameter Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print len(STEP_MEM_RANGE)*len(NUM_HIDDEN_NEURON_RANGE)*len(INITIALIZATIONS)**2*len(ACTIVATIONS)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Parameter Space Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Init epoch overview\n",
    "epoch_accounts = [{}, {}]\n",
    "\n",
    "# Set output files\n",
    "stats_files = [path.join(SAVE_PATH, \"MCPG_Stats.json\"), path.join(SAVE_PATH, \"QAC_Stats.json\")]\n",
    "\n",
    "# Apply brute force parameter space exploration\n",
    "for STEP_MEM in STEP_MEM_RANGE:\n",
    "    for NUM_HIDDEN_NEURON in NUM_HIDDEN_NEURON_RANGE:\n",
    "        for INITIALIZATION_V in INITIALIZATIONS:\n",
    "            for ACTIVATION_V in ACTIVATIONS:\n",
    "                for INITIALIZATION_P in INITIALIZATIONS:\n",
    "                    for ACTIVATION_P in ACTIVATIONS:\n",
    "                        # Initialize models\n",
    "                        try: model_v, model_p = _create_network_2('value'), _create_network_2('policy') \n",
    "                        except Exception: continue\n",
    "\n",
    "                        # Create model id\n",
    "                        EPOCH = '_'.join([repr(STEP_MEM), str(NUM_HIDDEN_NEURON), INITIALIZATION_V, INITIALIZATION_P, ACTIVATION_V, ACTIVATION_P])\n",
    "\n",
    "                        # Train model / Play epoch\n",
    "                        double_stats = (monte_carlo_policy_gradient(model_v, model_p), q_actor_critic(model_v, model_p))\n",
    "\n",
    "                        # Produce statistics\n",
    "                        for i, stats in enumerate(double_stats):\n",
    "                            _produce_statistics(stats, stats_files[i], epoch_accounts[i])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify Working Parameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for stat_file in stats_files:\n",
    "    print _get_statistics(stat_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Snippet 15: Justification:: Parameter Space Exploration for Action-Value (Q) Actor-Critic Policy Gradients, Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "#### Alter Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INITIALIZATION_V = 'he_uniform'\n",
    "INITIALIZATION_P = 'he_uniform'\n",
    "ACTIVATION_V = 'softplus'\n",
    "ACTIVATION_P = 'softplus'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Space Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Init epoch overview\n",
    "epoch_account = {}\n",
    "\n",
    "# Set output file\n",
    "stats_file = path.join(SAVE_PATH, \"QAC_Stats_V2.json\")\n",
    "\n",
    "# Apply brute force parameter space exploration\n",
    "for STEP_MEM in STEP_MEM_RANGE:\n",
    "    for NUM_HIDDEN_NEURON in NUM_HIDDEN_NEURON_RANGE:\n",
    "        # Initialize model\n",
    "        try: model_v, model_p = _create_network_2('value'), _create_network_2('policy')  \n",
    "        except Exception: raise\n",
    "\n",
    "        # Create model id\n",
    "        EPOCH = '_'.join([repr(STEP_MEM), str(NUM_HIDDEN_NEURON), INITIALIZATION_V, INITIALIZATION_P, ACTIVATION_V, ACTIVATION_P])\n",
    "\n",
    "        # Train model / Play epoch\n",
    "        stats = q_actor_critic(model_v, model_p) # , render=True\n",
    "        _produce_statistics(stats, stats_file, epoch_account)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify Working Parameter Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print _get_statistics(stats_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Snippet 16: Conclusion:: Display Learning Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run through long episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_EPISODE = 1000\n",
    "NUM_HIDDEN_NEURON = 600\n",
    "STEP_MEM = 1\n",
    "INITIALIZATION_V = 'he_uniform'\n",
    "INITIALIZATION_P = 'he_uniform'\n",
    "ACTIVATION_V = 'softplus'\n",
    "ACTIVATION_P = 'softplus'\n",
    "\n",
    "# Initialize model\n",
    "model_v, model_p = _create_network_2('value'), _create_network_2('policy') \n",
    "\n",
    "# Create model id\n",
    "EPOCH = '_'.join([repr(STEP_MEM), str(NUM_HIDDEN_NEURON), INITIALIZATION_V, INITIALIZATION_P, ACTIVATION_V, ACTIVATION_P])\n",
    "\n",
    "# Train model / Play epoch\n",
    "stats = q_actor_critic(model_v, model_p)\n",
    "with open(path.join(SAVE_PATH, \"QAC_long_epoch_stats.json\"), \"w\") as outfile: json.dump(stats, outfile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_EPISODE = 1000\n",
    "NUM_EXPLORATION_STEP = NUM_EPISODE * 20 \n",
    "\n",
    "NUM_HIDDEN_NEURON = 200\n",
    "STEP_MEM = 6 \n",
    "INITIALIZATION = 'glorot_uniform'\n",
    "ACTIVATION = 'relu'\n",
    "\n",
    "# Initialize model\n",
    "model = _create_network_1() \n",
    "\n",
    "# Create model id\n",
    "EPOCH = '_'.join([repr(STEP_MEM), str(NUM_HIDDEN_NEURON), INITIALIZATION, ACTIVATION])\n",
    "\n",
    "# Train model / Play epoch\n",
    "stats = dqn_3(model) # , render=True\n",
    "with open(path.join(SAVE_PATH, \"DQN_long_epoch_stats.json\"), \"w\") as outfile: json.dump(stats, outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the learning progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extend namespace\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read in epoch statistics\n",
    "    # 1: summed up episode reward\n",
    "    # 2: # successes\n",
    "    # 3: # solved\n",
    "    # 4: # episode steps\n",
    "    \n",
    "# Get statistics\n",
    "stats_files = [path.join(SAVE_PATH, \"DQN_long_epoch_stats.json\"), path.join(SAVE_PATH, \"QAC_long_epoch_stats.json\")]\n",
    "\n",
    "# Prepare plots\n",
    "fontsize = 14\n",
    "\n",
    "# Plot\n",
    "for i, stats_file in enumerate(stats_files):\n",
    "    with open(path.join(stats_file)) as json_file: \n",
    "        epoch_stats = json.load(json_file)\n",
    "\n",
    "    # Calculate epoch statistics\n",
    "    episodes = sorted([ int(k)+1 for k, v in epoch_stats.items() ])\n",
    "    episode_rewards = [ v[0] for k, v in epoch_stats.items() ]\n",
    "    average_episode_rewards = [ v[0] / v[3] for k, v in epoch_stats.items() ]\n",
    "    successful_steps = [ v[1] for k, v in epoch_stats.items() ]\n",
    "    solved_steps = [ v[2] for k, v in epoch_stats.items() ]\n",
    "    episode_lengths = [ v[3] for k, v in epoch_stats.items() ]\n",
    "\n",
    "    # Show success\n",
    "    print \"Number of successful steps: {}\".format(sum(successful_steps))\n",
    "    print \"Number of solved steps: {}\".format(sum(solved_steps))\n",
    "\n",
    "    # Calculate cumulative epoch statistics\n",
    "    cumulative_successful_steps = 0\n",
    "    cumulative_successful_steps_list = []\n",
    "    for successful_step in successful_steps:\n",
    "        cumulative_successful_steps += successful_step\n",
    "        cumulative_successful_steps_list.append(cumulative_successful_steps)\n",
    "    \n",
    "    # Generate\n",
    "    plt.figure(i, figsize=(20, 30), dpi=640)\n",
    "    if i == 0: plot_title = 'Deep Q Learning'\n",
    "    else: plot_title = 'Action-Value (Q) Actor-Critic'\n",
    "\n",
    "    plt.subplot(3,1,1) # numrows, numcols, fignum, where fignum ranges from 1 to numrows*numcols\n",
    "    plt.plot(episodes, average_episode_rewards, color='lightblue')\n",
    "    plt.title(plot_title)\n",
    "    plt.xlabel(\"Episodes\", fontsize=fontsize)\n",
    "    plt.ylabel('Average Reward', fontsize=fontsize)\n",
    "\n",
    "    plt.subplot(3,1,2) # numrows, numcols, fignum, where fignum ranges from 1 to numrows*numcols\n",
    "    plt.plot(episodes, episode_rewards, color='blue')\n",
    "    plt.title(plot_title)\n",
    "    plt.xlabel(\"Episodes\", fontsize=fontsize)\n",
    "    plt.ylabel('Reward', fontsize=fontsize)\n",
    "\n",
    "    plt.subplot(3,1,3) \n",
    "    plt.plot(episodes, cumulative_successful_steps_list, color='violet')\n",
    "    plt.title(plot_title)\n",
    "    plt.xlabel(\"Episodes\", fontsize=fontsize)\n",
    "    plt.ylabel('# Successes', fontsize=fontsize)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
