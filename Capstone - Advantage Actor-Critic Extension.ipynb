{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###TODO dqn, change TOTAL_EXPLORATION_STEPS to NUM_EXPLORATION_STEPS\n",
    "###TODO DQN, could vary EPSILON_RANGE as well\n",
    "###TODO DQN render=False default argument for functions\n",
    "###TODO DQN model.fit nb_epoch=10 default?\n",
    "### t vs frame in comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from keras.engine import training\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Dense, Activation\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from sys import stdout\n",
    "import json\n",
    "from os import path, makedirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(3293734)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-10-28 23:38:12,714] Making new env: LunarLander-v2\n"
     ]
    }
   ],
   "source": [
    "ENV = gym.make(\"LunarLander-v2\")\n",
    "NUM_INPUT = ENV.reset().shape[0]\n",
    "NUM_ACTION = ENV.action_space.n\n",
    "ACTIONS = np.arange(0, ACTION_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_EPISODE = 1000\n",
    "GAMMA = 0.99\n",
    "FRAME_MEM = 1\n",
    "HIDDEN_DIM = 200\n",
    "ACTIVATION_VAL = 'tanh'\n",
    "INITIALIZATION_VAL = 'glorot_uniform'\n",
    "ACTIVATION_POL ='relu'\n",
    "INITIALIZATION_POL = 'glorot_normal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _create_network(model_type): \n",
    "    model = Sequential()\n",
    "    \n",
    "    if model_type == 'value':\n",
    "        # Regression function estimate for calculating the advantage with advantage actor-critic policy gradients\n",
    "        model.add(Dense(HIDDEN_DIM, init=INITIALIZATION_VAL, input_dim=FRAME_MEM*NUM_INPUT)) \n",
    "        model.add(Activation(ACTIVATION_VAL))\n",
    "        model.add(Dense(1, init=INITIALIZATION_VAL))\n",
    "        model.compile(optimizer='rmsprop', loss='mse') ###TODO regression fchollet bookmarks, add to biblio\n",
    "    \n",
    "    if model_type == 'policy':\n",
    "        model.add(Dense(HIDDEN_DIM, init=INITIALIZATION_POL, input_dim=FRAME_MEM*NUM_INPUT)) \n",
    "        model.add(Activation(ACTIVATION_POL))\n",
    "        model.add(Dense(NUM_ACTION, init=INITIALIZATION_POL))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/fchollet/keras/issues/3062 ###TODO in biblio\n",
    "def get_trainable_params(model):\n",
    "    params = []\n",
    "    for layer in model.layers:\n",
    "        params += training.collect_trainable_weights(layer)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def advantage_actor_critic(model_val, model_pol, render=False): \n",
    "    # Start a new epoch\n",
    "    t = 0; success = 0; solved = False\n",
    "\n",
    "    # Play through episodes\n",
    "    for episode in range(NUM_EPISODE+1): \n",
    "        te = 0\n",
    "        trpre = []\n",
    "        x_t = ENV.reset()\n",
    "        s_t = np.tile(x_t, FRAME_MEM)\n",
    "        done = False\n",
    "\n",
    "        # Start an episode\n",
    "        while not done:\n",
    "            t += 1; te += 1\n",
    "            \n",
    "            # Reset the environment for a new gaming step\n",
    "            if render: ENV.render()\n",
    "            \n",
    "            # POLICY ESTIMATOR: PREDICTION\n",
    "            # Estimate probabilities for each possible action\n",
    "            probs = model_pol.predict(s_t[np.newaxis])[0]\n",
    "            \n",
    "            # Take an action: Sample an action from the probabilities distribution\n",
    "            a_t = np.random.choice(ACTIONS, p=probs)\n",
    "            \n",
    "            # Observe after action a_t\n",
    "            s_t1, r_t, done, info = ENV.step(a_t)\n",
    "            \n",
    "            # Keep track of the episode transition and the probability of the action taken\n",
    "            trpre.append((s_t, a_t, r_t, probs[a_t]))\n",
    "            \n",
    "            # Update state\n",
    "            s_t = s_t1\n",
    "        \n",
    "        # Update policy after each episode \n",
    "        for t, transition in enumerate(trpre):\n",
    "        \n",
    "            # Calculate the total discounted reward after this episode frame\n",
    "            total_return = sum(GAMMA**i * j[2] for i, j in enumerate(trpre[t:])) \n",
    "\n",
    "            # Get state at t\n",
    "            ep_s_t = transition[0][np.newaxis]\n",
    "\n",
    "            # Update value estimator\n",
    "            model_val.fit(ep_s_t, np.asarray([total_return]), verbose=0)\n",
    "\n",
    "            # Calculate baseline for the action taken / Predict the baseline\n",
    "            baseline_value = model_val.predict(ep_s_t)[0][0]\n",
    "\n",
    "            # Calculate advantage FOR THE PICKED ACTION ONLY (target / y_true)\n",
    "            advantage = total_return - baseline_value\n",
    "\n",
    "            # Update policy estimator / Compute the policy gradients\n",
    "            # loss = (-ln(prob of action taken) * advantage/target\n",
    "            # (cannot use standard model.fit keras)\n",
    "            # https://github.com/fchollet/keras/issues/3062 in references on top ###TODO\n",
    "            loss = -np.log(transition[3]) * advantage\n",
    "            network_params = get_trainable_params(model_pol)\n",
    "            param_grad = tf.gradients(loss, network_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_pol, model_val = _create_network(\"policy\"), _create_network(\"value\")\n",
    "advantage_actor_critic(model_val, model_pol) # Train model / Play episode"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
