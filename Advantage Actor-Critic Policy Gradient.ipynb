{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-10-28 23:06:07,220] Making new env: LunarLander-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'dense_input_12:0' shape=(?, 8) dtype=float32>]\n",
      "[<tf.Tensor 'add_63:0' shape=(?, 1) dtype=float32>]\n",
      "['loss']\n",
      "[<tf.Tensor 'dense_input_11:0' shape=(?, 8) dtype=float32>]\n",
      "[<tf.Tensor 'Softmax_5:0' shape=(?, 4) dtype=float32>]\n",
      "['loss', 'acc']\n",
      "[ 0.18308531  0.17087589  0.27821431  0.36782447] 1.0\n",
      "[ 0.27199739  0.19958311  0.24294177  0.28547773] 1.0\n",
      "[ 0.2735799   0.1953778   0.23712538  0.29391688] 1.0\n",
      "[ 0.26688197  0.06791279  0.35919517  0.3060101 ] 1.0\n",
      "[ 0.24937166  0.24207726  0.22923347  0.27931756] 1.0\n",
      "[ 0.27044809  0.20189556  0.24371576  0.28394064] 1.0\n",
      "[ 0.23314312  0.218219    0.22661613  0.32202175] 1.0\n",
      "[ 0.27347621  0.21600142  0.2416352   0.26888713] 1.0\n",
      "[ 0.26879242  0.17499436  0.22281675  0.33339649] 1.0\n",
      "[ 0.27970156  0.20455715  0.25126079  0.26448053] 1.0\n",
      "[ 0.18919045  0.19687845  0.26634994  0.34758124] 1.0\n",
      "[ 0.21931063  0.22331965  0.26479807  0.29257163] 1.0\n",
      "[ 0.28085199  0.21148805  0.23273998  0.27491999] 1.0\n",
      "[ 0.1907233   0.18381749  0.29057893  0.33488029] 1.0\n",
      "[ 0.27924353  0.20972851  0.22902462  0.28200334] 1.0\n",
      "[ 0.20128118  0.19426964  0.26371366  0.3407355 ] 1.0\n",
      "[ 0.25479382  0.10141483  0.33150396  0.31228748] 1.0\n",
      "[ 0.27712435  0.21073118  0.23674075  0.27540365] 1.0\n",
      "[ 0.19878848  0.21966182  0.28205451  0.29949516] 1.0\n",
      "[ 0.29883096  0.26982877  0.20519324  0.22614709] 1.0\n",
      "[ 0.24313602  0.17512621  0.25424451  0.32749328] 1.0\n",
      "[ 0.18304285  0.24508674  0.21631931  0.35555112] 1.0\n",
      "[ 0.38144389  0.27852815  0.1143786   0.22564934] 1.0\n",
      "[ 0.20823687  0.20554151  0.25616136  0.33006018] 1.0\n",
      "[ 0.27624077  0.2010328   0.23342079  0.2893056 ] 1.0\n",
      "[ 0.19048291  0.21344714  0.2693018   0.32676822] 1.0\n",
      "[ 0.27364945  0.07643653  0.3179082   0.3320058 ] 1.0\n",
      "[ 0.27378193  0.20327713  0.23919646  0.28374451] 1.0\n",
      "[ 0.27116522  0.07749152  0.32202569  0.32931757] 1.0\n",
      "[ 0.25849524  0.08913159  0.30923709  0.34313604] 1.0\n",
      "[ 0.23863006  0.23027596  0.23418108  0.29691294] 1.0\n",
      "[ 0.25865749  0.12212335  0.27146512  0.34775406] 1.0\n",
      "[ 0.21087605  0.19511899  0.26258293  0.331422  ] 1.0\n",
      "[ 0.25912726  0.13472252  0.27103585  0.33511442] 1.0\n",
      "[ 0.26036766  0.17697254  0.22789709  0.33476269] 1.0\n",
      "[ 0.27613226  0.20249774  0.22984417  0.29152578] 1.0\n",
      "[ 0.28344366  0.08258475  0.32832801  0.30564365] 1.0\n",
      "[ 0.2334564   0.23367906  0.25038269  0.28248191] 1.0\n",
      "[ 0.2451292   0.17015837  0.24627385  0.3384386 ] 1.0\n",
      "[ 0.20329279  0.2215119   0.26221165  0.31298369] 1.0\n",
      "[ 0.2689442   0.20183733  0.27723113  0.25198737] 1.0\n",
      "[ 0.27804652  0.20855652  0.23567736  0.27771956] 1.0\n",
      "[ 0.26584873  0.10242742  0.3110491   0.32067478] 1.0\n",
      "[ 0.19601113  0.19879381  0.25228822  0.35290676] 1.0\n",
      "[ 0.27933329  0.12212943  0.27630392  0.32223335] 1.0\n",
      "[ 0.21474789  0.24708669  0.2454039   0.2927615 ] 1.0\n",
      "[ 0.27825981  0.21107151  0.2481221   0.26254657] 1.0\n",
      "[ 0.25335193  0.23432975  0.23695461  0.27536374] 1.0\n",
      "[ 0.24482998  0.30403337  0.20143861  0.24969804] 1.0\n",
      "[ 0.27601326  0.22808351  0.23424782  0.26165545] 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "np.random.seed(3293734)\n",
    "\n",
    "ENV = gym.make(\"LunarLander-v2\")\n",
    "INPUT_DIM = ENV.reset().shape[0]\n",
    "N_ACTIONS = ENV.action_space.n\n",
    "ACTIONS = np.arange(0, N_ACTIONS)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.engine import training ###TODO extension\n",
    "import tensorflow as tf ###TODO extension\n",
    "\n",
    "from sys import stdout\n",
    "\n",
    "def _create_network_pol():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(HIDDEN_DIM, init='glorot_normal', input_dim=D*INPUT_DIM))  # input_shape=(D*INPUT_DIM,)\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(N_ACTIONS, init='glorot_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) # categorical_crossentropy\n",
    "    return model\n",
    "\n",
    "# Regression function estimate for calculating the baseline/advantage\t\n",
    "def _create_network_val():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, init='glorot_uniform', input_dim=D*INPUT_DIM)) # input_shape=(D*INPUT_DIM,)\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(1, init='glorot_uniform')) # input_shape=(D*INPUT_DIM,)\n",
    "    model.compile(loss='mse', optimizer='rmsprop') # Adam(lr=1e-6)\n",
    "    return model\n",
    "\n",
    "# https://github.com/fchollet/keras/issues/3062\n",
    "def get_trainable_params(model):\n",
    "    params = []\n",
    "    for layer in model.layers:\n",
    "        params += training.collect_trainable_weights(layer)\n",
    "    return params\n",
    "\n",
    "D = 1\n",
    "HIDDEN_DIM = 200\n",
    "model_pol = _create_network_pol()\n",
    "model_val = _create_network_val()\n",
    "\n",
    "print model_val.inputs\n",
    "print model_val.outputs\n",
    "print model_val.metrics_names\n",
    "\n",
    "print model_pol.inputs\n",
    "print model_pol.outputs\n",
    "print model_pol.metrics_names\n",
    "\n",
    "\n",
    "for episode in range(50):\n",
    "\n",
    "    # Instantiate interactions list\n",
    "    interactions = list()\n",
    "    discount_factor=0.99 # TODO parametrize brute force\n",
    "\n",
    "    s_t = ENV.reset()  # TODO stack\n",
    "    done = False\n",
    "\n",
    "    # Start an episode\n",
    "    while not done:\n",
    "        ###ENV.render()\n",
    "        probs = model_pol.predict(s_t[np.newaxis])[0]\n",
    "        # Take an action: Sample an action from the returned probabilities distribution\n",
    "        a_t = np.random.choice(ACTIONS, p=probs)\n",
    "        ###stdout.write('\\r'+str(a_t))\n",
    "        ###stdout.flush\n",
    "\n",
    "        # step the environment and get new measurements\n",
    "        s_t1, r_t, done, info = ENV.step(a_t)\n",
    "\n",
    "        # Keep track of the transition and the probabilities\n",
    "        interactions.append((s_t, a_t, r_t, probs))\n",
    "\n",
    "        # Update state\n",
    "        s_t = s_t1\n",
    "\n",
    "    # Ex post: Go through the episode and make policy updates\n",
    "    for t, transition in enumerate(interactions):\n",
    "        \n",
    "        # The return after this timestep (Discount reward as of frame n of game)\n",
    "        total_return = sum(discount_factor**i * j[2] for i, j in enumerate(interactions[t:])) \n",
    "        \n",
    "        # Get state at t, ex post\n",
    "        ep_s_t = transition[0][np.newaxis]\n",
    "        \n",
    "        # Update value estimator\n",
    "        model_val.fit(ep_s_t, np.asarray([total_return]), nb_epoch=10, verbose=0) # TODO nb_epoch=10\n",
    "        \n",
    "        # Calculate baseline FOR THE PICKED ACTION ONLY (Regression task)\n",
    "        baseline_value = model_val.predict(ep_s_t)[0][0]\n",
    "        \n",
    "        # Calculate advantage FOR THE PICKED ACTION ONLY  (target / y_true)\n",
    "        advantage = total_return - baseline_value\n",
    "        \n",
    "        # Update policy estimator\n",
    "        # Get the targets\n",
    "        targets = transition[3]\n",
    "        # Define the loss \n",
    "        #self.loss = -tf.log(self.picked_action_prob) * self.target\n",
    "        loss = -np.log(targets[a_t]) * advantage # -tf.log\n",
    "        # Compute the gradients\n",
    "        network_params = get_trainable_params(model_pol)\n",
    "        param_grad = tf.gradients(loss, network_params)\n",
    "        # (cannot use standard model.fit keras)\n",
    "        # https://github.com/fchollet/keras/issues/3062 in references on top\n",
    "        ###estimator_policy.update(transition.state, advantage, transition.action)\n",
    "        \n",
    "    print targets, np.sum(np.array(targets))\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
