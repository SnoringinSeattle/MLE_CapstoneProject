{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs\n",
    "\n",
    "- create a desc for each function\n",
    "- comment code properly\n",
    "- append code with full model\n",
    "- Computationally intensive! / Use Spark on AWS\n",
    "    - https://eu-central-1.console.aws.amazon.com/ec2/v2/home?region=eu-central-1#LaunchInstanceWizard:\n",
    "    - http://blog.insightdatalabs.com/spark-cluster-step-by-step/\n",
    "    - http://blog.insightdatalabs.com/jupyter-on-apache-spark-step-by-step/\n",
    "    - http://stackoverflow.com/questions/37327021/using-jupyter-notebook-on-spark-on-emr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "### Purpose\n",
    "\n",
    "- Get started with neural nets, Convolutions?, Fully-connected layers, activations . not eventually but anyway\n",
    "- See a thing learn is exciting (05 smartcab)\n",
    "- The field of ML I know least\n",
    "- NOT: 2d inputs, convolutions / different input preprocessing // too: I.astype(np.float).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credits and Thanks\n",
    "\n",
    "- Tambet Matiisen\n",
    "https://www.nervanasys.com/demystifying-deep-reinforcement-learning/ https://github.com/tambetm/simple_dqn/blob/master/src/replay_memory.py\n",
    "- Andrew Trask\n",
    "https://iamtrask.github.io\n",
    "- Eder Santana\n",
    "http://edersantana.github.io/articles/keras_rl/\n",
    "- Deep Mind\n",
    "https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "- Ben Lau\n",
    "https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html\n",
    "- Francois Chollet\n",
    "https://github.com/fchollet/keras/tree/master/examples \n",
    "https://keras.io\n",
    "- Sebastian Raschka\n",
    "http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html\n",
    "- Christopher Olah\n",
    "http://colah.github.io/posts/2014-07-Conv-Nets-Modular/ TODO\n",
    "- Karpathy\n",
    "TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Dependencies\n",
    "\n",
    "We are working with Open AI Gym (https://gym.openai.com/) as a training environment for our to-be-defined AI agent.\n",
    "\n",
    "Lunar Lander environment (https://gym.openai.com/envs/LunarLander-v2) is particularily appealing to me. It is based on box2d, which simulates real life physics. Charming! And, with its 1D input state vector, it is a first step for creating and tuning an AI agent, before we preceed with convolution preprocessing of 2D inputs.\n",
    "\n",
    "The environment home page says the following:\n",
    "\n",
    "*\"Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine.\"*\n",
    "\n",
    "Model building and training is done with Keras (https://keras.io). This modular, minimalist library makes ANN life as easy as it can get, and in plus runs on both Theano and Tensorflow backends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: The Starting Point: Q-Learning, or: Evaluating Bellman Equations from data\n",
    "\n",
    "Q learning is about revisiting states. We are in a specific state s at time t, and because the state space is sufficiently small, we might discover that the agent has already been in s before. It therefore has made an experience for s by taking an action, and collecting a reward or punishment. All this is stored in the agent's \"memory\" (possible a dictionary of dictionaries, main keys being the states, values being the actions (keys) and their values). We want the agent to take advantage of this \"memory\": We look up the expected lifetime rewards per each possible action in s (a.k.a. action-value function, q values), select the maximum q value, and execute the chosen action. \n",
    "\n",
    "We now have fresh evidence about the consequence of a specific action in a specific state: We know the initial state s_t, we know the selected action a_t, we know the reward r_t, and we know the new state this all lead to, s_t1. This knowledge we now use to update the agent's memory: We calculate a new q value for s_t by taking the observed reward r_t, and adding to it the discounted maximum q value for s_t1. The difference to the old q value is the new q value for action a_t in state s_t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Q-Learning Often Does Not Work: Exploding State Spaces\n",
    "\n",
    "Revisiting states is often not possible, even in the long run, because there are simply too many combinations of relevant inputs which constitute a state. Just think a small number of inputs, each input being a floating number with 4 digits. Even this small setting is creating a large amount of combinations: the state space explodes. Revisiting states is very unlikely, we will need a huge number of trials to generate memory updates. As a consequence, learning is slow, or even not happening.\n",
    "\n",
    "In order to illustrate the point, we are going to set up a basic q-learning algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(3293734)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GAMMA is the factor by which future expected rewards are discounted\n",
    "- ALPHA is the learning rate TODO\n",
    "- N_EPISODES denotes the maximum number of episodes an epoch will embrace\n",
    "- Q_TABLE is the agent's memory. The states are the keys, and dictionaries of actions and their respective values are the values\n",
    "- VALUE_INIT is the initial value for the actions of a state, once the state is visited the first time. It is set to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "ALPHA = 0.1\n",
    "N_EPISODES = 10000\n",
    "Q_TABLE = {}\n",
    "VALUE_INIT = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Environment\n",
    "\n",
    "We create an instance ENV of the Lunar lander environment. Its input dimensions INPUT_DIM are obtained by resetting the environment, and the actions by the environment method *action_space*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-10-20 23:01:35,123] Making new env: LunarLander-v2\n"
     ]
    }
   ],
   "source": [
    "ENV = gym.make(\"LunarLander-v2\")\n",
    "INPUT_DIM = ENV.reset().shape[0]\n",
    "N_ACTIONS = ENV.action_space.n\n",
    "ACTIONS = np.arange(0, N_ACTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Training Epoch\n",
    "\n",
    "We are going to create the main building block of this exercise: The training block for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_ql(render):\n",
    "    # Start a new epoch\n",
    "    # Bookkeeping\n",
    "    # Init counter for how many times we revisited states\n",
    "    revisiting_states = 0 \n",
    "    # Count # of episodes, # successes (episode with >= 100 reward), and if the environment is solved\n",
    "    episode = 0; success = 0; solved = False \n",
    "\n",
    "    # Play through episodes\n",
    "    for episode in range(N_EPISODES): \n",
    "        \n",
    "        # Reset environment and get first observation\n",
    "        x_t = ENV.reset() \n",
    "        # Take only the current observation as state, in order to keep the state space as small as possible\n",
    "        s_t = tuple(x_t) \n",
    "        done = False\n",
    "\n",
    "        # Continue playing until done\n",
    "        while not done: \n",
    "            # Reset the environment for a new gaming step\n",
    "            if render: ENV.render() \n",
    "            \n",
    "            # Look up the action with the highest value at s_t, as well as its value\n",
    "            a_t, q, r_s  = best_action(s_t)\n",
    "\n",
    "            # Observe after action a_t\n",
    "            x_t, r_t, done, info = ENV.step(a_t) \n",
    "            # Again, take only the current observation\n",
    "            s_t1 = tuple(x_t) \n",
    "            \n",
    "            # Look up the action with the highest value at s_t1, as well as its value\n",
    "            a_t1, Q_sa, _ = best_action(s_t1) \n",
    "            \n",
    "            # Look up the action with the highest value at s_t1, as well as its value\n",
    "            Q_TABLE[s_t][a_t] = q + ALPHA * (r_t + GAMMA * Q_sa - q) \n",
    "            \n",
    "            # Update\n",
    "            s_t = s_t1\n",
    "            episode += 1\n",
    "            revisiting_states += r_s\n",
    "            \n",
    "            # Bookkeeping\n",
    "            if r_t >= 100: success += 1\n",
    "            if r_t >= 200: \n",
    "                solved = True\n",
    "                break\n",
    "                \n",
    "    print \"Number of states in Q table: {}, Number of revisited states: {}, Successes {}, Solved {}\".format(len(Q_TABLE), revisiting_states, success, solved)\n",
    "\n",
    "# Create helper function to initialize and query Q-table\n",
    "def best_action(state):\n",
    "    # Create init scenario for table queries at t and t1\n",
    "    if state not in Q_TABLE or sum(Q_TABLE[state].values()) == 0: \n",
    "        # Bookkeeping: Init revisiting states counter\n",
    "        revisit_state = 0 \n",
    "        # Init q function\n",
    "        q_function = {} \n",
    "        for A in ACTIONS: q_function[A] = VALUE_INIT\n",
    "        Q_TABLE[state] = q_function \n",
    "        # Do random action\n",
    "        action = np.random.choice(ACTIONS, 1)[0] \n",
    "    else: \n",
    "        revisit_state = 1\n",
    "        # Select action according to max q\n",
    "        action = max(Q_TABLE[state], key=Q_TABLE[state].get) \n",
    "    # Get q value for action selected\n",
    "    q = Q_TABLE[state][action] \n",
    "    return action, q, revisit_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###train_ql(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aftermath\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: What to Do? Do Not Update the Q Function, But the Q  Function Estimator: Deep Q Learning\n",
    "\n",
    "In this situation, we replace the \"revisiting states\" by a function approximator: \n",
    "\n",
    "We let a Artifical Neural Net (ANN) estimate the q function for the state s the agent is visiting at time t. \n",
    "\n",
    "Once we performed the action based on the maximum of the q function (just the action with the highest expected lifetime reward at time t), we know the reward, and the subsequent state. \n",
    "\n",
    "Based on this, we are able to update the agent's memory. But this time, we do not update the q function directly. Instead, we are updating the ANN, which means that we are updating the weights used in the ANN. And that is how exactly:\n",
    "\n",
    "- At time t, we already know the estimation of the q function for state s_t: We used it to pick an action a_t accordingly. Read again: this is the ESTIMATION of the q function.\n",
    "\n",
    "- After action a_t, we know s_t, a_t, r_t and s_t1. This allows us to update the q function, BUT ONLY FOR THE ACTION TAKEN. We take r_t, and add to it the discounted expected lifetime reward, in other words we let the ANN estimate the q function for state s_t1. \n",
    "\n",
    "- For the action taken, we can update the q value now. All the other actions are not performed, we do not know about the reward, or a subsequent state s_t1. So, we cannot learn for those. This updated q function is the TARGET.\n",
    "\n",
    "- We feed the error, which is the difference between the ESTIMATION and the TARGET.\n",
    "\n",
    "- We backpropagate the error through the network, such that the weights are updated.\n",
    "\n",
    "Next time we estimate the q function for another state s, we have updated weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Gist of ANNs\n",
    "\n",
    "Firstly, I wanted to understand, what a neural network really does. [iamtrask]'s excellent toy examples helped me understand it completely. I create a network with one hidden layer, and the output layer, both sigmoid activated, which returns probablities for each of the outputs.\n",
    "\n",
    "layer_2_wloss (\"weighted loss\") is where the magic happens: Each output loss is multiplied by the slope / gradient of the predicted value on the sigmoid curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create A Basic ANN With Only Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Size  1, alpha 0.01: Final avg loss   0.490319423746, Improvement 8.732919899e-05\n",
      "Hidden Size  1, alpha  0.1: Final avg loss   0.491668642877, Improvement 0.00133784209108\n",
      "Hidden Size  1, alpha    1: Final avg loss   0.487484240232, Improvement -0.00431861155987\n",
      "Hidden Size  1, alpha   10: Final avg loss   0.479636240829, Improvement -0.00735965114923\n",
      "Hidden Size  1, alpha  100: Final avg loss   0.537309747563, Improvement 0.0577621131427\n",
      "Hidden Size  1, alpha 1000: Final avg loss   0.500000000008, Improvement 1.47215573065e-13\n",
      "Hidden Size  1, alpha 1000: Final avg loss   0.500000000009, Improvement 2.90878432452e-14\n",
      "Hidden Size  2, alpha 0.01: Final avg loss    0.49179645197, Improvement 0.000703792555061\n",
      "Hidden Size  2, alpha  0.1: Final avg loss   0.494399207777, Improvement 0.0025300856342\n",
      "Hidden Size  2, alpha    1: Final avg loss   0.444229209816, Improvement -0.0501898542302\n",
      "Hidden Size  2, alpha   10: Final avg loss   0.279860079398, Improvement -0.148380116168\n",
      "Hidden Size  2, alpha  100: Final avg loss   0.256014956256, Improvement -0.0227598082233\n",
      "Hidden Size  2, alpha 1000: Final avg loss   0.252320253907, Improvement -0.0035140846321\n",
      "Hidden Size  2, alpha 1000: Final avg loss   0.251699026267, Improvement -0.000601443115939\n",
      "Hidden Size  3, alpha 0.01: Final avg loss    0.47591609619, Improvement 0.000389501989701\n",
      "Hidden Size  3, alpha  0.1: Final avg loss   0.490033105039, Improvement 0.0140239351916\n",
      "Hidden Size  3, alpha    1: Final avg loss   0.411692423906, Improvement -0.0787712813293\n",
      "Hidden Size  3, alpha   10: Final avg loss   0.277560612358, Improvement -0.122009033795\n",
      "Hidden Size  3, alpha  100: Final avg loss   0.253422885827, Improvement -0.0267818957236\n",
      "Hidden Size  3, alpha 1000: Final avg loss   0.251684460969, Improvement -0.0016030912331\n",
      "Hidden Size  3, alpha 1000: Final avg loss   0.251322149446, Improvement -0.000351340974615\n"
     ]
    }
   ],
   "source": [
    "# Create the sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Create the sigmoid function's derivative\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "hidden_sizes = np.arange(1, 4)\n",
    "training_steps = 100\n",
    "\n",
    "alphas = [0.01, 0.1, 1, 10, 100, 1000, 1000]\n",
    "\n",
    "X = np.random.randint(2, size=(4, 3))\n",
    "y = np.random.randint(2, size=(4, 1))\n",
    "\n",
    "# Test the hidden sizes\n",
    "for hidden_size in hidden_sizes: \n",
    "    \n",
    "    # Initialize 1st set of weights\n",
    "    W1 = np.random.rand(X.shape[1], hidden_size) \n",
    "\n",
    "    # Initialize 2nd set of weights\n",
    "    W2 = np.random.rand(hidden_size, y.shape[1]) \n",
    "    \n",
    "    # Test the alphas\n",
    "    for alpha in alphas: \n",
    "        \n",
    "        for i in range(training_steps):\n",
    "            # Forward propagate\n",
    "            # Initialize hidden layer (fully connected)\n",
    "            layer_1 = np.dot(X, W1)\n",
    "            # Apply sigmoid activation\n",
    "            layer_1 = sigmoid(layer_1) \n",
    "            \n",
    "            # Initialize output layer(fully connected)\n",
    "            layer_2 = np.dot(layer_1, W2) \n",
    "            # Apply sigmoid activation \n",
    "            layer_2 = sigmoid(layer_2) \n",
    "\n",
    "            # Get loss\n",
    "            layer_2_loss = y - layer_2 \n",
    "\n",
    "            ''' Apply SGD to the loss: the more certain the estimate, the less weighted it will get: \n",
    "                The gradient at the extremes is smaller than in the middle\n",
    "            '''\n",
    "            layer_2_wloss = layer_2_loss * sigmoid_derivative(layer_2) # element-wise multiplication!\n",
    "            \n",
    "            # Backpropagate\n",
    "            # Compute the effect of the hidden layer to the weighted loss\n",
    "            layer_1_loss = np.dot(layer_2_wloss, W2.T) \n",
    "\n",
    "            # Apply SGD\n",
    "            layer_1_wloss = layer_1_loss * sigmoid_derivative(layer_1) \n",
    "                \n",
    "            # Update weights\n",
    "            W2 += alpha * np.dot(layer_1.T, layer_2_wloss)\n",
    "            W1 += alpha * np.dot(X.T, layer_1_wloss)\n",
    "\n",
    "            if i == 1: first_error = np.mean(np.abs(layer_2_loss))\n",
    "            if i == training_steps - 1: print \"Hidden Size {}, alpha {}: Final avg loss {}, Improvement {}\".format(\n",
    "                                                '%2s' % hidden_size, \\\n",
    "                                                '%4s' % alpha, \\\n",
    "                                                '%16s' % np.mean(np.abs(layer_2_loss)), \\\n",
    "                                                np.mean(np.abs(layer_2_loss)) - first_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aftermath\n",
    "\n",
    "TODO alpha, layer size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q Network Step by Step. Step 1: Deep Q Learning from Single Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Namespace (Extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Hyperparameters (Extension)\n",
    "\n",
    "We start with a first set of static hyperparameters. Some of them will undergo changes along the way:\n",
    "\n",
    "- D_RANGE is the number of time steps the agent should take into account as the current state it is in: This is the \"operational\" memory of the agent. I will refer to it as time step memory.\n",
    "- Note that we do not use ALPHA, Q_TABLE, and Q_INIT anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D_RANGE = np.arange(1, 31) # Constant over one epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Keras Model\n",
    "\n",
    "- one fully connected layer\n",
    "- multicat output (softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _create_network():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, init='glorot_normal', input_shape=(D*INPUT_DIM,))) #default is: init='glorot_uniform' \n",
    "    model.add(Dense(N_ACTIONS, init='glorot_normal', activation='softmax')) # default is: init='glorot_uniform' \n",
    "    model.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Build Training Epoch\n",
    "  \n",
    "TODO modified; in what sense, compared to 00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_dqn1(model, render):\n",
    "    # Start a new epoch\n",
    "    episode = 0; success = 0; solved = False\n",
    "\n",
    "    for episode in range(N_EPISODES):\n",
    "        \n",
    "        x_t = ENV.reset()\n",
    "        # Pile up four times the same init observation, in order to be consistent with the model input\n",
    "        s_t = np.tile(x_t, D)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if render: ENV.render()\n",
    "    \n",
    "            # Estimate rewards for each action (targets), at s_t\n",
    "            q = model.predict(s_t[np.newaxis])[0] \n",
    "\n",
    "            # Take action with highest estimated reward (argmax returns index)\n",
    "            a_t = np.argmax(q) \n",
    "            \n",
    "            # Observe after action a_t\n",
    "            x_t, r_t, done, info = ENV.step(a_t) \n",
    "            \n",
    "            # Create state at t1: Append x observations, throw away the earliest\n",
    "            s_t1 = np.concatenate((x_t, s_t[:(D-1) * INPUT_DIM,]), axis=0) \n",
    "\n",
    "            # Estimate rewards for each action (targets), at s_t1 (again a forward pass)\n",
    "            Q_sa = model.predict(s_t1[np.newaxis])[0] \n",
    "\n",
    "            ''' Create reference/targets by updating estimated reward for chosen action:\n",
    "                For action taken, replace estimated reward by remaining cumulative lifetime reward\n",
    "            ''' \n",
    "            targets = q\n",
    "            targets[a_t] = r_t + GAMMA * np.max(Q_sa) if not done else r_t\n",
    "\n",
    "            ''' Learn!\n",
    "                - Again, predict q values for state s_t\n",
    "                - Calculate loss by comparing predictions to targets: they will differ only for the action taken\n",
    "                - backpropagate error for action taken, update weights\n",
    "            ''' \n",
    "            model.fit(s_t[np.newaxis], targets[np.newaxis], nb_epoch=10, verbose=0)\n",
    "\n",
    "            # Update\n",
    "            s_t = s_t1\n",
    "            episode += 1\n",
    "        \n",
    "            # Bookkeeping\n",
    "            if r_t >= 100: success += 1\n",
    "            if r_t >= 200: \n",
    "                solved = True\n",
    "                break\n",
    "\n",
    "    print \"D {}, Successes {}, Solved {}\".format(D, success, solved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model\n",
    "\n",
    "What can learn from this basic implementation? Surely, does it make the impression to learn at all? We also should think of the agent's time step memory. Does it make sense to let the agent know only its current state, or shall we allow him to take into consideration also some of the states before? If yes, how far back should it remember? To get a first hint to answer this question, let's run a simulation. We loop over a range of D candidates, and produce some summary statistics in order to assess the performance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor D in D_RANGE:\\n    model = _create_network() # Initialize model\\n    train_dqn1(model, render=True) # Train model\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for D in D_RANGE:\n",
    "    model = _create_network() # Initialize model\n",
    "    train_dqn1(model, render=True) # Train model\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aftermath\n",
    "\n",
    "That is quite working nicely already! The frantic, purpose-less behaviour is gone most of the time, or is vanishing quickly within a few episodes only.\n",
    "\n",
    "Some observations: \n",
    "- Too little time step memory? With only one frame in the frame memory, I often observed extensive swinging movement: the agent tries to correct with the left or right engine, fires too much, the lander is tipping over to the opposite side. Then, it corrects again, and again, and from all these corrections forgets to fire against the moon's gravity, and the agent is crashing into the surface. TODO Which are the best?\n",
    "- Local minima. There are plenty of times one can see the agent trapped into a locally optimal policy. For example, it stays on the ground, engaging left and right engine forever, perfectly stable, but not reaching the ultimate goal. Or a setting where left and right engines are engaged, but the lower engine does not fire at all, over long episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Deep Q Network Step by Step.  Step 2: *Epsilon Greedy* Deep Q Learning from Single Observations\n",
    "\n",
    "At this point, let us tackle the issue of getting stuck in local minima. As a remedy, Reinforcement Learning makes use the so called *epsilon greedy* action selection policy. It allows for a random move with probability epsilon, and by that introduces the notion of exploration (random moves) vs. exploitation (act on estimation of the q function). \n",
    "\n",
    "*Exploration* will reduce the probability of getting stuck in local minima, which are *not* reflecting the best action given a certain experience level of the agen. It's just like fresh air for the AI brain, introducing random ideas from outside. \n",
    "\n",
    "On the other hand, the agent needs to train and get experience with his selected moves. It needs evidence that one decision was (not) the right one, and to update its decicion finding process (the weights of the ANN. It only gets it by acting according to its own decisions undisturbed by random inputs. This is where *exploitation* comes in.\n",
    "\n",
    "In RL, usually epsilon decreases over a certain exploration period. This reflects the idea that the agent will start with many random moves to fathom the environment by just observing. With time and growing experience, it will decrease the share of random moves, since it feels more confident in its own decisions. \n",
    "\n",
    "I will follow th custom of allowing random moves at a linearly decreasing exploration rate during the exploration period. The dqn paper TODO is starting with epsilon = 1 / complete randomness. Let us see what brings good results here. Balancing exploration and exploitation is in itself subject to learning.\n",
    "\n",
    "Concretely, we now define a interval between the maximum and minimum epsilon allowed: EPSILON_RANGE.\n",
    "\n",
    "In order to implement exploration and exploitation, we need to keep track of the number of time steps only. We thus stop counting episodes (N_EPISODE) and establish global settings for the total number of time steps (TOTAL_TIME_STEPS) and the total number of exploration time steps (TOTAL_EXPLORATION_STEPS).\n",
    "\n",
    "Again following TODO the dqn paper, I assign 1/10th of the total time steps to exploration, the rest to training.\n",
    "\n",
    "During training, the agent will run on the min epsilon constantly.\n",
    "\n",
    "#### Set Hyperparameters (Extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TOTAL_TIME_STEPS = 1e2 * 4 # TODO dqn 10**7\n",
    "TOTAL_EXPLORATION_STEPS = TOTAL_TIME_STEPS / 10\n",
    "EPSILON_RANGE = [0.5, 0.0001] # TODO dqn [1, 0.01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Training Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_dqn2(model, render): \n",
    "    # Start a new epoch\n",
    "    episode = 0; success = 0; solved = False\n",
    "    epsilon = EPSILON_RANGE[0] ##NEW Initialize epsilon at its maximum value\n",
    "\n",
    "    #for episode in range(N_EPISODES): ###NEW Discard the episode loop\n",
    "    while episode <= TOTAL_TIME_STEPS: ###NEW Install a loop over all time steps\n",
    "        \n",
    "        x_t = ENV.reset()\n",
    "        s_t = np.tile(x_t, D)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if render: ENV.render()\n",
    "            \n",
    "            ###NEW Anneal random exploration rate epsilon over exploration period\n",
    "            epsilon = epsilon - epsilon / TOTAL_EXPLORATION_STEPS if epsilon > EPSILON_RANGE[1] else EPSILON_RANGE[1]\n",
    "    \n",
    "            # Estimate rewards for each action (targets), at s_t\n",
    "            q = model.predict(s_t[np.newaxis])[0]\n",
    "\n",
    "            # Take action with highest estimated reward \n",
    "            ###NEW Do this with probability 1-epsilon (\"epsilon greedy\" policy)\n",
    "            a_t = np.argmax(q) if np.random.random() > epsilon else np.random.choice(ACTIONS, 1)[0]\n",
    "\n",
    "            # Observe after action a_t\n",
    "            x_t, r_t, done, info = ENV.step(a_t)\n",
    "        \n",
    "            # Create state at t1: Append x observations, throw away the earliest\n",
    "            s_t1 = np.concatenate((x_t, s_t[:(D-1) * INPUT_DIM,]), axis=0)\n",
    "\n",
    "            # Estimate rewards for each action (targets), at s_t1 (again a forward pass)\n",
    "            Q_sa = model.predict(s_t1[np.newaxis])[0]\n",
    "\n",
    "            ''' Create reference/targets by updating estimated reward for chosen action\n",
    "                For action taken, replace estimated reward by remaining cumulative lifetime reward\n",
    "            ''' \n",
    "            targets = q\n",
    "            targets[a_t] = r_t + GAMMA * np.max(Q_sa) if not done else r_t\n",
    "\n",
    "            ''' Learn!\n",
    "                - Again, predict q values for state s_t\n",
    "                - Calculate loss by comparing predictions to targets: they will differ only for the action taken\n",
    "                - backpropagate error for action taken, update weights\n",
    "            ''' \n",
    "            model.fit(s_t[np.newaxis], targets[np.newaxis], nb_epoch=10, verbose=0)\n",
    "\n",
    "            # Update state and episode\n",
    "            s_t = s_t1\n",
    "            episode += 1\n",
    "        \n",
    "            # Bookkeeping\n",
    "            if r_t >= 100: success += 1\n",
    "            if r_t >= 200: \n",
    "                solved = True\n",
    "                break\n",
    "\n",
    "    print \"D {}, Successes {}, Solved {}\".format(D, success, solved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor D in D_RANGE:\\n    # Initialize model\\n    model = _create_network()\\n    # Train model\\n    train_dqn2(model, render=True)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for D in D_RANGE:\n",
    "    # Initialize model\n",
    "    model = _create_network()\n",
    "    # Train model\n",
    "    train_dqn2(model, render=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Aftermath\n",
    "\n",
    "TODO\n",
    "It gets shakier. It definitively needs more time to learn. This is the cost, at which exploration comes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q Network Step by Step. Step 3: Deep Q Learning *from Stored Experiences* \n",
    "\n",
    "It has been shown that learning on the fly from observations XXX TODO. dQN, TODO. Instead, the trick is to learn from a memory storage in batches, the so called Experience Replay Memory (ERM). \n",
    "\n",
    "We are thus going to create the main database of the agent: It is the place where it \n",
    "- stores states and its experiences with the states (transitions s_t, a_t, r_t,and s_t1)\n",
    "- recalls on the memory, collects a memory sample, trains on the sample, and updates the Q function estimator.\n",
    "\n",
    "The ERM is set up once per epoch and is fed at each time step with fresh transition evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Namespace (Extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Hyperparameters (Extension)\n",
    "\n",
    "ERM_SIZE is setting the size of the experience replay memory. Following the recommendation of the dqn TODO, we set it equal to the number of exploration steps. The BATCH_SIZE denotes the size of the sample which is drawn uniformly without replacement from the ERM at each time step. Again, its size is following the recommendations of the dqn paper TODO. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ERM_SIZE = TOTAL_EXPLORATION_STEPS\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Training Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_dqn3(model, render):\n",
    "    # Start a new epoch\n",
    "    episode = 0; success = 0; solved = False\n",
    "    epsilon = EPSILON_RANGE[0]\n",
    "    ERM = deque(maxlen=ERM_SIZE) ###NEW If too long, throw away the earliest (latest is ERM[-1])\n",
    "\n",
    "    while episode <= TOTAL_TIME_STEPS:\n",
    "        \n",
    "        x_t = ENV.reset()\n",
    "        s_t = np.tile(x_t, D)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if render: ENV.render()\n",
    "            \n",
    "            # Anneal random exploration rate epsilon over exploration period\n",
    "            epsilon = epsilon - epsilon / TOTAL_EXPLORATION_STEPS if epsilon > EPSILON_RANGE[1] else EPSILON_RANGE[1]\n",
    "    \n",
    "            # Estimate rewards for each action (targets), at s_t\n",
    "            q = model.predict(s_t[np.newaxis])[0]\n",
    "\n",
    "            # Take action with highest estimated reward, \"epsilon greedy\"\n",
    "            a_t = np.argmax(q) if np.random.random() > epsilon else np.random.choice(ACTIONS, 1)[0]\n",
    "\n",
    "            # Observe after action a_t\n",
    "            x_t, r_t, done, info = ENV.step(a_t)\n",
    "        \n",
    "            # Create state at t1: Append x observations, throw away the earliest\n",
    "            s_t1 = np.concatenate((x_t, s_t[:(D-1) * INPUT_DIM,]), axis=0)\n",
    "            \n",
    "            ###NEW Store transition in experience replay memory\n",
    "            ERM.append((s_t, a_t, r_t, s_t1))\n",
    "\n",
    "            ###NEW Choose a batch of maximum length BATCH_SIZE\n",
    "            minibatch = np.array([ ERM[i] for i in np.random.choice(np.arange(0, len(ERM)), min(len(ERM), BATCH_SIZE)) ])\n",
    "            \n",
    "            ###NEW Compute targets/reference for each transition in minibatch\n",
    "            inputs = deque(); targets = deque()\n",
    "            for m in minibatch:\n",
    "                inputs.append(m[0]) # Append s_t of batch transition m to inputs\n",
    "                m_q    = model.predict(m[0][np.newaxis])[0] # Estimate rewards for each action (targets), at s_t\n",
    "                m_Q_sa = model.predict(m[3][np.newaxis])[0] # Estimate rewards for each action (targets), at s_t1\n",
    "                m_targets = m_q\n",
    "                m_targets[m[1]] = m[2] + GAMMA * np.max(m_Q_sa)\n",
    "                targets.append(m_targets) # Append target of batch transition m to targets\n",
    "                \n",
    "            ###NEW Train the model by backpropagating the errors and update weights\n",
    "            model.train_on_batch(np.array(inputs), np.array(targets))\n",
    "            \n",
    "            # Update state and episode\n",
    "            s_t = s_t1\n",
    "            episode += 1\n",
    "        \n",
    "            # Bookkeeping\n",
    "            if r_t >= 100: success += 1\n",
    "            if r_t >= 200: \n",
    "                solved = True\n",
    "                break\n",
    "\n",
    "    print \"D {}, Successes {}, Solved {}\".format(D, success, solved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor D in D_RANGE:\\n    # Initialize model\\n    model = _create_network()\\n    # Train model\\n    train_dqn3(model, render=True)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for D in D_RANGE:\n",
    "    # Initialize model\n",
    "    model = _create_network()\n",
    "    # Train model\n",
    "    train_dqn3(model, render=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aftermath\n",
    "\n",
    "The question crossed my mind: Why don't we predict beforehand, on the fly, at every time step? Would that not be computationally efficient? This has the huge disadvantage that we predict with the knowledge available at timestep t. This might be faulty, and the faulty prediction stays as target reference in the batch, and is used to compare the loss for the taken action between the prediction at timestep t and the potenitially long ago target estimation. This will bias the learning process significantly. Thus, we select a batch, calculate the estimations and targets for the complete batch, both with the knowledge of the current time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q Network Step by Step. Step 4: Deep Q Learning from Stored Experiences, *Refined*\n",
    "\n",
    "At this point, we will finalize training by applying some more tweaks (outside the model block). The goal is to check parameter combinations by brute force. We install some extra bookeeping, and will save a model for every training method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Namespace (Extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sys import stdout\n",
    "import json\n",
    "from keras.models import model_from_json\n",
    "from os import getcwd, path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Hyperparameters (Extension)\n",
    "\n",
    "- Discount factor gamma. We will replace the fixed hyperparameter by an arbitrary range of gamma candidates (GAMMA_range).\n",
    "- Take action every n-th time step (time step per action TSPA_RANGE) TODO\n",
    "- Reward Clipping (R_CLIP). TODO\n",
    "- SAVE_PATH is the working directory where the training epoch models are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GAMMA_RANGE = np.arange(0.0, 1.01, 0.01) # Constant over one epoch\n",
    "TSPA_RANGE = np.arange(1, 5) # Constant over one epoch\n",
    "R_CLIP = [False, True] # Constant over one epoch\n",
    "SAVE_PATH = getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Training Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_dqn4(model, render):\n",
    "    # Start a new epoch\n",
    "    episode = 0; success = 0; solved = False\n",
    "    epsilon = EPSILON_RANGE[0]\n",
    "    ERM = deque(maxlen=ERM_SIZE)\n",
    "    MH5 = path.join(SAVE_PATH, \"Models\", MODEL_ID+\".h5\") ###NEW Define path and name of h5 container\n",
    "    MJS = path.join(SAVE_PATH, \"Models\", MODEL_ID+\".json\") ###NEW Define path and name of json container\n",
    "\n",
    "    while episode <= TOTAL_TIME_STEPS:\n",
    "        \n",
    "        x_t = ENV.reset()\n",
    "        s_t = np.tile(x_t, D)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if render: ENV.render()\n",
    "            \n",
    "            # Anneal random exploration rate epsilon over exploration period\n",
    "            epsilon = epsilon - epsilon / TOTAL_EXPLORATION_STEPS if epsilon > EPSILON_RANGE[1] else EPSILON_RANGE[1]\n",
    "    \n",
    "            # Estimate rewards for each action (targets), at s_t\n",
    "            q = model.predict(s_t[np.newaxis])[0]\n",
    "\n",
    "            # Take action with highest estimated reward, \"epsilon greedy\"\n",
    "            ###NEW Act only every n-th time step\n",
    "            if episode % TSPA == 0: a_t = np.argmax(q) if np.random.random() > epsilon else np.random.choice(ACTIONS, 1)[0]\n",
    "\n",
    "            # Observe after action a_t\n",
    "            x_t, r_t, done, info = ENV.step(a_t)\n",
    "            \n",
    "            ###NEW Clip rewards\n",
    "            if R_CLIP and r_t != 0: r_t = abs(r_t) / r_t\n",
    "            \n",
    "            # Create state at t1: Append x observations, throw away the earliest\n",
    "            s_t1 = np.concatenate((x_t, s_t[:(D-1) * INPUT_DIM,]), axis=0)\n",
    "            \n",
    "            # Store transition in experience replay memory\n",
    "            ERM.append((s_t, a_t, r_t, s_t1))\n",
    "\n",
    "            # Choose a batch of maximum length BATCH_SIZE\n",
    "            minibatch = np.array([ ERM[i] for i in np.random.choice(np.arange(0, len(ERM)), min(len(ERM), BATCH_SIZE)) ])\n",
    "            \n",
    "            # Compute targets/reference for each transition in minibatch\n",
    "            inputs = deque(); targets = deque()\n",
    "            for m in minibatch:\n",
    "                inputs.append(m[0]) # Append s_t of batch transition m to inputs\n",
    "                m_q    = model.predict(m[0][np.newaxis])[0] # Estimate rewards for each action (targets), at s_t\n",
    "                m_Q_sa = model.predict(m[3][np.newaxis])[0] # Estimate rewards for each action (targets), at s_t1\n",
    "                m_targets = m_q\n",
    "                m_targets[m[1]] = m[2] + GAMMA * np.max(m_Q_sa)\n",
    "                targets.append(m_targets) # Append target of batch transition m to targets\n",
    "                \n",
    "            # Train the model by backpropagating the errors and update weights\n",
    "            model.train_on_batch(np.array(inputs), np.array(targets))\n",
    "                \n",
    "            ###NEW Save progress every 100 iterations\n",
    "            if episode % 100 == 0:\n",
    "                model.save_weights(MH5, overwrite=True)\n",
    "            with open(MJS, \"w\") as outfile: json.dump(model.to_json(), outfile)\n",
    "            \n",
    "            # Update state and episode\n",
    "            s_t = s_t1\n",
    "            episode += 1\n",
    "        \n",
    "            # Bookkeeping\n",
    "            if r_t >= 100: success += 1\n",
    "            if r_t >= 200: \n",
    "                solved = True\n",
    "                break\n",
    "\n",
    "    return success, solved ###NEW Return values for epoch's bookkeeping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmax_success = 0 ###NEW Establish counter to detect champion parameter settings\\nbest_epochs = deque(maxlen=50) ###NEW Establish a list to catch the 50 best performing parameter settings and their performance\\nepoch = 0\\n\\nprint \"Session contains {} epochs\\n\".format(len(D_RANGE)*len(GAMMA_RANGE)*len(TSPA_RANGE)*len(R_CLIP))\\n\\nfor D in D_RANGE:\\n    # Initialize model\\n    model = _create_network()\\n    for GAMMA in GAMMA_RANGE: \\n        for TSPA in TSPA_RANGE:\\n            for R_C in R_CLIP:\\n                # Count epochs\\n                epoch += 1\\n                # Create model id\\n                MODEL_ID = repr(D)+\"_\"+str(round(GAMMA, 2))+\"_\"+repr(TSPA)+\"_\"+repr(R_C)[:1]\\n                # Train model\\n                success, solved = train_dqn4(model, render=True) \\n                stdout.write(\"\\rEpoch: {}, D: {}, Gamma: {}, Time steps per action: {}, Reward clipping: {}, Successes: {}, Solved: {}\".format(                                                     epoch,                                                     D,                                                     round(GAMMA, 2),                                                     TSPA,                                                     R_C,                                                     success,                                                     solved))\\n                stdout.flush()\\n                if solved or (success >= max_success and success > 0):\\n                    max_success = success\\n                    best_epochs.append((D, GAMMA, TSPA, R_C, success))\\n                    print \"\\n\\nBest Epochs:\"\\n                    for be in best_epochs: print be\\n                    print \"\\n\"\\n                    if solved: break\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "max_success = 0 ###NEW Establish counter to detect champion parameter settings\n",
    "best_epochs = deque(maxlen=50) ###NEW Establish a list to catch the 50 best performing parameter settings and their performance\n",
    "epoch = 0\n",
    "\n",
    "print \"Session contains {} epochs\\n\".format(len(D_RANGE)*len(GAMMA_RANGE)*len(TSPA_RANGE)*len(R_CLIP))\n",
    "\n",
    "for D in D_RANGE:\n",
    "    # Initialize model\n",
    "    model = _create_network()\n",
    "    for GAMMA in GAMMA_RANGE: \n",
    "        for TSPA in TSPA_RANGE:\n",
    "            for R_C in R_CLIP:\n",
    "                # Count epochs\n",
    "                epoch += 1\n",
    "                # Create model id\n",
    "                MODEL_ID = repr(D)+\"_\"+str(round(GAMMA, 2))+\"_\"+repr(TSPA)+\"_\"+repr(R_C)[:1]\n",
    "                # Train model\n",
    "                success, solved = train_dqn4(model, render=True) \n",
    "                stdout.write(\"\\rEpoch: {}, D: {}, Gamma: {}, Time steps per action: {}, Reward clipping: {}, Successes: {}, Solved: {}\".format( \\\n",
    "                                                    epoch, \\\n",
    "                                                    D, \\\n",
    "                                                    round(GAMMA, 2), \\\n",
    "                                                    TSPA, \\\n",
    "                                                    R_C, \\\n",
    "                                                    success, \\\n",
    "                                                    solved))\n",
    "                stdout.flush()\n",
    "                if solved or (success >= max_success and success > 0):\n",
    "                    max_success = success\n",
    "                    best_epochs.append((D, GAMMA, TSPA, R_C, success))\n",
    "                    print \"\\n\\nBest Epochs:\"\n",
    "                    for be in best_epochs: print be\n",
    "                    print \"\\n\"\n",
    "                    if solved: break\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Combine models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_dqn5(model, render):\n",
    "    # Start a new epoch\n",
    "    episode = 0; success = 0; solved = False\n",
    "    epsilon = EPSILON_RANGE[0]\n",
    "    ERM = deque(maxlen=ERM_SIZE)\n",
    "    MH5 = path.join(SAVE_PATH, \"Models\", MODEL_ID+\".h5\")\n",
    "    MJS = path.join(SAVE_PATH, \"Models\", MODEL_ID+\".json\")\n",
    "\n",
    "    while episode <= TOTAL_TIME_STEPS:\n",
    "        \n",
    "        x_t = ENV.reset()\n",
    "        s_t = np.tile(x_t, D)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if render: ENV.render()\n",
    "            \n",
    "            # Anneal random exploration rate epsilon over exploration period\n",
    "            epsilon = epsilon - epsilon / TOTAL_EXPLORATION_STEPS if epsilon > EPSILON_RANGE[1] else EPSILON_RANGE[1]\n",
    "    \n",
    "            # Estimate rewards for each action (targets), at s_t\n",
    "            q = model.predict(s_t[np.newaxis])[0]\n",
    "\n",
    "            # Take action with highest estimated reward, \"epsilon greedy\"\n",
    "            if episode % TSPA == 0: a_t = np.argmax(q) if np.random.random() > epsilon else np.random.choice(ACTIONS, 1)[0]\n",
    "\n",
    "            # Observe after action a_t\n",
    "            x_t, r_t, done, info = ENV.step(a_t)\n",
    "            \n",
    "            # Clip rewards\n",
    "            if R_CLIP and r_t != 0: r_t = abs(r_t) / r_t\n",
    "            \n",
    "            # Create state at t1: Append x observations, throw away the earliest\n",
    "            s_t1 = np.concatenate((x_t, s_t[:(D-1) * INPUT_DIM,]), axis=0)\n",
    "            \n",
    "            ###NEW START Do a first model fit on a one-by-one observation basis\n",
    "            # Estimate rewards for each action (targets), at s_t1 (again a forward pass)\n",
    "            Q_sa = model.predict(s_t1[np.newaxis])[0] \n",
    "\n",
    "            ''' Create reference/targets by updating estimated reward for chosen action\n",
    "                For action taken, replace estimated reward by remaining cumulative lifetime reward\n",
    "            ''' \n",
    "            targets = q\n",
    "            targets[a_t] = r_t + GAMMA * np.max(Q_sa) if not done else r_t\n",
    "\n",
    "            ''' Learn!\n",
    "                - Again, predict q values for state s_t\n",
    "                - Calculate loss by comparing predictions to targets: they will differ only for the action taken\n",
    "                - backpropagate error for action taken, update weights\n",
    "            ''' \n",
    "            model.fit(s_t[np.newaxis], targets[np.newaxis], nb_epoch=10, verbose=0)\n",
    "            ###NEW END\n",
    "            \n",
    "            # Store transition in experience replay memory\n",
    "            ERM.append((s_t, a_t, r_t, s_t1))\n",
    "\n",
    "            # Choose a batch of maximum length BATCH_SIZE\n",
    "            minibatch = np.array([ ERM[i] for i in np.random.choice(np.arange(0, len(ERM)), min(len(ERM), BATCH_SIZE)) ])\n",
    "            \n",
    "            # Compute targets/reference for each transition in minibatch\n",
    "            inputs = deque(); targets = deque()\n",
    "            for m in minibatch:\n",
    "                inputs.append(m[0]) # Append s_t of batch transition m to inputs\n",
    "                m_q    = model.predict(m[0][np.newaxis])[0] # Estimate rewards for each action (targets), at s_t\n",
    "                m_Q_sa = model.predict(m[3][np.newaxis])[0] # Estimate rewards for each action (targets), at s_t1\n",
    "                m_targets = m_q\n",
    "                m_targets[m[1]] = m[2] + GAMMA * np.max(m_Q_sa)\n",
    "                targets.append(m_targets) # Append target of batch transition m to targets\n",
    "                \n",
    "            # Train the model by backpropagating the errors and update weights\n",
    "            model.train_on_batch(np.array(inputs), np.array(targets))\n",
    "                \n",
    "            # Save progress every 100 iterations\n",
    "            if episode % 100 == 0:\n",
    "                model.save_weights(MH5, overwrite=True)\n",
    "            with open(MJS, \"w\") as outfile: json.dump(model.to_json(), outfile)\n",
    "            \n",
    "            # Update state and episode\n",
    "            s_t = s_t1\n",
    "            episode += 1\n",
    "        \n",
    "            # Bookkeeping\n",
    "            if r_t >= 100: success += 1\n",
    "            if r_t >= 200: \n",
    "                solved = True\n",
    "                break\n",
    "\n",
    "    # Return values for epoch's bookkeeping\n",
    "    return success, solved "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session contains 24240 epochs\n",
      "\n",
      "Epoch: 7, D: 1, Gamma: 0.0, Time steps per action: 4, Reward clipping: False, Successes: 0, Solved: False"
     ]
    }
   ],
   "source": [
    "###REMOVE \n",
    "TOTAL_TIME_STEPS = 1e2 * 2 # TODO dqn 10**7\n",
    "\n",
    "max_success = 0\n",
    "best_epochs = deque(maxlen=50)\n",
    "epoch = 0\n",
    "\n",
    "print \"Session contains {} epochs\\n\".format(len(D_RANGE)*len(GAMMA_RANGE)*len(TSPA_RANGE)*len(R_CLIP))\n",
    "\n",
    "for D in D_RANGE:\n",
    "    # Initialize model\n",
    "    model = _create_network()\n",
    "    for GAMMA in GAMMA_RANGE: \n",
    "        for TSPA in TSPA_RANGE:\n",
    "            for R_C in R_CLIP:\n",
    "                # Count epochs\n",
    "                epoch += 1\n",
    "                # Create model id\n",
    "                MODEL_ID = repr(D)+\"_\"+str(round(GAMMA, 2))+\"_\"+repr(TSPA)+\"_\"+repr(R_C)[:1]\n",
    "                # Train model\n",
    "                success, solved = train_dqn5(model, render=True) \n",
    "                stdout.write(\"\\rEpoch: {}, D: {}, Gamma: {}, Time steps per action: {}, Reward clipping: {}, Successes: {}, Solved: {}\".format( \\\n",
    "                                                    epoch, \\\n",
    "                                                    D, \\\n",
    "                                                    round(GAMMA, 2), \\\n",
    "                                                    TSPA, \\\n",
    "                                                    R_C, \\\n",
    "                                                    success, \\\n",
    "                                                    solved))\n",
    "                stdout.flush()\n",
    "                if solved or (success >= max_success and success > 0):\n",
    "                    max_success = success\n",
    "                    best_epochs.append((D, GAMMA, TSPA, R_C, success))\n",
    "                    print \"\\n\\nBest Epochs:\"\n",
    "                    for be in best_epochs: print be\n",
    "                    print \"\\n\"\n",
    "                    if solved: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    if args['mode'] == 'Run':\n",
    "        OBSERVE = 999999999    #We keep observe, never train\n",
    "        epsilon = FINAL_EPSILON\n",
    "        print (\"Now we load weight\")\n",
    "        model.load_weights(\"model.h5\")\n",
    "        adam = Adam(lr=1e-6)\n",
    "        model.compile(loss='mse',optimizer=adam)\n",
    "        print (\"Weight load successfully\")  \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TODO Policy Gradients"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
