{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs\n",
    "\n",
    "- create a desc for each function\n",
    "- comment code properly\n",
    "- model:\n",
    "    - hinton dropout\n",
    "    - numpy only model\n",
    "- loss # model.metrics_names: ['loss', 'acc']\n",
    "- append code with full model\n",
    "\n",
    "\n",
    "## Purpose\n",
    "\n",
    "- Get started with neural nets? Convolutions, Fully-connected layers, activations\n",
    "- See a thing learn is exciting (05 smartcab)\n",
    "- The field of ML I know least\n",
    "\n",
    "## Credits and Thanks\n",
    "\n",
    "- Tambet Matiisen\n",
    "https://www.nervanasys.com/demystifying-deep-reinforcement-learning/ https://github.com/tambetm/simple_dqn/blob/master/src/replay_memory.py\n",
    "- Andrew Trask\n",
    "https://iamtrask.github.io\n",
    "- Eder Santana\n",
    "http://edersantana.github.io/articles/keras_rl/\n",
    "- Deep Mind\n",
    "https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "- Ben Lau\n",
    "https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html\n",
    "- Francois Chollet\n",
    "https://github.com/fchollet/keras/tree/master/examples \n",
    "https://keras.io\n",
    "- Sebastian Raschka\n",
    "http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html\n",
    "- Christopher Olah\n",
    "http://colah.github.io/posts/2014-07-Conv-Nets-Modular/ TODO\n",
    "- Karpathy\n",
    "TODO\n",
    "\n",
    "\n",
    "\n",
    "## Storyline\n",
    "\n",
    "1.\tQ-Learning \n",
    "a.\tRe-visit state s_t, if already existing \n",
    "b.\tperform action with maximum value (action-value: q), \n",
    "\n",
    "2.\tToo many possible states: even with lunar lander (8 inputs). What to do?\n",
    "a.\tANN-Approximate / estimate q-value based on s_t, \n",
    "b.\tperform action based on the argmax of the estimation of q-values, \n",
    "c.\trecalculate q-value for taken action with r_t, and (another approximation, based on s_t1) expected, discounted reward based upon s_t1 (target for supervised learning problem)\n",
    "d.\tcalculate error for taken action (mse)\n",
    "e.\tbackprop error through ANN\n",
    "3.\tToo shaky! Do experience replay: train ANN on batch of transitions every x-th timestep, in stead of single observations/s_t each time step\n",
    "4.\tDo proper parametrization: epsilon, alpha, gamma (dropout)\n",
    "5.\t1D inputs vs 2D inputs: use different input preprocessing\n",
    "6.\tComputationally intensive! \n",
    "a.\tUse AWS. \n",
    "b.\tOK. But how about alternatives?: Policy Gradients TODO COMPARE\n",
    "7.\tSUPPORT FOR 2\n",
    "a.\tWhat is a neural net; by this meaning fully-connected layer: Create a ANN from scratch\n",
    "b.\tMaybe: What is a convolution? TODO\n",
    "\n",
    "Start with 1d, maybe 2d later TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where It Starts: Implementing the Bellman Equation, or: Pure Q-Learning\n",
    "\n",
    "Q learning is about revisiting states. We are in a specific state s at time t, and because the state space is sufficiently small, we might discover that the agent has already been in s before. It therefore has made an experience for s by taking an action, and collecting a reward or punishment. All this is stored in the agent's \"memory\" (possible a dictionary of dictionaries, main keys being the states, values being the actions (keys) and their values). We want the agent to take advantage of this \"memory\": We look up the expected lifetime rewards per each possible action in s (a.k.a. action-value function, q values), select the maximum q value, and execute the chosen action. \n",
    "\n",
    "We now have fresh evidence about the consequence of a specific action in a specific state: We know the initial state s_t, we know the selected action a_t, we know the reward r_t, and we know the new state this all lead to, s_t1. This knowledge we now use to update the agent's memory: We calculate a new q value for s_t by taking the observed reward r_t, and adding to it the discounted maximum q value for s_t1. The difference to the old q value is the new q value for action a_t in state s_t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why It Often Does Not Work: Exploding State Spaces\n",
    "\n",
    "Revisiting states is often not possible, because there are simply too many combinations of relevant inputs which constitute a state. Just think a small number of inputs, each input being a floating number with 4 digits. Even this small setting is creating a large amount of combinations: the state space explodes. Revisiting states is very unlikely, we will need a huge number of trials to generate memory updates. As a consequence, learning is slow, or even not happening.\n",
    "\n",
    "#### TODO: lunar lander with pure q learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning: Do Not Update the Q Function Directly, but the Q function Estimator\n",
    "\n",
    "In this situation, we replace the \"revisiting states\" by a function approximator: We let a Artifical Neural Net (ANN) estimate the q function for the state s the agent is visiting at time t. \n",
    "\n",
    "Once we performed the action based on the maximum of the q function (just the action with the highest expected lifetime reward at time t), we know the reward, and the subsequent state. \n",
    "\n",
    "Based on this, we are able to update the agent's memory. But this time, we do not update the q function directly. Instead, we are updating the ANN, which means that we are updating the weights used in the ANN. And that is how exactly:\n",
    "\n",
    "- At time t, we already know the estimation of the q function for state s_t: We used it to pick an action a_t accordingly. Read again: this is the ESTIMATION of the q function.\n",
    "\n",
    "- After action a_t, we know s_t, a_t, r_t and s_t1. This allows us to update the q function, BUT ONLY FOR THE ACTION TAKEN. We take r_t, and add to it the discounted expected lifetime reward, in other words we let the ANN estimate the q function for state s_t1. \n",
    "\n",
    "- For the action taken, we can update the q value now. All the other actions are not performed, we do not know about the reward, or a subsequent state s_t1. So, we cannot learn for those. This updated q function is the TARGET.\n",
    "\n",
    "- We feed the error, which is the difference between the ESTIMATION and the TARGET.\n",
    "\n",
    "- We backpropagate the error through the network, such that the weights are updated.\n",
    "\n",
    "Next time we estimate the q function for another state s, we have updated weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites for Deep Learning: Understanding the Gist of ANNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create A Basic ANN With Only Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Size 1, alpha 0.01: Final avg loss 0.332482398892, Improvement -0.0284334709819\n",
      "Hidden Size 1, alpha 0.1: Final avg loss 0.173228479187, Improvement -0.156241580326\n",
      "Hidden Size 1, alpha 1: Final avg loss 0.0417834158496, Improvement -0.122337346044\n",
      "Hidden Size 1, alpha 10: Final avg loss 0.0114937426301, Improvement -0.0279103012607\n",
      "Hidden Size 1, alpha 100: Final avg loss 0.00346337950767, Improvement -0.00744523640445\n",
      "Hidden Size 1, alpha 1000: Final avg loss 0.00107518867278, Improvement -0.00221871846119\n",
      "Hidden Size 1, alpha 1000: Final avg loss 0.000777709786278, Improvement -0.000287819531062\n",
      "Hidden Size 2, alpha 0.01: Final avg loss 0.255131325874, Improvement -0.0323567995625\n",
      "Hidden Size 2, alpha 0.1: Final avg loss 0.125350688814, Improvement -0.126584169372\n",
      "Hidden Size 2, alpha 1: Final avg loss 0.0335318290234, Improvement -0.0856951767789\n",
      "Hidden Size 2, alpha 10: Final avg loss 0.00919545195892, Improvement -0.0224759100392\n",
      "Hidden Size 2, alpha 100: Final avg loss 0.00269408510105, Improvement -0.00602389217425\n",
      "Hidden Size 2, alpha 1000: Final avg loss 0.000814883048915, Improvement -0.00174423081143\n",
      "Hidden Size 2, alpha 1000: Final avg loss 0.000585790631481, Improvement -0.000221627437158\n",
      "Hidden Size 3, alpha 0.01: Final avg loss 0.217830593655, Improvement -0.0357935098636\n",
      "Hidden Size 3, alpha 0.1: Final avg loss 0.101666287372, Improvement -0.112816195781\n",
      "Hidden Size 3, alpha 1: Final avg loss 0.0290411788879, Improvement -0.0677795493339\n",
      "Hidden Size 3, alpha 10: Final avg loss 0.00824230796289, Improvement -0.019257551707\n",
      "Hidden Size 3, alpha 100: Final avg loss 0.00242369850562, Improvement -0.00539529605522\n",
      "Hidden Size 3, alpha 1000: Final avg loss 0.000727509715683, Improvement -0.0015743604456\n",
      "Hidden Size 3, alpha 1000: Final avg loss 0.000521424872178, Improvement -0.000199362810032\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "np.random.seed(324)\n",
    "\n",
    "hidden_sizes = np.arange(1, 4)\n",
    "training_steps = 100\n",
    "\n",
    "alphas = [0.01, 0.1, 1, 10, 100, 1000, 1000]\n",
    "\n",
    "X = np.random.randint(2, size=(4, 3))\n",
    "y = np.random.randint(2, size=(4, 1))\n",
    "#print X\n",
    "#print y\n",
    "\n",
    "# Test the hidden sizes\n",
    "for hidden_size in hidden_sizes:\n",
    "    \n",
    "    # Initialize 1st set of weights\n",
    "    W1 = np.random.rand(X.shape[1], hidden_size)\n",
    "\n",
    "    # Initialize 2nd set of weights\n",
    "    W2 = np.random.rand(hidden_size, y.shape[1])\n",
    "        \n",
    "    # Test the alphas\n",
    "    for alpha in alphas:\n",
    "        \n",
    "        for i in range(training_steps):\n",
    "            # Initialize hidden (fully connected) layer\n",
    "            layer_1 = sigmoid(np.dot(X, W1))\n",
    "\n",
    "            # Initialize y (fully connected) layer\n",
    "            layer_2 = sigmoid(np.dot(layer_1, W2))\n",
    "\n",
    "            # Get loss (MSE)\n",
    "            layer_2_loss = y - layer_2\n",
    "\n",
    "            ''' Apply SGD to the loss: the more certain the estimate, the less weighted it will get: \n",
    "                The gradient at the extremes is smaller than in the middle\n",
    "            '''\n",
    "            layer_2_wloss = layer_2_loss * sigmoid_derivative(layer_2) # element-wise multiplication!\n",
    "            #print layer_2\n",
    "            #print sigmoid_derivative(layer_2) \n",
    "\n",
    "            # Compute the effect of the hidden layer to the weighted loss\n",
    "            layer_1_loss = np.dot(layer_2_wloss, W2.T)\n",
    "\n",
    "            # Apply SGD\n",
    "            layer_1_wloss = layer_1_loss * sigmoid_derivative(layer_1)\n",
    "\n",
    "            # Update the weights\n",
    "            W2 += alpha * np.dot(layer_1.T, layer_2_wloss)\n",
    "            W1 += alpha * np.dot(X.T, layer_1_wloss)\n",
    "\n",
    "            if i == 1: first_error = np.mean(np.abs(layer_2_loss))\n",
    "            if i == training_steps - 1: print \"Hidden Size {}, alpha {}: Final avg loss {}, Improvement {}\".format(\n",
    "                                                hidden_size, alpha, np.mean(np.abs(layer_2_loss)), \n",
    "                                                np.mean(np.abs(layer_2_loss)) - first_error)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aftermath\n",
    "\n",
    "TODO alpha, layer size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning - Step by Step\n",
    "\n",
    "We are working with Open AI Gym (https://gym.openai.com/) as a training environment for our to-be-defined AI agent.\n",
    "\n",
    "Lunar Lander environment is particularily appealing to me due to two reasons:\n",
    "1. It is based on box2d, which simulates real life physics\n",
    "2. It is a starting point for creating and tuning an AI agent with a 1D vector of 8 floating numbers as a state, and four actions: left, right, upper, and lower engine fire. \n",
    "\n",
    "Model building and training is done with Keras (https://keras.io). This modular, minimalist library makes ANN life as easy as it can get, and in plus runs on both Theano and Tensorflow.\n",
    "\n",
    "\n",
    "### Step 1: Deep Q Learning from Single Observations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Namespace (Extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Hyperparameters\n",
    "\n",
    "We start with a first set of static hyperparameters. Some of them will undergo changes along the way:\n",
    "\n",
    "- D_RANGE is the number of frames the agent should take into account as the current state it is in: This is the \"operational\" memory of the agent. I will refer to it as time step memory.\n",
    "- GAMMA is the factor by which future expected rewards are discounted.\n",
    "- N_EPISODES denotes the maximum number of episodes an epoch will embrace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D_RANGE = [1, 16]  # to loop over!\n",
    "GAMMA = 0.99\n",
    "N_EPISODES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Environment\n",
    "\n",
    "We create an instance ENV of the Lunar lander environment. Its input dimensions INPUT_DIM are obtained by resetting the environment, and the actions by the environment method *action_space*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-10-07 16:40:12,207] Making new env: LunarLander-v2\n"
     ]
    }
   ],
   "source": [
    "ENV = gym.make(\"LunarLander-v2\")\n",
    "INPUT_DIM = ENV.reset().shape[0]\n",
    "N_ACTIONS = ENV.action_space.n\n",
    "ACTIONS = np.arange(0, N_ACTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Keras Model\n",
    "\n",
    "- one fully connected layer\n",
    "- multicat output (softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _create_network():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, init='glorot_normal', input_shape=(D*INPUT_DIM,))) #init='glorot_normal'\n",
    "    model.add(Dense(N_ACTIONS, init='glorot_normal', activation='softmax')) #init='glorot_normal'\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Build Training Epoch\n",
    "  \n",
    "We are going to create the main building block of this exercise: The training block for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_01(model, render):\n",
    "    # Start a new epoch\n",
    "    episode = 0\n",
    "    success = 0\n",
    "\n",
    "    for episode in range(N_EPISODES):\n",
    "        \n",
    "        x_t = ENV.reset()\n",
    "        s_t = np.tile(x_t, D)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if render: ENV.render()\n",
    "    \n",
    "            # estimate rewards for each action (targets), at s_t\n",
    "            q = model.predict(s_t[np.newaxis])[0]\n",
    "\n",
    "            # Take action with highest estimated reward\n",
    "            a_t = np.argmax(q) #argmax returns index\n",
    "\n",
    "            # Observe after action a_t\n",
    "            x_t, r_t, done, info = ENV.step(a_t)\n",
    "        \n",
    "            # Create state at t1: Append x observations, throw away the earliest\n",
    "            s_t1 = np.concatenate((x_t, s_t[:(D-1) * INPUT_DIM,]), axis=0)\n",
    "\n",
    "            # Estimate rewards for each action (targets), at s_t1 (again a forward pass)\n",
    "            Q_sa = model.predict(s_t1[np.newaxis])[0]\n",
    "\n",
    "            ''' Create reference/targets by updating estimated reward for chosen action\n",
    "                For action taken, replace estimated reward by remaining cumulative lifetime reward\n",
    "            ''' \n",
    "            targets = q\n",
    "            targets[a_t] = r_t + GAMMA * np.max(Q_sa) if not done else r_t\n",
    "\n",
    "            ''' Learn!\n",
    "                - Again, predict q values for state s_t\n",
    "                - Calculate loss by comparing predictions to targets: they will differ only for the action taken\n",
    "                - backpropagate error for action taken, update weights\n",
    "            ''' \n",
    "            model.fit(s_t[np.newaxis], targets[np.newaxis], nb_epoch=10, verbose=0)\n",
    "\n",
    "            # Update state and episode\n",
    "            s_t = s_t1\n",
    "            episode += 1\n",
    "        \n",
    "            # Bookkeeping\n",
    "            if r_t > 100: success += 1\n",
    "\n",
    "    print \"Memory Length {}, Episodes {}, Number of Successes {}\".format(D, episode, success)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model\n",
    "\n",
    "What can learn from this basic implementation? Surely, does it make the impression to learn at all? We also should think of the agent's time step memory. Does it make sense to let the agent know only its current state, or shall we allow him to take into consideration also some of the states before? If yes, how far back should it remember? To get a first hint to answer this question, let's run a simulation. We loop over a range of D candidates, and produce some summary statistics in order to assess the performance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Length 1, Episodes 113, Number of Successes 0\n",
      "Memory Length 2, Episodes 290, Number of Successes 0\n",
      "Memory Length 3, Episodes 119, Number of Successes 0\n",
      "Memory Length 4, Episodes 247, Number of Successes 0\n",
      "Memory Length 5, Episodes 60, Number of Successes 0\n",
      "Memory Length 6, Episodes 216, Number of Successes 0\n",
      "Memory Length 7, Episodes 213, Number of Successes 0\n",
      "Memory Length 8, Episodes 109, Number of Successes 0\n",
      "Memory Length 9, Episodes 240, Number of Successes 0\n",
      "Memory Length 10, Episodes 85, Number of Successes 0\n",
      "Memory Length 11, Episodes 59, Number of Successes 1\n",
      "Memory Length 12, Episodes 567, Number of Successes 0\n",
      "Memory Length 13, Episodes 151, Number of Successes 0\n",
      "Memory Length 14, Episodes 97, Number of Successes 0\n",
      "Memory Length 15, Episodes 317, Number of Successes 1\n"
     ]
    }
   ],
   "source": [
    "for D in np.arange(D_RANGE[0], D_RANGE[1]):\n",
    "    # Initialize model\n",
    "    model = _create_network()\n",
    "    # Train model\n",
    "    train_01(model, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aftermath\n",
    "\n",
    "That is quite working nicely already! The frantic, purpose-less behaviour is gone most of the time.\n",
    "\n",
    "Some observations: \n",
    "- With only one frame in the frame memory, extensive swinging movement\n",
    "\n",
    "\n",
    "- TODO Schaukelbewegung bei zu grossen D? \n",
    "- Local minima: Not engaging the lower engine, and crash relentlessly\n",
    "- Getting stuck in local minima: introduce epsilon greedy action selection policy. It allows for a random move with probability epsilon and by that introduces the notion of exploration (random moves) vs. exploitation (act on estimation of the q function). \n",
    "\n",
    "Exploitation will reduce the probability of getting stuck in local minima, which are NOT reflecting the best action given a certain state. It's just like fresh air for the AI brain, fresh ideas from outside. \n",
    "\n",
    "On the other hand, the agent needs to train and get experience with his selected moves. It needs evidence that one decision was (not) the right one, and to update its decicion finding process (the weights of the ANN. It only gets it by acting according to its own decisions undisturbed by random inputs. This is where exploitation comes in.\n",
    "\n",
    "Balancing exploration and exploitation is a also subject of learning, i.e. tweaking the parameter and follow/challenge benchmark methods. I will now introduce the random exploration rate epsilon, which determines the rate of random moves at a given time \n",
    "\n",
    "I will follow th custom of allowing random moves at a linearly decreasing exploration rate during the so called EXPLORATION PERIOD: \n",
    "\n",
    "- We define a maximum and a minimum epsilon. \n",
    "- We let epsilon decrease linearly decrease from its max to min value over the period of EXPLORATION PERIOD.\n",
    "Afterwards, the agent will run on the min epsilon constantly during TRAINING PERIOD, until the total number of time steps is reached.\n",
    "\n",
    "TODO dqn [1, 0.1], i.e. introduce an observation period at the beginning where every move is random.\n",
    "\n",
    "Again following TODO dqn example, I assign 1/10th of the total time steps to exploration, the rest to training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 2: *Epsilon Greedy* Deep Q Learning from Single Observations\n",
    "\n",
    "In order to implement exploration and exploitation, we need to keep track of the number of time steps/frames only. We thus stop counting episodes (N_EPISODE) and establish global settings for \n",
    "- the total number of time steps: TOTAL_TIME_STEPS\n",
    "- the total number of exploration time steps: TOTAL_EXPLORATION_STEPS\n",
    "\n",
    "Since epsilon is not a constant, but decreasing linarily over the exploration period, we create an interval between the maximum and minimum epsilon allowed: EPSILON_RANGE. \n",
    "\n",
    "#### Set Hyperparameters (Extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#N_EPISODES = 10\n",
    "TOTAL_TIME_STEPS = 1e2 # TODO dqn 10**7\n",
    "TOTAL_EXPLORATION_STEPS = TOTAL_TIME_STEPS / 10 # TODO dqn 10**7\n",
    "EPSILON_RANGE = [0.5, 0.0001] # TODO dqn [1, 0.01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Training Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_02(model, render): \n",
    "    # Start a new epoch\n",
    "    episode = 0\n",
    "    success = 0\n",
    "    epsilon = EPSILON_RANGE[0] ##NEW Initialize epsilon at its maximum value\n",
    "\n",
    "    #for episode in range(N_EPISODES): ###NEW Discard the episode loop\n",
    "    while episode <= TOTAL_TIME_STEPS: ###NEW Install a loop over all time steps\n",
    "        \n",
    "        x_t = ENV.reset()\n",
    "        s_t = np.tile(x_t, D)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if render: ENV.render()\n",
    "            \n",
    "            ###NEW Anneal random exploration rate epsilon over exploration time steps\n",
    "            epsilon = epsilon - epsilon / TOTAL_EXPLORATION_STEPS if epsilon > EPSILON_RANGE[1] else EPSILON_RANGE[1]\n",
    "    \n",
    "            # estimate rewards for each action (targets), at s_t\n",
    "            q = model.predict(s_t[np.newaxis])[0]\n",
    "\n",
    "            # Take action with highest estimated reward \n",
    "            ###NEW Do this with probability 1-epsilon (\"epsilon greedy\" policy)\n",
    "            a_t = np.argmax(q) if np.random.random() > epsilon else np.random.choice(ACTIONS, 1)[0]\n",
    "\n",
    "            # Observe after action a_t\n",
    "            x_t, r_t, done, info = ENV.step(a_t)\n",
    "        \n",
    "            # Create state at t1: Append x observations, throw away the earliest\n",
    "            s_t1 = np.concatenate((x_t, s_t[:(D-1) * INPUT_DIM,]), axis=0)\n",
    "\n",
    "            # Estimate rewards for each action (targets), at s_t1 (again a forward pass)\n",
    "            Q_sa = model.predict(s_t1[np.newaxis])[0]\n",
    "\n",
    "            ''' Create reference/targets by updating estimated reward for chosen action\n",
    "                For action taken, replace estimated reward by remaining cumulative lifetime reward\n",
    "            ''' \n",
    "            targets = q\n",
    "            targets[a_t] = r_t + GAMMA * np.max(Q_sa) if not done else r_t\n",
    "\n",
    "            ''' Learn!\n",
    "                - Again, predict q values for state s_t\n",
    "                - Calculate loss by comparing predictions to targets: they will differ only for the action taken\n",
    "                - backpropagate error for action taken, update weights\n",
    "            ''' \n",
    "            model.fit(s_t[np.newaxis], targets[np.newaxis], nb_epoch=10, verbose=0)\n",
    "\n",
    "            # Update state and episode\n",
    "            s_t = s_t1\n",
    "            episode += 1\n",
    "        \n",
    "            # Bookkeeping\n",
    "            if r_t > 100: success += 1\n",
    "\n",
    "    print \"Memory Length {}, Episodes {}, Number of Successes {}\".format(D, episode, success)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Length 1, Episodes 146, Number of Successes 0\n",
      "Memory Length 2, Episodes 126, Number of Successes 0\n",
      "Memory Length 3, Episodes 144, Number of Successes 0\n",
      "Memory Length 4, Episodes 123, Number of Successes 0\n",
      "Memory Length 5, Episodes 104, Number of Successes 0\n",
      "Memory Length 6, Episodes 112, Number of Successes 0\n",
      "Memory Length 7, Episodes 109, Number of Successes 0\n",
      "Memory Length 8, Episodes 107, Number of Successes 0\n",
      "Memory Length 9, Episodes 269, Number of Successes 0\n",
      "Memory Length 10, Episodes 146, Number of Successes 0\n",
      "Memory Length 11, Episodes 244, Number of Successes 0\n",
      "Memory Length 12, Episodes 110, Number of Successes 0\n",
      "Memory Length 13, Episodes 111, Number of Successes 0\n",
      "Memory Length 14, Episodes 140, Number of Successes 0\n",
      "Memory Length 15, Episodes 156, Number of Successes 0\n"
     ]
    }
   ],
   "source": [
    "for D in np.arange(D_RANGE[0], D_RANGE[1]):\n",
    "    # Initialize model\n",
    "    model = _create_network()\n",
    "    # Train model\n",
    "    train_02(model, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Aftermath\n",
    "\n",
    "TODO\n",
    "It gets shakier. It definitively needs more time to learn. This is the cost, at which exploration comes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Deep Q Learning *from Stored Experiences* \n",
    "\n",
    "It has been shown that learning on the fly from observations XXX TODO. dQN, TODO. Instead, the trick is to learn from a memory storage in batches, the so called Experience Replay Memory (ERM). \n",
    "\n",
    "We are thus going to create the main database of the agent: It is the place where it \n",
    "- stores states and its experiences with the states (transitions s_t, a_t, r_t,and s_t1)\n",
    "- recalls on the memory, collects a memory sample, trains on the sample, and updates the Q function estimator.\n",
    "\n",
    "The ERM is set up once per epoch and is fed at each time step with fresh transition evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Namespace (Extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Hyperparameters (Extension)\n",
    "\n",
    "ERM_SIZE is setting the size of the experience replay memory. Following the recommendation of the dqn TODO, we set it equal to the number of exploration steps. The BATCH_SIZE denotes the size of the sample which is drawn uniformly without replacement from the ERM at each time step. Again, its size is following the recommendations of the dqn paper TODO. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ERM_SIZE = TOTAL_EXPLORATION_STEPS\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Training Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_03(model, render):\n",
    "    # Start a new epoch\n",
    "    episode = 0\n",
    "    success = 0\n",
    "    epsilon = EPSILON_RANGE[0]\n",
    "    ERM = deque(maxlen=ERM_SIZE) ###NEW If too long, throw away the earliest (latest is ERM[-1])\n",
    "\n",
    "    while episode <= TOTAL_TIME_STEPS:\n",
    "        \n",
    "        x_t = ENV.reset()\n",
    "        s_t = np.tile(x_t, D)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if render: ENV.render()\n",
    "            \n",
    "            # Anneal random exploration rate epsilon over exploration time steps\n",
    "            epsilon = epsilon - epsilon / TOTAL_EXPLORATION_STEPS if epsilon > EPSILON_RANGE[1] else EPSILON_RANGE[1]\n",
    "    \n",
    "            # estimate rewards for each action (targets), at s_t\n",
    "            q = model.predict(s_t[np.newaxis])[0]\n",
    "\n",
    "            # Take action with highest estimated reward, \"epsilon greedy\"\n",
    "            a_t = np.argmax(q) if np.random.random() > epsilon else np.random.choice(ACTIONS, 1)[0]\n",
    "\n",
    "            # Observe after action a_t\n",
    "            x_t, r_t, done, info = ENV.step(a_t)\n",
    "        \n",
    "            # Create state at t1: Append x observations, throw away the earliest\n",
    "            s_t1 = np.concatenate((x_t, s_t[:(D-1) * INPUT_DIM,]), axis=0)\n",
    "            \n",
    "            ###NEW Store transition in experience replay memory\n",
    "            ERM.append((s_t, a_t, r_t, s_t1))\n",
    "\n",
    "            ###NEW Choose a batch of maximum length BATCH_SIZE\n",
    "            minibatch = np.array([ ERM[i] for i in np.random.choice(np.arange(0, len(ERM)), min(len(ERM), BATCH_SIZE)) ])\n",
    "            \n",
    "            ###NEW Compute targets/reference for each transition in minibatch\n",
    "            inputs = deque()\n",
    "            targets = deque()\n",
    "            for m in minibatch:\n",
    "                inputs.append(m[0]) # Append s_t of batch transition m to inputs\n",
    "                m_q    = model.predict(m[0][np.newaxis])[0] # Estimate rewards for each action (targets), at s_t\n",
    "                m_Q_sa = model.predict(m[3][np.newaxis])[0] # Estimate rewards for each action (targets), at s_t1\n",
    "                m_targets = m_q\n",
    "                m_targets[m[1]] = m[2] + GAMMA * np.max(m_Q_sa)\n",
    "                targets.append(m_targets) # Append target of batch transition m to targets\n",
    "                \n",
    "            ###NEW Train the model by backpropagating the errors and update weights\n",
    "            model.train_on_batch(np.array(inputs), np.array(targets))\n",
    "            \n",
    "            # Update state and episode\n",
    "            s_t = s_t1\n",
    "            episode += 1\n",
    "        \n",
    "            # Bookkeeping\n",
    "            if r_t > 100: success += 1\n",
    "\n",
    "    print \"Memory Length {}, Episodes {}, Number of Successes {}\".format(D, episode, success)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Length 1, Episodes 112, Number of Successes 0\n",
      "Memory Length 2, Episodes 111, Number of Successes 0\n",
      "Memory Length 3, Episodes 194, Number of Successes 0\n",
      "Memory Length 4, Episodes 201, Number of Successes 0\n",
      "Memory Length 5, Episodes 112, Number of Successes 0\n",
      "Memory Length 6, Episodes 139, Number of Successes 0\n",
      "Memory Length 7, Episodes 128, Number of Successes 0\n",
      "Memory Length 8, Episodes 186, Number of Successes 0\n",
      "Memory Length 9, Episodes 107, Number of Successes 0\n",
      "Memory Length 10, Episodes 186, Number of Successes 0\n",
      "Memory Length 11, Episodes 139, Number of Successes 0\n",
      "Memory Length 12, Episodes 164, Number of Successes 0\n",
      "Memory Length 13, Episodes 164, Number of Successes 0\n",
      "Memory Length 14, Episodes 102, Number of Successes 0\n",
      "Memory Length 15, Episodes 187, Number of Successes 0\n"
     ]
    }
   ],
   "source": [
    "for D in np.arange(D_RANGE[0], D_RANGE[1]):\n",
    "    # Initialize model with frame memory D\n",
    "    model = _create_network()\n",
    "    # Train model\n",
    "    train_03(model, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aftermath\n",
    "\n",
    "The question crossed my mind: Why don't we predict beforehand, on the fly, at every time step? Would that not be computationally efficient? This has the huge disadvantage that we predict with the knowledge available at timestep t. This might be faulty, and the faulty prediction stays as target reference in the batch, and is used to compare the loss for the taken action between the prediction at timestep t and the potenitially long ago target estimation. This will bias the learning process significantly. Thus, we select a batch, calculate the estimations and targets for the complete batch, both with the knowledge of the current time steps.\n",
    "\n",
    "TODO is np.array(minibatch) really faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Deep Q Learning from Stored Experiences, *Refined*\n",
    "\n",
    "At this point, we will finalize training by applying some more tweaks (outside the model block).\n",
    "\n",
    "#### Set Hyperparameters (Extension)\n",
    "\n",
    "- The learning rate decreases over the total number of time steps. \n",
    "    - We establish a hyperparameter to establish the starting alpha (ALPHA_MAX) \n",
    "    - We set up a switch ALPHA_LIN_DECREASE_FLAG, which specifies if alpha decreases linearly or non-linearly\n",
    "    - If alpha decreases nonlinarly, we set up a range of denominators for the alpha decay ALPHA_DENOM_RANGE\n",
    "- Discount factor gamma. We will replace the fixed hyperparameter by an arbitrary range of gamma candidates (GAMMA_range).\n",
    "- Take action every n-th time step (time step per action TSPA) TODO\n",
    "- Reward Clipping (R_CLIP_FLAG) TODO. Reward clipping is a boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALPHA_MAX = 3\n",
    "ALPHA_LIN_DECREASE_FLAG = [False, True] # to loop over!\n",
    "ALPHA_DENOM_RANGE = [2, 21] # to loop over!\n",
    "GAMMA_RANGE = [0.1, 1.6] # to loop over!\n",
    "R_CLIP_FLAG = [False, True] # to loop over!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least we will store the weights XXX TODO JSON ODER KERAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Training Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_04(model, render):\n",
    "    # Start a new epoch\n",
    "    episode = 0\n",
    "    success = 0\n",
    "    epsilon = EPSILON_RANGE[0]\n",
    "    alpha = ALPHA_MAX ###NEW Initialize the learning rate at its maximum\n",
    "    ERM = deque(maxlen=ERM_SIZE)\n",
    "\n",
    "    while episode <= TOTAL_TIME_STEPS:\n",
    "        \n",
    "        x_t = ENV.reset()\n",
    "        s_t = np.tile(x_t, D)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if render: ENV.render()\n",
    "            \n",
    "            # Anneal random exploration rate epsilon over exploration time steps\n",
    "            epsilon = epsilon - epsilon / TOTAL_EXPLORATION_STEPS if epsilon > EPSILON_RANGE[1] else EPSILON_RANGE[1]\n",
    "    \n",
    "            # estimate rewards for each action (targets), at s_t\n",
    "            q = model.predict(s_t[np.newaxis])[0]\n",
    "\n",
    "            # Take action with highest estimated reward, \"epsilon greedy\"\n",
    "            a_t = np.argmax(q) if np.random.random() > epsilon else np.random.choice(ACTIONS, 1)[0]\n",
    "\n",
    "            # Observe after action a_t\n",
    "            x_t, r_t, done, info = ENV.step(a_t)\n",
    "            \n",
    "            ###NEW Clip rewards\n",
    "            if R_CLIP and r_t != 0: r_t = abs(r_t) / r_t\n",
    "        \n",
    "            # Create state at t1: Append x observations, throw away the earliest\n",
    "            s_t1 = np.concatenate((x_t, s_t[:(D-1) * INPUT_DIM,]), axis=0)\n",
    "            \n",
    "            # Store transition in experience replay memory\n",
    "            ERM.append((s_t, a_t, r_t, s_t1))\n",
    "\n",
    "            # Choose a batch of maximum length BATCH_SIZE\n",
    "            minibatch = np.array([ ERM[i] for i in np.random.choice(np.arange(0, len(ERM)), min(len(ERM), BATCH_SIZE)) ])\n",
    "            \n",
    "            # Compute targets/reference for each transition in minibatch\n",
    "            inputs = deque()\n",
    "            targets = deque()\n",
    "            for m in minibatch:\n",
    "                inputs.append(m[0]) # Append s_t of batch transition m to inputs\n",
    "                m_q    = model.predict(m[0][np.newaxis])[0] # Estimate rewards for each action (targets), at s_t\n",
    "                m_Q_sa = model.predict(m[3][np.newaxis])[0] # Estimate rewards for each action (targets), at s_t1\n",
    "                m_targets = m_q\n",
    "                m_targets[m[1]] = m[2] + alpha * (GAMMA * np.max(m_Q_sa)) ###NEW Establish learning at rate alpha\n",
    "                targets.append(m_targets) # Append target of batch transition m to targets\n",
    "                \n",
    "            # Train the model by backpropagating the errors and update weights\n",
    "            model.train_on_batch(np.array(inputs), np.array(targets))\n",
    "            \n",
    "            ###NEW Anneal the learning rate alpha over all time steps\n",
    "            if ALPHA_LIN_DECREASE: alpha -= float(alpha) / TOTAL_TIME_STEPS if alpha >= 0 else 0 \n",
    "            else: alpha -= float(alpha) / ALPHA_DENOM\n",
    "            \n",
    "            # Update state and episode\n",
    "            s_t = s_t1\n",
    "            episode += 1\n",
    "        \n",
    "            # Bookkeeping\n",
    "            if r_t > 100: success += 1          \n",
    "\n",
    "    print \"Memory Length {}, Episodes {}, Successes {}\".format(D, episode, success)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model\n",
    "\n",
    "Fully fledged training! TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Length 1, Episodes 116, Number of Successes 0\n",
      "Memory Length 1, Episodes 117, Number of Successes 0\n",
      "Memory Length 1, Episodes 122, Number of Successes 0\n",
      "Memory Length 1, Episodes 222, Number of Successes 0\n",
      "Memory Length 1, Episodes 136, Number of Successes 0\n",
      "Memory Length 1, Episodes 130, Number of Successes 0\n",
      "Memory Length 1, Episodes 107, Number of Successes 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-138-629c26809644>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                     \u001b[1;31m# Train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                     \u001b[0mtrain_04\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-136-45b78e02c945>\u001b[0m in \u001b[0;36mtrain_04\u001b[1;34m(model, render)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;31m# Train the model by backpropagating the errors and update weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[1;31m###NEW Anneal the learning rate alpha over all time steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WettsteinM\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\keras\\models.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    710\u001b[0m         return self.model.train_on_batch(x, y,\n\u001b[0;32m    711\u001b[0m                                          \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 712\u001b[1;33m                                          class_weight=class_weight)\n\u001b[0m\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m     def test_on_batch(self, x, y,\n",
      "\u001b[1;32mC:\\Users\\WettsteinM\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1218\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1219\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1220\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WettsteinM\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36m_make_train_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    699\u001b[0m                                              \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m                                              \u001b[0mupdates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m                                              **self._function_kwargs)\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_test_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WettsteinM\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\keras\\backend\\theano_backend.pyc\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, updates, **kwargs)\u001b[0m\n\u001b[0;32m    680\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Invalid argument '%s' passed to K.function\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 682\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    683\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WettsteinM\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\keras\\backend\\theano_backend.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputs, outputs, updates, **kwargs)\u001b[0m\n\u001b[0;32m    666\u001b[0m                                         \u001b[0mallow_input_downcast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m                                         \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m                                         **kwargs)\n\u001b[0m\u001b[0;32m    669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WettsteinM\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\theano\\compile\\function.pyc\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[0;32m    318\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                    output_keys=output_keys)\n\u001b[0m\u001b[0;32m    321\u001b[0m     \u001b[1;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;31m# borrowed used defined inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WettsteinM\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\theano\\compile\\pfunc.pyc\u001b[0m in \u001b[0;36mpfunc\u001b[1;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[0;32m    477\u001b[0m                          \u001b[0maccept_inplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m                          \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 479\u001b[1;33m                          output_keys=output_keys)\n\u001b[0m\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WettsteinM\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\theano\\compile\\function_module.pyc\u001b[0m in \u001b[0;36morig_function\u001b[1;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[0;32m   1774\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1775\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1776\u001b[1;33m                    \u001b[0moutput_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1777\u001b[0m             defaults)\n\u001b[0;32m   1778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WettsteinM\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\theano\\compile\\function_module.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputs, outputs, mode, accept_inplace, function_builder, profile, on_unused_input, fgraph, output_keys)\u001b[0m\n\u001b[0;32m   1454\u001b[0m                         optimizer, inputs, outputs)\n\u001b[0;32m   1455\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1456\u001b[1;33m                     \u001b[0moptimizer_profile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1458\u001b[0m                 \u001b[0mend_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WettsteinM\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\theano\\gof\\opt.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, fgraph)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \"\"\"\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_requirements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WettsteinM\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\theano\\gof\\opt.pyc\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, fgraph, *args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0morig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0morig\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WettsteinM\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\theano\\gof\\opt.pyc\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fgraph)\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                 \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m                 \u001b[0msub_prof\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m                 \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m                 \u001b[0msub_profs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_prof\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WettsteinM\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\theano\\gof\\opt.pyc\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, fgraph, *args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0morig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0morig\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WettsteinM\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\theano\\gof\\opt.pyc\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fgraph, start_from)\u001b[0m\n\u001b[0;32m   2194\u001b[0m                         \u001b[0mnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchange_tracker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_imported\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2195\u001b[0m                         \u001b[0mt_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2196\u001b[1;33m                         \u001b[0mlopt_change\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2197\u001b[0m                         \u001b[0mtime_opts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlopt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt_opt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2198\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlopt_change\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WettsteinM\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\theano\\gof\\opt.pyc\u001b[0m in \u001b[0;36mprocess_node\u001b[1;34m(self, fgraph, node, lopt)\u001b[0m\n\u001b[0;32m   1770\u001b[0m         \u001b[0mlopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlopt\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal_opt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1771\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1772\u001b[1;33m             \u001b[0mreplacements\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1773\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1774\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfailure_callback\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WettsteinM\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\theano\\gof\\opt.pyc\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, node, get_nodes)\u001b[0m\n\u001b[0;32m   1527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1528\u001b[0m         u = match(self.in_pattern, node.out, unify.Unification(), True,\n\u001b[1;32m-> 1529\u001b[1;33m                   self.pdb)\n\u001b[0m\u001b[0;32m   1530\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1531\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WettsteinM\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\theano\\gof\\opt.pyc\u001b[0m in \u001b[0;36mmatch\u001b[1;34m(pattern, expr, u, allow_multiple_clients, pdb)\u001b[0m\n\u001b[0;32m   1485\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mretry_with_equiv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1486\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1487\u001b[1;33m                     \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallow_multiple_clients\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1488\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1489\u001b[0m                         \u001b[1;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WettsteinM\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\theano\\gof\\opt.pyc\u001b[0m in \u001b[0;36mmatch\u001b[1;34m(pattern, expr, u, allow_multiple_clients, pdb)\u001b[0m\n\u001b[0;32m   1511\u001b[0m                     isinstance(expr, graph.Constant)):\n\u001b[0;32m   1512\u001b[0m                 if numpy.all(\n\u001b[1;32m-> 1513\u001b[1;33m                         theano.tensor.constant(pattern).value == expr.value):\n\u001b[0m\u001b[0;32m   1514\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1515\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WettsteinM\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\theano\\tensor\\basic.pyc\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(x, name, ndim, dtype)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m     ret = constant_or_value(x, rtype=TensorConstant, name=name, ndim=ndim,\n\u001b[1;32m--> 422\u001b[1;33m                             dtype=dtype)\n\u001b[0m\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[1;31m# We create a small cache of frequently used constant.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WettsteinM\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\theano\\tensor\\basic.pyc\u001b[0m in \u001b[0;36mconstant_or_value\u001b[1;34m(x, rtype, name, ndim, dtype)\u001b[0m\n\u001b[0;32m    408\u001b[0m                 \u001b[0mTensorType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbroadcastable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbcastable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m                 \u001b[0mx_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 410\u001b[1;33m                 name=name)\n\u001b[0m\u001b[0;32m    411\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WettsteinM\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\theano\\tensor\\var.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, type, data, name)\u001b[0m\n\u001b[0;32m    866\u001b[0m     \"\"\"\n\u001b[0;32m    867\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m         \u001b[0mConstant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WettsteinM\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\theano\\gof\\graph.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, type, data, name)\u001b[0m\n\u001b[0;32m    550\u001b[0m     \u001b[1;31m# __slots__ = ['data']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m         \u001b[0mVariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\WettsteinM\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\theano\\gof\\graph.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, type, owner, index, name)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mowner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscratchpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for D in np.arange(D_RANGE[0], D_RANGE[1]):\n",
    "    for ALPHA_LIN_DECREASE in ALPHA_LIN_DECREASE_FLAG:\n",
    "        for ALPHA_DENOM in ALPHA_DENOM_RANGE:\n",
    "            for R_CLIP in R_CLIP_FLAG:\n",
    "                for GAMMA in np.arange(GAMMA_RANGE[0], GAMMA_RANGE[1], 0.1): \n",
    "                    # Initialize model with frame memory D\n",
    "                    model = _create_network()\n",
    "                    # Train model\n",
    "                    train_04(model, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
